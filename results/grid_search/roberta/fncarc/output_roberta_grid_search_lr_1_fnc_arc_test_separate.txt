Script started on 2020-02-14 20:40:04+0000
ubuntu@run-gpu-mg:~/fnd_implementation$ exittime python3 eval_separate.py --model=robberta --model_type=roberta-base --dataset_name=fnc_arcM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=robe[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=rober[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=robert[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=roberta[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct_separate.py --model=robert[1@aM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ce_separate.py --model=rober[1@tM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cs_separate.py --model=robe[1@rM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct_separate.py --model=rob[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
There are 2 GPU(s) available.
The following GPU is used:  Tesla V100-PCIE-16GB
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/ubuntu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/ubuntu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/model_pretrained/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/model_pretrained/pytorch_model.bin
INFO:__main__:Loading initialized pretrained model from /home/ubuntu/fnd_implementation/roberta/model_pretrained
  ******************************************* 
           Freezing Embedding Layers          
  *******************************************
 
name:  roberta.embeddings.word_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.position_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.token_type_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.LayerNorm.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.LayerNorm.bias
param.requires_grad:  False
=====
name:  roberta.encoder.layer.0.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.pooler.dense.weight
param.requires_grad:  True
=====
name:  roberta.pooler.dense.bias
param.requires_grad:  True
=====
name:  classifier.dense.weight
param.requires_grad:  True
=====
name:  classifier.dense.bias
param.requires_grad:  True
=====
name:  classifier.out_proj.weight
param.requires_grad:  True
=====
name:  classifier.out_proj.bias
param.requires_grad:  True
=====
/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_abfb6296_2020-02-14_16-10-26vjyufsvp/params.json
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_abfb6296_2020-02-14_16-10-26vjyufsvp/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_abfb6296_2020-02-14_16-10-26vjyufsvp/outputs/checkpoint-3/pytorch_model.bin
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/28972 [00:00<?, ?it/s]  0%|                                       | 1/28972 [00:00<5:58:55,  1.35it/s]  2%|â–‹                                    | 501/28972 [00:00<4:06:58,  1.92it/s]  3%|â–ˆâ–                                  | 1001/28972 [00:01<2:49:57,  2.74it/s] 10%|â–ˆâ–ˆâ–ˆâ–‹                                | 3001/28972 [00:01<1:50:28,  3.92it/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 4001/28972 [00:01<1:14:22,  5.60it/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 5501/28972 [00:01<48:57,  7.99it/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 7501/28972 [00:02<31:21, 11.41it/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 8226/28972 [00:02<21:13, 16.29it/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 9001/28972 [00:02<14:19, 23.22it/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 10001/28972 [00:02<09:33, 33.07it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 12001/28972 [00:02<05:59, 47.20it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 13001/28972 [00:03<03:57, 67.13it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 14001/28972 [00:03<02:36, 95.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 16001/28972 [00:03<01:36, 134.62it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 17501/28972 [00:04<01:00, 190.29it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 18392/28972 [00:04<00:39, 269.33it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 19037/28972 [00:04<00:27, 362.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 20001/28972 [00:04<00:18, 492.40it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 20501/28972 [00:05<00:13, 643.31it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 22501/28972 [00:05<00:07, 902.73it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 23226/28972 [00:05<00:04, 1157.94it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25001/28972 [00:06<00:02, 1386.91it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28972/28972 [00:06<00:00, 4645.78it/s]
INFO:__main__:  ******************************************* 
INFO:__main__:              Running Final Testing           
INFO:__main__:  ******************************************* 
 
INFO:__main__:  ******************************************* 
INFO:__main__:  Model: roberta
INFO:__main__:  Length dataset testing: 28972
INFO:__main__:  Length dataloader testing: 7243
INFO:__main__:  Number of epochs: 3
INFO:__main__:  Maximal sequence length: 512
INFO:__main__:  Batch size: 4
INFO:__main__:  Learning rate: 1e-05
INFO:__main__:  Learning rate type: cosine
INFO:__main__:  ******************************************* 
 
test_separate.py:222: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch in tqdm_notebook(test_dataloader, desc="Testing"):
HBox(children=(FloatProgress(value=0.0, description='Testing', max=7243.0, style=ProgressStyle(description_width='initial')), HTML(value='')))

-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |   1564    |    229    |    407    |    21     |
-------------------------------------------------------------
| disagree  |    141    |    512    |    126    |    10     |
-------------------------------------------------------------
|  discuss  |    503    |    289    |   3995    |    178    |
-------------------------------------------------------------
| unrelated |    29     |    39     |    115    |   20814   |
-------------------------------------------------------------
Score: 21323.25 out of 22597.75	(94.36005797037316%)
Accuracy: 0.9279649316581527
F1 overall: 0.7687649903855439
F1 per class: [0.7016599371915657, 0.5511302475780409, 0.8315986677768527, 0.9906711089957163]

real	6m34.854s
user	5m59.684s
sys	1m44.860s
ubuntu@run-gpu-mg:~/fnd_implementation$ exit
exit

Script done on 2020-02-14 20:48:08+0000

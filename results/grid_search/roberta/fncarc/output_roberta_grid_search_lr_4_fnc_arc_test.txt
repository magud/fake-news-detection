Script started on 2020-02-20 21:11:07+0000
ubuntu@run-gpu-mg:~/fnd_implementation$ exittime python3 eval_separate.py --model=robberta --model_type=roberta-base --dataset_name=fnc_arcM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=robe[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=rober[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=robert[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=roberta[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct_separate.py --model=robert[1@aM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ce_separate.py --model=rober[1@tM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cs_separate.py --model=robe[1@rM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct_separate.py --model=rob[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
There are 2 GPU(s) available.
The following GPU is used:  Tesla V100-PCIE-16GB
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/ubuntu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/ubuntu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/model_pretrained/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/model_pretrained/pytorch_model.bin
INFO:__main__:Loading initialized pretrained model from /home/ubuntu/fnd_implementation/roberta/model_pretrained
  ******************************************* 
           Freezing Embedding Layers          
  *******************************************
 
name:  roberta.embeddings.word_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.position_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.token_type_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.LayerNorm.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.LayerNorm.bias
param.requires_grad:  False
=====
name:  roberta.encoder.layer.0.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.pooler.dense.weight
param.requires_grad:  True
=====
name:  roberta.pooler.dense.bias
param.requires_grad:  True
=====
name:  classifier.dense.weight
param.requires_grad:  True
=====
name:  classifier.dense.bias
param.requires_grad:  True
=====
name:  classifier.out_proj.weight
param.requires_grad:  True
=====
name:  classifier.out_proj.bias
param.requires_grad:  True
=====
/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/params.json
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3/pytorch_model.bin
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/28972 [00:00<?, ?it/s]  0%|                                       | 1/28972 [00:00<5:49:27,  1.38it/s]  2%|▋                                    | 501/28972 [00:01<4:00:29,  1.97it/s]  3%|█▏                                  | 1001/28972 [00:01<2:45:27,  2.82it/s] 10%|███▋                                | 3001/28972 [00:01<1:47:33,  4.02it/s] 14%|████▉                               | 4001/28972 [00:01<1:12:25,  5.75it/s] 21%|███████▊                              | 6001/28972 [00:01<46:38,  8.21it/s] 26%|█████████▊                            | 7501/28972 [00:02<30:31, 11.72it/s] 29%|███████████▏                          | 8501/28972 [00:02<20:23, 16.73it/s] 32%|████████████▎                         | 9345/28972 [00:02<13:41, 23.88it/s] 35%|█████████████                        | 10187/28972 [00:02<09:13, 33.92it/s] 45%|████████████████▌                    | 13001/28972 [00:03<05:30, 48.36it/s] 48%|█████████████████▉                   | 14001/28972 [00:03<03:37, 68.73it/s] 55%|████████████████████▍                | 16001/28972 [00:04<02:13, 97.39it/s] 59%|█████████████████████▏              | 17001/28972 [00:04<01:26, 138.15it/s] 61%|█████████████████████▊              | 17535/28972 [00:04<00:58, 194.58it/s] 66%|███████████████████████▌            | 19001/28972 [00:04<00:36, 273.00it/s] 71%|█████████████████████████▍          | 20501/28972 [00:05<00:22, 375.80it/s] 78%|███████████████████████████▉        | 22501/28972 [00:05<00:12, 528.38it/s] 80%|████████████████████████████▋       | 23106/28972 [00:05<00:08, 713.15it/s] 86%|███████████████████████████████     | 25001/28972 [00:06<00:04, 905.15it/s]100%|███████████████████████████████████| 28972/28972 [00:06<00:00, 4628.27it/s]
INFO:__main__:  ******************************************* 
INFO:__main__:              Running Final Testing           
INFO:__main__:  ******************************************* 
 
INFO:__main__:  ******************************************* 
INFO:__main__:  Model: roberta
INFO:__main__:  Length dataset testing: 28972
INFO:__main__:  Length dataloader testing: 3622
INFO:__main__:  Number of epochs: 3
INFO:__main__:  Maximal sequence length: 512
INFO:__main__:  Batch size: 8
INFO:__main__:  Learning rate: 4e-05
INFO:__main__:  Learning rate type: linear
INFO:__main__:  ******************************************* 
 
test_separate.py:232: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch in tqdm_notebook(test_dataloader, desc="Testing"):
HBox(children=(FloatProgress(value=0.0, description='Testing', max=3622.0, style=ProgressStyle(description_width='initial')), HTML(value='')))

-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |   1434    |    278    |    434    |    24     |
-------------------------------------------------------------
| disagree  |    178    |    530    |    168    |    15     |
-------------------------------------------------------------
|  discuss  |    562    |    203    |   3857    |    173    |
-------------------------------------------------------------
| unrelated |    63     |    58     |    184    |   20811   |
-------------------------------------------------------------
Score: 21168.0 out of 22597.75	(93.67304267017734%)
Accuracy: 0.9192323622808228
F1 overall: 0.74916610836151
F1 per class: [0.6507828454731109, 0.5408163265306123, 0.8173341809705446, 0.9877310804717719]

real	6m10.175s
user	5m15.942s
sys	2m0.724s
ubuntu@run-gpu-mg:~/fnd_implementation$ exit
exit

Script done on 2020-02-20 21:45:18+0000

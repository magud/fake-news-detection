Script started on 2020-02-20 20:34:44+0000
ubuntu@run-gpu-mg:~/fnd_implementation$ exittime python3 model_grid_search.py --modell=roberta --model_type=roberta-base --dataset_name=fnc_arcM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=r[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=ro[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=rob[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=robe[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=rober[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=robert[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta [1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta -[C[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --m[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --mo[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --mod[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --mode[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --model[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta --model_[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ce.py --model=roberta --model[1@_M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cv.py --model=roberta --mode[1@lM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ca.py --model=roberta --mod[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cl.py --model=roberta --mo[1@dM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_.py --model=roberta --m[1@oM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cs.py --model=roberta --[1@mM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ce.py --model=roberta -[1@-M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ca.py --model=roberta [C[1@-M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C.py --model=roberta -[C[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cp.py --model=roberta [C[1@-M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ca.py --model=roberta[1@ M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cr.py --model=robert[1@aM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ca.py --model=rober[1@tM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct.py --model=robe[1@rM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ce.py --model=rob[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
There are 2 GPU(s) available.
The following GPU is used:  Tesla V100-PCIE-16GB
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/ubuntu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "multi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/ubuntu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/ubuntu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/model_pretrained/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/model_pretrained/pytorch_model.bin
INFO:__main__:Loading initialized pretrained model from /home/ubuntu/fnd_implementation/roberta/model_pretrained
  ******************************************* 
           Freezing Embedding Layers          
  *******************************************
 
name:  roberta.embeddings.word_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.position_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.token_type_embeddings.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.LayerNorm.weight
param.requires_grad:  False
=====
name:  roberta.embeddings.LayerNorm.bias
param.requires_grad:  False
=====
name:  roberta.encoder.layer.0.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.0.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.1.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.2.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.3.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.4.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.5.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.6.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.7.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.8.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.9.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.10.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.query.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.query.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.key.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.key.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.value.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.self.value.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.intermediate.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.intermediate.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.dense.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.dense.bias
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  roberta.encoder.layer.11.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  roberta.pooler.dense.weight
param.requires_grad:  True
=====
name:  roberta.pooler.dense.bias
param.requires_grad:  True
=====
name:  classifier.dense.weight
param.requires_grad:  True
=====
name:  classifier.dense.bias
param.requires_grad:  True
=====
name:  classifier.out_proj.weight
param.requires_grad:  True
=====
name:  classifier.out_proj.bias
param.requires_grad:  True
=====

*******************************************
Evaluating the following pipelines 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/', '/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/']

*******************************************

*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:00<2:54:46,  1.13it/s]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<1:57:12,  1.62it/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 2001/11898 [00:01<1:11:15,  2.31it/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3001/11898 [00:01<44:50,  3.31it/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4501/11898 [00:01<26:06,  4.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:02<15:50,  6.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 8501/11898 [00:02<05:53,  9.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 3996.63it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 372
  Number of epochs: 3
  Batch size: 32
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/outputs/checkpoint-3/pytorch_model.bin
eval_separate.py:214: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch in tqdm_notebook(eval_dataloader, desc="Evaluating"):
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=372.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.1290990.1449650.2600710.0875360.0220210.0046280.0009540.2430980.1650690.0741100.5918540.2754490.0233610.1509930.6504290.0435600.0028592.2316800.5860910.0710250.0041320.0003530.0001570.0004040.0002600.0004970.0003900.4253670.0176100.0007370.1463010.0048540.0008800.2343680.1562600.1111420.3275970.0091820.0007290.0004610.0021160.0005570.0001430.0001340.0001840.0001350.0001790.0003150.0001410.0009520.0006280.2039590.0043110.0002660.0002470.2032610.0046930.4380680.0111650.0022620.0007920.0977500.0020810.0005900.0004380.0093430.0005000.0003380.0002490.1369440.4611650.2029450.0039790.0003590.1488080.0024160.1455490.1867720.0026620.5080590.2190300.0050510.9630900.7162250.2204050.9026400.0113761.5090930.0175910.0004820.1136690.7343730.8876770.0330990.0685020.0860040.0863920.1244810.0024851.1486690.0122050.1697980.4978710.1600430.7883560.0080190.3347140.0419100.2217210.5195330.0168120.3222060.1804560.1595000.9161531.2144620.2669250.0030160.2548670.1383340.5104450.0069370.0002410.0830310.0008000.0293120.0032760.0021390.0174100.3097770.0334350.1856580.0189680.1147500.1386220.3453280.4451030.0034340.2180100.1162650.5279520.0043470.1661720.8660681.7347080.1419880.5762880.0049910.0006290.0698910.8354450.8107940.0056290.0001660.0004700.0645290.1110610.1128760.0136490.3228930.0050390.0004270.1786000.2074000.0014950.0016320.2809940.0021890.0010160.0013460.0013262.4891110.0176520.0009250.0983140.4593560.1961230.0022760.0037710.0005160.4760390.3788020.3020990.0544730.0019780.0078730.0140500.0005190.0114530.0073890.0151471.0279730.6174070.0086770.0006050.0006141.0928621.3992900.4094590.0022760.1470530.2162430.1637200.1395980.0033101.3475110.5381070.0051200.0004890.0004940.0071030.0037660.0006010.0001891.5497920.9122770.0124990.0034660.9046860.3645340.1593030.0022400.9100630.3868980.0026250.0012230.1492580.0280111.0442930.5742150.2151750.4633090.2064590.5070690.9484140.5728860.1291480.0010490.0002930.0407710.0702570.0560190.1602010.0015370.3875690.1631750.5289320.9282110.0045980.0006420.0747480.5884310.3420820.2448880.0018470.0030400.0010170.0009730.2199400.0041170.1118700.1808920.1173910.0048190.0015340.0772720.2071471.1648490.0072680.0008970.5466570.5198710.8251240.4605650.0035030.2549330.0437190.0047940.3281781.3502970.0095990.0038860.0022860.2371950.5882210.1785610.8413010.7105770.2556540.3089920.5151980.5544710.5234890.5011930.5200070.2291580.3686860.4542570.1963680.4475880.2869510.2755480.6387980.2892630.4704420.5566360.7112510.2896860.6178040.3274240.3316180.2816650.2035420.4367610.5099030.2684250.4528070.5908670.2744720.5096640.2412190.4155140.6900830.5892750.8813280.3998530.3393300.3716800.3036850.3015040.4277420.2403740.4919500.2998230.3139180.3365610.9684670.1663370.1921060.4544760.3237460.4191700.3056750.6222260.3820751.3330150.1632930.2510460.4156320.3357271.0428020.1369510.2306190.4772400.3881230.5405590.7394010.6008990.2596140.5139420.3833470.3553310.4487000.6566880.5677790.3072140.6736830.7019430.1617650.2105530.6219410.442835

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/
With following parameter combination: {'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9eeee9a8_2020-02-20_19-38-598ivla8p_/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    722    |    77     |    137    |     7     |
-------------------------------------------------------------
| disagree  |    91     |    269    |    66     |    11     |
-------------------------------------------------------------
|  discuss  |    203    |    92     |   1513    |    65     |
-------------------------------------------------------------
| unrelated |    21     |    30     |    63     |   8531    |
-------------------------------------------------------------
Score: 8707.75 out of 9226.0	(94.38272274008237%)
Accuracy: 0.9274668011430492
F1 overall: 0.7852352021098534
F1 per class: [0.7292929292929293, 0.594475138121547, 0.8285870755750274, 0.9885856654499102]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:00<2:56:49,  1.12it/s]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<1:58:35,  1.60it/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 2001/11898 [00:01<1:12:05,  2.29it/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 2501/11898 [00:01<47:56,  3.27it/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 4001/11898 [00:01<28:12,  4.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:02<16:00,  6.66it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 8501/11898 [00:02<05:57,  9.52it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 5310.79it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 744
  Number of epochs: 3
  Batch size: 16
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=744.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.2239940.2273320.1153590.4824490.4385370.0893380.0178280.0045820.0020400.0017030.0019250.0016710.0017690.0016490.4325860.4167610.1380330.4377490.0258540.5739970.9779621.3159970.4896470.0247030.0041610.0016530.0061050.2861491.1792490.0521650.0032190.0017110.0015240.0015092.9531482.6624300.8895320.0256410.0024570.5976940.0169720.4365780.0123030.7904100.4571630.0121050.1108440.3657930.6152740.0144020.0025100.0152670.0031740.0067020.0059260.6516050.0131260.2999920.0066320.0015810.3448970.0071620.0015970.0015500.0062590.0029990.3767960.0072910.0028680.0015670.0318670.0186240.6327330.0104270.0018940.0018750.1126070.0035080.0019910.0025240.4440190.0097970.1874210.0037000.0015020.0014730.0014740.0014910.0014750.0016270.0014670.0014800.0016180.0018930.0169160.0023690.0019100.0017800.0027420.0027800.2956180.0057760.8179190.0102310.0025670.0019560.0016580.0014610.0018780.0014580.0014580.6727680.0247350.1243680.0113540.4301970.0067690.0018750.0026300.0457700.0021250.0034380.0019170.5822530.0075340.0019120.0596320.0028740.0015940.0028190.4404030.0065610.0016470.0025590.0016460.0019240.0014750.0017700.0017890.1474050.5867850.0062360.3957850.3293480.3765610.0042150.0017000.0019510.0034390.0023970.0020890.0020880.8374581.2015000.0095550.3850600.0040760.4464570.3866010.7097790.3903260.0039040.0015980.1781230.3475850.6860660.0161811.2835790.3829610.1157870.0988030.0043340.2348250.0036000.2478311.2107490.0089430.0021530.0020870.0014820.5636330.0054900.0017850.7399300.2639630.2764070.2996610.0040150.0035150.1158440.0027110.0022310.0614900.3339170.0886610.0024740.4295270.0067130.0020361.9293650.0119660.0025220.0030780.0027870.5580390.7471820.3518490.3660930.9780371.7254350.0102060.0020950.3834800.6947720.0087340.2925850.5159901.4363210.5153290.4826980.0088550.0030780.2961250.5864280.3039340.2854850.3821540.0031591.1590410.7750270.1085390.9472850.3963860.0066960.0049330.0340150.0082820.0106600.0016230.0091591.1073050.2297580.0036400.0712840.2532220.0025730.0022940.0036390.0091490.0015870.0038900.0072000.0093381.2575060.0097320.0042790.0120610.0031050.4268050.1568390.4167130.5955760.2718370.0066120.0131210.2948760.4295170.0102130.3222510.0047500.3411950.7776711.1346030.6558930.0041220.0015790.4303920.0030480.3180200.4456840.6014360.0040310.0020100.0023340.2990780.2571570.5112671.3379831.3490461.3795850.0081560.3211430.0026780.6798270.0045120.0025930.0021970.0023910.0020700.0043610.0027680.7148440.2331260.4079050.0161870.0017460.0017250.0017610.0145590.0153040.0290881.9172111.6027910.3709631.6370880.0076970.0022340.6076050.4623280.0107350.1467180.0028130.0024510.0015700.0014950.3636180.3899950.0029540.0016650.0016310.0023060.0068670.6317890.4309230.0029050.0022370.0020930.0028320.0028280.0036770.4317300.0059072.8959761.3101420.3183550.4773190.0040200.0022211.2313640.2720280.4582460.4141711.0877520.0062650.0026810.0020820.2349450.0032420.0077320.0118560.0373570.6885740.3480950.3468080.4371950.0028390.0487020.0297740.0024900.0048580.0429540.1581320.3045350.0026360.0029380.0015540.0023460.3428470.3176830.0140220.2848880.3811220.7464080.4201120.3198590.5417290.0035990.0479970.1900960.1025320.0028720.0021221.5434271.2434322.2611481.1075920.7313130.3185870.0024380.0016520.3166160.3192940.0050830.3178620.3184890.0023090.3164060.0026950.0041530.0516700.1502841.5643130.9740830.7426350.4755060.0064620.0019510.0018080.0018040.0019540.0140901.0908940.2984570.0023030.0020440.0015010.4305130.0025260.0016222.8623421.8581260.3905730.0044410.3906160.3617920.0053780.0066270.3584470.3518271.5620030.6478060.0058910.0026190.3515220.0045361.4810950.4903460.0031430.0028240.0023820.0032640.0026690.3470380.0064830.0035050.0078341.1892501.0283920.5738730.6183370.6538030.0332820.6844950.0103750.2593890.3673640.0213170.0137981.0519040.4492390.0025741.1901502.3670500.0095370.0019430.0019970.0020530.0015850.5091100.5521821.3626060.1117790.0023590.0407940.0034130.4297260.0035390.8550740.0032111.5882560.3159710.9002250.7741130.6925022.3426991.0810550.0060440.0028100.0022460.0024510.4256780.4173290.3795061.0434020.1750120.3714260.3225440.0029800.0220030.0017940.0149550.0087010.0087730.0035000.0024170.0032590.2950370.3344550.2658540.0043440.0100370.0070690.0049390.8623060.3958870.3302840.0073640.0080080.0032510.0037510.0057010.0017680.4300130.0058890.3213260.0047290.3471641.4526900.4569600.3160171.1914840.0500960.8827560.3115440.5183250.8168870.5215671.0124870.0039060.2947671.4119671.7091290.4215160.0035300.1738960.4086180.4315660.6522680.0036702.4820420.0110800.0046120.0111860.0057620.0049680.0058480.0244960.5071790.8251610.4989530.3805670.4158620.7551490.7091320.8493870.7028641.0335900.4653730.7324691.0132840.7280261.3240331.2256511.2410181.1752690.4995530.6559360.2703590.6949620.4827890.4505150.5640090.2385050.5885321.1007030.1248330.0248310.3646720.2881270.7129070.5701740.4812380.1117260.6703940.2827560.9906000.1279720.6022090.1459720.9523110.4473130.5795080.9407230.5399730.3603370.3065830.6187850.4293480.5729880.4209920.2702020.3551310.3260140.8131350.2909910.2020670.5452792.0462470.6761460.3517630.4558020.2700080.6328490.9433740.7736080.5258280.3413930.1256440.8436410.7450950.1428460.2191840.5517340.1239151.1328950.7196191.1695620.2966890.9173440.7457631.5093430.5558640.4155500.3485170.7334480.3089430.9424730.3387590.6785510.8400860.9084481.2830060.6613800.5376440.4477480.2467350.3006200.1587930.4530940.1899360.0160690.5814940.8003310.9542830.2116120.1656680.1433370.6226200.7065710.5790070.6303810.6429260.7267590.3548210.4056120.7670610.1080161.5665250.6183870.2787061.0629991.3240230.2166110.2349560.0512060.3014380.2216180.7539650.6843440.3165150.9200891.0416930.6362940.1090890.5457030.2058780.4974750.4210810.5912190.2239191.2298460.4717551.6127191.1921480.6006810.4196460.2207380.7874100.2846630.6340310.9775570.6064580.3980291.5093660.9270941.2097010.7655160.2045761.0176460.7683290.4254890.1812240.7136111.6167330.7350780.5614000.3684760.5421860.0233620.3823740.8362710.3819610.0151471.003142

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/
With following parameter combination: {'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_ebd7ce00_2020-02-20_10-54-26rxehd0sl/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    744    |    90     |    188    |    29     |
-------------------------------------------------------------
| disagree  |    86     |    232    |    68     |    18     |
-------------------------------------------------------------
|  discuss  |    163    |    93     |   1449    |    108    |
-------------------------------------------------------------
| unrelated |    44     |    53     |    74     |   8459    |
-------------------------------------------------------------
Score: 8569.25 out of 9226.0	(92.88153045740299%)
Accuracy: 0.9147755925365607
F1 overall: 0.75816037913362
F1 per class: [0.7126436781609196, 0.5321100917431193, 0.8067928730512249, 0.981094873579216]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:01<3:19:49,  1.01s/it]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:14:01,  1.42it/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1501/11898 [00:01<1:25:35,  2.02it/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 2501/11898 [00:01<54:09,  2.89it/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3001/11898 [00:01<35:54,  4.13it/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4501/11898 [00:01<20:54,  5.90it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:02<12:40,  8.41it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 8501/11898 [00:02<04:42, 12.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:03<00:00, 3959.05it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 372
  Number of epochs: 3
  Batch size: 32
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=372.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.2103630.2108540.2901490.1474800.0369490.0075690.0013740.3105360.3384440.1301340.8672640.2825390.0239660.1925280.7424620.0496780.0031842.0018370.5693310.032187
0.0865860.0042570.0002790.0006620.0001780.1243280.0055310.5587380.1921900.0067070.1542540.0051050.0014980.2253970.1607110.0054020.0030490.0003150.0003430.0003050.0015370.0009940.0001030.0000820.0001280.0000820.0001220.0005150.0000900.0006940.0004840.0776050.0017900.0002430.0010470.0928660.1551500.3694630.0069140.0011700.0791620.2774060.0049360.0003090.0003790.0019590.0003260.0002460.0001710.0491500.2319700.1919060.0043190.0002880.0023370.0004220.0908510.0439090.0007630.5554770.2217860.6754340.4446701.3414370.5951751.4546710.0177161.9681660.0227030.0004560.4248140.0081430.0699820.0085070.3576120.0089600.0007600.0341740.0018331.1756150.0122500.0118360.2835300.0049250.6894740.0069260.3284040.0341770.2144860.3522000.0058860.4870990.3276570.1812070.9677021.3814260.3165260.0031660.1507000.1384950.0157000.0009470.0001930.0013430.0000900.0064870.0071100.0033230.0332950.0206890.4383260.9037670.0765870.0271250.0659380.2832660.1992690.0015760.2158250.0630620.2453450.0022130.0048850.4803560.6008520.1869150.5123840.0977970.0010670.0007980.6165780.7030930.0048820.0001110.0004531.0941710.9056350.1796370.0050040.1135390.0060030.0003250.1215040.1769810.0012320.0011170.2642480.0141270.0014980.0014930.0018512.8477160.0199800.0006990.1333390.0171830.0358300.0091640.1469040.0012610.4238490.3458620.3101240.0196750.0016070.0090320.0014320.0008650.1209780.0039700.0050451.0430620.6471010.0050540.0004080.0004460.6263210.8340820.3472600.0018960.3044820.2541580.1788240.1811420.0041071.0838450.7275810.0083340.0004260.0003540.0259700.0023680.0003620.0001211.5661510.5890810.0171410.0084701.6656801.1464290.3233540.0023800.9316760.3154450.0023010.0280890.1355520.0222671.1582720.6586350.0081840.1165510.1724610.2263970.2871890.3883370.1823390.0010860.0001730.2551710.5514220.0639670.0622700.0009410.3271730.1442030.5891590.8745580.0042060.0004320.3534650.7659090.3917890.1447180.0021020.0024930.0033990.0006840.2279040.0018390.0022770.2821820.0059520.0015060.2106400.2594610.1984931.5535670.1712380.1652730.5272440.6193130.7830290.6396100.0040980.2576880.1293970.0761580.1960521.4191180.0068100.0388330.0020920.2028070.6842070.2100670.5252750.6336140.2789750.5094620.4053910.7842800.5580860.5235070.6062050.2609210.3192900.5267110.2623990.6414270.2851340.3217130.7520420.2145440.6005610.7181530.7860760.1602210.4167250.2318630.3934790.2910530.3619280.4715200.5477470.2763200.4267270.6609910.2757930.4538410.3957000.2271780.4421480.5364901.0765690.3946640.4935840.5187720.4994050.2050990.2268070.2024540.4390990.4153360.3258980.3387021.0027080.2100170.1853360.8270810.4656760.6265890.3738960.4181530.4010601.2272260.1648750.1666870.6747780.4562090.8543660.2105190.3020010.4724810.4213480.6795161.2460720.6085850.2463090.5256590.2734220.3815070.2839470.4582590.4713830.3141050.9442930.6585740.1897800.2896630.3984610.543363

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/
With following parameter combination: {'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fa393b78_2020-02-20_13-53-53gdbhx0ep/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    722    |    66     |    127    |    12     |
-------------------------------------------------------------
| disagree  |    119    |    315    |    87     |    16     |
-------------------------------------------------------------
|  discuss  |    177    |    55     |   1499    |    60     |
-------------------------------------------------------------
| unrelated |    19     |    32     |    66     |   8526    |
-------------------------------------------------------------
Score: 8727.25 out of 9226.0	(94.59408194233687%)
Accuracy: 0.9297360900991763
F1 overall: 0.7974991401202445
F1 per class: [0.7352342158859471, 0.6268656716417911, 0.8397759103641457, 0.9881207625890943]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:00<3:02:49,  1.08it/s]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:02:36,  1.55it/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1501/11898 [00:01<1:18:18,  2.21it/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 2501/11898 [00:01<49:33,  3.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 4001/11898 [00:01<29:09,  4.51it/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4530/11898 [00:01<19:03,  6.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 6001/11898 [00:02<10:40,  9.20it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 5565.72it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 372
  Number of epochs: 3
  Batch size: 32
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=372.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0017660.0024870.1216100.2584130.0651320.0136810.0028730.8444440.3114650.2462641.0959470.3054760.0263430.2637600.5610960.0379940.0028981.3246580.4980720.2194230.1975290.1962240.2015980.0267060.2623850.0115740.0013240.4357880.2274900.0083700.2235460.0077280.0017650.2230220.1892360.0069460.1891020.0058900.0011850.0010140.0044720.0010890.0005510.0005380.0006010.0005380.0006010.0008380.0005430.0016460.0011420.2716110.0065510.0007250.0006660.2955740.0091290.3374010.0071070.0010550.0011560.0028240.0010240.1104080.0028120.0385610.0013950.0007960.0006620.1083300.4748410.2176480.0048730.0008430.3503570.0057400.3669290.1723890.0029260.6517120.2314760.0055101.6458112.5176350.7627710.3933250.0062021.6956720.0203450.0010240.1774220.0650920.3123980.0064620.7656270.3419680.3560630.2504730.0059091.1150870.0124700.0061790.6317950.0095960.8716070.2127560.3579690.0091000.2272520.4175920.2943260.4914960.3028480.2263361.1160101.9804550.2632740.0034730.1919580.1835620.0118330.0013300.0006050.0019670.0005460.1702600.3403840.2141540.0153710.1819500.0039691.1124860.3063760.0082650.1872980.5822571.0051570.0079260.2157010.0025260.0010450.0011030.1782840.4412770.7699360.1877250.1945510.0026860.0011830.3753541.0099200.4190830.0035690.0005500.0010980.5760800.7331050.7823840.0079820.0135880.0047790.0009320.2045710.2255730.0020370.1266260.2167300.0021390.0013230.0020680.2982791.8948710.0129570.0014140.3499200.0168210.0019820.0014880.1864970.0021380.6285040.3317010.1684820.0775660.0036070.0164250.0027650.0010370.0713650.0172680.2492791.1161090.6836030.2183930.0022220.0011731.2592181.8970240.4265220.0027940.0007590.0008030.0006210.0007430.0035440.8323760.6499250.0104100.0009820.0009620.3311680.0066690.0008090.0005951.6536371.0083320.1958840.0479361.7756282.7446360.3423660.0035790.8927870.3756340.0030370.0018950.2659670.0024691.2797630.6823320.4215410.5003820.0110840.1808660.4546360.5983900.5572290.0031440.0006740.3204490.3876780.0566540.2256860.0105620.3603710.0033840.5393360.7705030.0048540.0011810.4027950.8488540.5598550.3708600.0026860.0014170.0016870.0318250.1625510.0023120.0050350.2341340.4562220.0034720.6712780.1300380.2136701.5321740.1070400.0049740.9884100.8761710.8183010.8515060.0061060.2888560.0030740.0045500.2821290.2725220.0054370.0046910.0040110.3217291.3081890.4094940.9031520.9878321.1464281.1091940.7845891.0247540.7240420.5226810.5962950.3601480.4074261.0363750.2586490.9281250.2260700.2906900.5002210.6559290.5381880.9558821.1957560.2186090.7071790.5998260.2519380.5808080.2803090.9586050.6723080.4306840.8740710.9275850.4853630.6172690.7576150.3615221.0986390.8000600.8352430.4285080.6361060.6076430.5894690.6836560.6962640.3329720.8226940.3469710.5666800.6104101.2320880.1442870.1432540.8944020.4781500.8213910.9463621.5472350.7946851.4874870.2353680.2834810.1926770.4404961.1372610.3745730.5302670.7930730.3366480.7335911.7954960.5497720.5124680.3302740.7701050.7373900.7616340.7488750.7102970.3473781.7693540.6863070.3530230.4240970.7763860.735795

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/
With following parameter combination: {'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_eec4fad4_2020-02-20_11-13-29kr0dt82u/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    703    |    105    |    122    |    35     |
-------------------------------------------------------------
| disagree  |    101    |    269    |    78     |    28     |
-------------------------------------------------------------
|  discuss  |    206    |    61     |   1507    |    46     |
-------------------------------------------------------------
| unrelated |    27     |    33     |    72     |   8505    |
-------------------------------------------------------------
Score: 8674.5 out of 9226.0	(94.02232820290483%)
Accuracy: 0.9231803664481425
F1 overall: 0.7739244001195547
F1 per class: [0.7022977022977023, 0.5699152542372882, 0.837454848569047, 0.9860297953741812]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:01<3:51:27,  1.17s/it]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:35:14,  1.22it/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1501/11898 [00:01<1:39:08,  1.75it/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 2501/11898 [00:01<1:02:43,  2.50it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:02<29:54,  3.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 6001/11898 [00:02<19:18,  5.09it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 4185.16it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 2975
  Number of epochs: 3
  Batch size: 4
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2975.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
1.2940631.2941450.6471550.2229610.0705520.0142070.0040540.0006620.0001670.0001000.0001760.0022870.0202790.0016440.0002010.0000970.0000900.0000890.0365840.0020080.0001830.0001310.0000880.0000880.0000860.0018340.0001530.1637310.0059290.0002870.0000920.0057330.0200750.0006900.0001020.0000850.0000850.0001440.0000870.0000870.0017200.0001240.0000850.4371890.0100190.0003060.0000900.0000840.0113410.0003150.0000890.0000850.0000840.0000840.0000840.0000820.7270581.2177291.1843541.6702360.0295160.0005650.0001153.3033720.0516960.0008761.7236411.6785071.6757240.0243670.0014660.0001020.0000821.7222760.0233550.0003931.7211011.7497751.7179690.0218291.6909711.7207440.0210651.7017921.7276541.7487901.6929090.0195400.0019080.0001041.6697330.0184310.0002840.0192240.0002950.0000920.0121440.0002080.0000840.0000850.0000820.0000820.0000870.6225320.9987660.0095940.0001750.4268070.0040440.0001210.0000851.4868450.4549210.4963520.9846230.0211190.9891150.0085360.0001540.0000830.0000830.0000830.0000820.0000830.0000830.0000830.0016770.0000950.0000830.0000830.0000830.0000830.0000850.0000820.0000830.0000833.9217693.5329273.0639780.0221251.7419913.5276091.7849932.5714521.7694831.3981630.0096580.0001480.0000870.0000850.0000820.0000830.0000830.0000990.0017510.0017420.0000941.2651820.0080970.0001570.0000890.0000900.0000920.0000890.0000890.0000910.0000920.0000900.0000900.0000920.0001020.0000900.0000970.0000980.0000900.0000890.0068350.0001260.0000920.0000890.0000900.0000990.0000910.0000970.0000950.0000910.0000890.0000880.0000870.0001020.0073700.0001810.0001270.0002170.0001120.0001010.0000920.0000830.0000840.0034060.0033220.0000990.0000820.0019360.0525390.0049011.0710890.0052870.0014751.0790030.0069360.0001180.0000830.0000870.0001020.0000840.0000880.9201170.2747300.0094550.0079660.5466290.9430160.0043110.0001010.0000830.0000830.0000830.0000840.0001121.1870940.0052210.0001050.0000820.0000820.0000830.0000830.0000820.0000830.0000820.2235510.0010090.0000880.0000890.0000880.0000870.0000940.0000930.0000880.0000880.0000900.0000870.0000870.0000900.0000880.0000890.0000880.0000881.6161780.0063270.0001120.0091770.0105690.0001210.0032830.0000940.7402610.0070570.0001110.0016901.6264850.0060830.0001030.0000830.0028930.9165710.0034040.0000940.0000830.0000820.0073520.0146570.0001340.7521740.5304120.0019430.0000880.0000840.0000820.9476621.0082620.0035460.0000950.0016800.0000890.0016930.0017620.0000890.0016960.0000880.0017310.0017650.0000890.0000940.0000830.0000830.0033450.0034800.0018210.0016820.0000870.0034150.0016880.0000860.0000810.0037560.0000940.0000830.0072170.0001100.0046370.0143590.0001351.0053710.0335100.0082180.0001060.0037400.0017730.0054030.0275070.0020120.0000890.0000830.0000850.0000830.0000830.0000830.0000830.0000850.0000830.0000830.0000820.0000830.0000820.0000830.0000840.0000820.0000830.0000830.0000830.0000830.0000820.0001180.0000830.0000830.0017100.0000880.0000830.0000830.0000840.0000840.0000820.0000830.0000830.0000830.0000830.0000840.0000820.0000830.0000830.0016920.0000870.0000840.0000930.0000950.0078260.0001130.0000930.0000920.0000930.0000950.0000930.0000950.0000950.0000930.0000940.0000940.0000930.0000940.0000970.0000910.0036580.0017330.0071510.0018370.0079770.0001110.0077880.0001070.0000900.0001020.0000890.0000890.0165520.0110900.0017380.0033541.6729381.1942161.1930840.0046300.0000950.0000810.0018240.0021340.0033750.0019210.0000860.0036830.0033320.0000890.0000830.0000820.0019000.0000860.0000820.0000820.0000810.0000820.0000810.0000830.0000820.0516220.0022180.0000870.0000820.0000820.0000830.0000820.0000820.0000820.0000850.0000810.0000830.0033670.0394360.0079200.0101030.1217910.0003600.0085870.0003120.0001030.0001000.0258510.0097981.0787920.0024380.0021610.0000921.7392950.0057990.0025230.0000920.0000840.0175041.7078480.0037320.0016830.0016810.0000850.0000820.0000840.0000820.0052070.0057030.0000940.0017800.0000850.0000820.0000820.0017390.0017120.0038560.0026631.3151980.0027890.0000930.0000870.0000870.0000870.0000870.0000871.2776001.5984310.0033130.0032860.0017460.0033980.0036370.0001160.0000920.0000930.0000912.0043190.0040530.0002860.0010170.0000960.0076310.0001070.0000940.0000930.0000930.0000920.0004410.0000920.0162030.0152860.0001290.0000860.0037630.6939380.0014300.0001060.1345510.0003650.0001090.0001000.0001100.0001050.0000950.0112240.0001140.0000880.0022120.0000860.0000820.0000820.0000820.0022140.0021360.0019290.0000860.0000820.0000820.0000820.0000820.0037520.0000880.0000820.0023380.0000870.0000810.0024830.0000871.1388900.0024760.0000921.1364202.5379400.8986450.0016790.0000890.0000850.0024870.0000871.1986361.2345110.0022470.0016910.0016780.0000841.2421653.7324030.0161840.0076670.0166370.0018640.0000840.0000810.0024490.0000850.0000820.0016890.0000830.0000810.0000810.0038920.0019880.0000850.0000800.0391620.8891740.0707680.0019310.0110490.0091270.0001010.0000860.0000870.0071880.0000980.0000870.0072750.0000990.0000830.0033570.0000870.0000811.5791371.2998451.5532030.0026111.2870540.0037760.0020060.0000840.0000811.3053440.0021830.0016870.0000840.0000820.0017060.0000840.0000820.0000820.2085670.0102800.0000980.0000811.2609910.0020700.0000851.2272900.0020081.2124360.0019780.0000840.0022131.2404660.0020110.0000840.0000810.0000810.0000810.0000810.0000810.0000810.0018441.6495690.1018460.0077620.0076150.0073880.0001010.0520180.6364470.1047550.3432640.5253600.0085470.0093290.0033170.0085780.0049670.8134210.6505942.1330931.4167980.0054080.0000931.3661480.0021070.0658190.0667360.0461100.0001510.0792371.3948250.0021280.8497141.1802490.0018150.0000940.0084010.0159620.0017930.0017350.0017170.0017160.0033170.0000860.0033480.0017110.6622610.0203140.0354722.6184042.4400760.0063460.0026550.0000890.0000850.0000840.0071990.0000940.0000880.0071440.0000950.0000840.0000850.0000840.0072160.0000960.0000850.0000860.0000850.0000870.0020240.0018020.0027130.0000880.0000840.0021110.0040130.0020780.0000890.0018170.0000860.0000841.5572831.2101612.7698520.0038450.0000861.2159201.2256220.0017372.7409321.0730690.0045340.0000880.6114410.0686300.0305060.0001240.0000810.0339730.0001270.0000830.0000820.0000823.5432300.0083890.0000930.0190320.0001100.0120800.0000980.0000830.0000830.0000830.2158240.0003660.0000830.0000830.0000830.0000840.8003010.0093150.0001040.0000880.0014230.0000920.1312120.0313210.0019260.0021250.0026020.0017270.0000830.0000811.6609440.8247720.0047520.0000870.0000811.6338600.0021460.0028140.0000840.0017850.0019262.5472383.8949201.1492510.0292810.0017670.0036570.0020020.0034600.0016900.3731902.9068752.6564461.7535130.0101700.0039260.0086931.5861880.0054051.3029851.6464023.3649971.1924440.0409420.0331183.1711620.0039570.0000950.0000920.0000920.1336320.0082660.0001021.4711430.0096300.0001090.0000920.0080870.0078050.7957300.0010450.9940970.1585810.1106950.0002320.0002480.0000910.0036970.7048160.0220400.0001110.0000840.0046910.0021230.0000860.0000841.1214860.8241002.0355600.0374080.0066040.0076340.0223770.0243490.0071160.0056120.0070561.0089783.1220861.0128561.0008640.0012421.5708131.3894320.4505610.0022671.3146190.3646870.0049760.0010050.0000951.1163561.1105640.0013570.0000940.9520540.0155621.0382180.0115480.0048950.0026870.0037350.0041610.0232190.0136160.0303190.0125250.0018460.0068430.0021860.0022100.0017710.0075690.0029490.0060100.0018580.0049040.0000880.0000811.2713420.0014870.0019450.0000830.0000820.0000820.0000820.0000820.0000821.2674072.6349050.0064720.0000880.0022821.2861841.2800271.7283510.0743960.0544060.2214720.0065180.8860471.2525931.6350680.0034420.0216270.0239770.0102010.0021450.0070200.0000890.0038200.0077650.0000950.0123050.0024390.0053300.0029510.0000870.0053080.0592390.0001480.0382553.1789714.4179251.1068151.2537510.0014050.0000950.0000820.0000830.0000820.0036100.0000870.0024561.0417840.0011680.0000830.0719201.2314301.5116740.0112080.0053110.0031490.0020450.0023840.0076790.0114140.0735330.0001550.0000800.0000800.0272410.0002980.0000860.0000830.0000840.0000860.0027350.0000900.0001041.0286620.0011260.6764280.0007680.0043812.1208890.0444100.0001670.2856540.0003750.0001060.0000980.0001330.0000860.0001020.9962281.0327560.0078630.0148410.0001040.6359971.2659350.0013550.0001010.0000970.4001030.0004930.0125730.0463190.0116880.0000980.0017330.0151540.0077870.0151150.0001000.3806250.0135410.0000981.8061122.5041211.0583391.2015360.0061540.0049400.0033420.0049671.6801300.0033620.0031690.0000900.0000870.0000870.0041962.7145480.0159330.0072160.0074750.0086430.0083290.0072860.0076540.0000940.0072540.0074790.0000950.0000862.4755712.6061841.2644170.0030381.2465811.1118111.0604502.2257962.3112580.9505840.0044250.0017890.0034740.0035820.0072180.0313831.9937560.0331320.0085050.0259560.0001080.9256080.0114130.0000980.0160640.5755510.0006180.0251122.1418600.0051240.0018330.0000820.0000801.3014061.1465570.0042790.0000841.1557040.0035522.7236622.3575570.0022380.0000830.0000820.0000810.0016760.0000830.0000810.0000810.0000810.0000820.0000821.6591920.0015830.0000820.0000810.0000810.0000820.0000810.0000810.0000820.0000810.0017440.9646360.3637820.0004070.0026030.0037800.0022410.0047480.0043830.0000850.0016870.0000830.0033090.0000840.0032930.0017090.0000830.0001230.0032960.0017270.0033670.0016910.0016840.5078990.0005690.0091170.0001740.0001010.0095152.2383911.5688754.6761834.4189683.3967202.3492981.4497840.0013420.0016860.1566472.4715483.5869820.0128501.4060700.9031172.1019711.1267400.0010620.0074750.0074390.0000970.3486890.0057730.0051090.0000870.0000840.0000820.0000840.0000850.6333111.5078070.0315510.4912840.0162140.0033010.0000850.0018680.0016980.0033940.0021300.0039640.0000840.0037780.0021480.0037790.0040330.0044010.0021950.0000830.0034800.0019660.0000820.0000860.1368100.1601820.0019610.6834390.0006500.0000810.1434680.0602710.8267602.4824771.2240961.5794780.0064190.0067591.3066732.7209901.5980950.0093270.0000970.0000900.0000900.0000900.0075510.0000940.0000890.0000890.0000900.0000910.0000900.0000890.0000890.0000890.0000870.0000900.0000900.0000920.0000900.0000890.0076030.0000960.0000940.0000880.0000900.0074490.0074630.0000961.8414621.7710713.0459323.0534051.0055952.0429873.0492492.0162170.0016990.0073550.0072120.0071800.0000921.5470230.0093173.1250450.0102074.4663020.0071070.0034540.0034020.0017490.0000830.0035020.0034680.0017730.0016950.0035100.0000870.0233370.0229250.0081880.2590660.6494970.0826941.6318781.2783052.9112833.9479431.6040901.2619210.0084960.0049660.0016990.0033050.0000860.0000830.0033240.0065420.0000880.0000830.0000830.0000850.0000830.0000820.0000860.0000820.0000830.0000830.0000860.0000831.2866041.2693510.0052890.0000860.0000830.0000820.0021480.0016760.0000850.0000820.0000840.0000820.0016960.0016830.0000840.0000830.0000820.0016800.0000920.0075210.0000960.0077360.0000991.1016370.0084610.0001000.0709252.2112260.0052750.0000861.7163390.0084380.0064230.0000860.0028040.0000820.0000810.0086330.0000870.0030460.0000830.0000810.0067140.0059430.0000850.0000800.0056620.0113690.0092390.0030830.0033510.0074410.0065600.0066480.0066760.0065990.0054630.0022160.0001250.0055760.0042780.0017660.0020030.0057330.3818731.4477502.4518402.2047642.1596292.4438482.4897720.0167880.0108780.0087020.0049310.0017330.0173890.0222880.0019520.0037960.0000870.0054610.0054290.0018460.0022690.0000860.0052940.0020690.0018240.1403520.5802150.7773870.0006400.0069110.0025781.3637400.9298061.0738160.0297110.0289451.0340340.0082550.0089960.0102580.0027730.0051150.7356800.0069500.0077130.7057691.2292610.7614990.7246050.7466230.6311070.2595881.2516880.2286180.1337870.0001750.3770830.0003450.5845740.5359931.1450800.0009140.0099460.0001030.0000920.0000930.0000940.0086570.0001040.0000960.0085660.0001060.0001010.0087450.0001040.4586050.0004111.1000631.0614070.0008260.0000930.0000930.0000900.0000921.0968190.0008481.1027240.0008620.0000940.0000911.0919180.0008640.0081340.0000980.0000920.0000940.0000920.0000910.0420881.1108710.0028090.5245370.0281770.1358170.7869470.9833550.5992170.8084740.0141970.6952680.1088650.0001581.4266660.3340780.4296981.8876242.7149641.4308030.0010471.0128730.0007650.0000862.9439780.0020630.0000890.0000850.0000850.0000870.0000851.3960390.0010130.0016950.0000820.0000810.0000810.0000810.0000810.0016830.0000820.0017371.6981803.2922280.0022640.0011750.0000820.0000820.0017241.3793460.0009920.0000810.0000810.9081760.9532521.1343690.6361541.5582451.2761885.2881953.9189221.3066510.0074770.0470550.0627970.0527600.0001150.0753330.7131490.0005460.0000840.0000820.0000810.1423270.0001750.4744280.0003890.1429470.0001770.0022940.0000850.0088690.0023000.0000850.0017330.0001080.0001100.0036180.0017410.0021210.0017260.0033550.0000930.0019340.0017290.0016950.0051940.0000860.0040740.0000860.0037580.2644710.0021861.5312171.0115270.0531980.6290900.5252190.7580440.6209130.8070811.1328690.0842180.2057800.1698440.0001880.1200460.0028241.2180200.6465870.0004890.0000810.0000810.0000810.0171240.0000920.0000810.0000820.0000810.0017100.0000820.0000810.0000810.0018510.0000820.0018520.0100320.0000880.0000810.0000810.0000820.0000810.0061440.0000850.0000910.0036070.0000840.0000810.0204700.0000940.0017270.0035970.0000830.0000810.0000820.0000810.0000810.0000810.0000820.0041770.0000850.0000810.0000810.0000810.0000810.0000810.0042890.0074920.0071380.0080430.0070920.1159810.1174650.1534491.0657150.0193740.6982660.7144391.7959330.0356011.1310561.1169173.1898933.1681462.1097800.0129190.0069490.0070190.0066002.7074081.9654390.0247270.0115000.0087210.2206350.0075690.0147880.0077270.0150190.0000900.0036910.0017560.0000820.0000830.0000840.0000830.0035750.0000860.0001060.0040520.0001630.0038620.0000920.0025720.0000840.0040030.0037010.0000862.4335691.2576110.0098490.0579921.2629250.7874520.0307980.0001070.0422980.0036370.0000840.0000820.0000820.0022370.0042760.0047760.0024170.0000820.0000810.0000810.0000820.0031870.0000830.0000810.0000820.0000810.0000820.0000810.0000820.0000810.0000820.0000840.0027390.6513670.5267581.2517301.5596562.8172112.6376951.1654420.1980241.3783281.3513430.7075160.0020990.0001400.1719980.0153770.0003381.3708660.0027580.0736160.7606230.0189430.0001750.0000840.7893480.0042620.0069000.0071760.0069450.0070100.0073570.0070350.5318770.2838270.1161991.1424920.0073910.0032840.0166451.1440660.0159901.2917801.2704943.7001951.5327291.2847301.2188290.0080120.0146450.0072820.0146040.0073360.0073220.0000940.0000910.0066010.0064960.0067180.0067280.0071650.0068700.0066100.0066400.0068750.0065021.2724341.3004950.0083812.5420731.2500211.4596560.0045560.0062230.0000850.0000810.0058760.0000850.0038590.0019810.0059920.0018940.0000820.0036840.0000930.0077130.0000970.0080610.0000910.0155190.0000970.0078460.0077550.0000950.0000910.0000941.0924630.0099710.0093960.0079251.1564910.0162160.0074780.0001800.0001030.0001000.0100340.0000990.0002120.0001081.0170801.0019720.1314390.0001560.0000860.6612460.7073261.0691930.6930991.3016950.0007930.0000840.6496050.0080550.0000950.0076030.0000960.0000910.0157530.0159790.0153470.0057632.9293321.3433182.7335631.6254580.0079782.5477712.5596911.9711720.0055921.0481760.0043061.2439711.0942871.1840000.8204851.0380181.9355550.5766820.7951541.2743810.6991610.9586400.0175970.0154790.5062250.0351222.2247880.2587650.0002210.0000840.0000840.0000840.3153780.0002520.0000840.0000830.0000830.5430700.2578652.7991980.0015680.0000850.0000830.0000833.2758011.5686271.1246430.0006990.0001020.0075860.0001010.7180270.0004630.0028730.0001580.0025470.0030250.0000940.0000850.0022990.0025390.0000900.0000840.0000850.0001320.0000870.0001710.0000911.5419200.0372251.6338442.9407182.4442782.5017091.5993580.6016032.1732421.1648770.0330511.0514310.0006290.0000830.0000850.0018740.0000850.0020620.0019350.0018310.0000960.5244210.0003600.0079750.0156910.0000970.0000880.0000910.0085511.6525840.0087610.0000930.0000900.0088810.0000930.0079080.0000880.0000800.0000800.0000800.0000800.0000800.0000800.0000800.8870870.4260090.0002970.0000800.0000810.0102000.0023620.0000820.0000890.0000810.0000820.0027560.8865340.0005321.0094160.0005920.0000800.0090910.0000841.8250812.5935080.0031200.9223100.0053250.0064841.1247670.0083470.0000880.0152390.0000910.0074370.0000870.0017690.0000830.0000830.0053870.0020470.0020110.0054720.0001150.0020690.0017520.0034120.0018630.0056390.0024840.0000830.0036340.3732600.2712190.7430530.0004600.0000920.0000900.0000920.9206132.4650810.0013130.0000920.9948490.0005840.0000900.9948480.9694390.0005680.0000920.0000900.0000890.9642280.9978840.0005860.0000940.0000980.0032660.0000950.0000930.0205800.0001030.0003140.0000940.0000940.0000950.0000940.0000960.0086720.0000970.0000940.0197690.0001040.0000970.0000950.1489990.0001700.0000940.0000900.0977310.0001320.0158470.8772610.0005110.0000840.0000850.0040800.0158830.0096100.0000890.8757760.0956020.0001313.2774355.7588043.2436154.6384524.6813790.0056130.0061910.0042730.0098220.0034670.0038410.0068820.0000870.0045530.0076720.0068000.0218571.1267810.0079190.0146470.0146361.0231090.0078910.0090840.0065122.0913160.0093350.1025110.0220930.2071660.0001810.0034080.0175751.0223060.0005820.0076561.0115010.0078770.0000980.0074670.0000980.0000930.0079510.0074950.0000970.0000980.0067120.0070030.0067620.0070300.0071720.0069771.1823600.0072650.0067920.3621660.0234650.0016830.0016870.0000820.0000840.0016800.0000870.0016741.6861100.0008720.0152910.0303930.0000961.3143260.0085210.1205571.1267440.0333670.0000970.0000810.0000830.0000810.0000811.1893320.0006371.1930421.1045990.0005942.4926591.4648020.0007610.0000810.0000811.1453660.0006110.0000810.0000810.0000810.0000810.9926482.9843111.0137070.0344770.0237640.0173690.0068600.0050370.0022940.0036340.0026360.0017460.0023570.0043730.0074880.0017300.0018010.0000830.0019890.0026110.0001030.0082580.0040020.0029350.0000870.0047140.0046530.0000830.0035000.0051500.0180100.0023720.0000830.0018000.0016960.0236780.0309570.0098650.0091120.0111511.0006210.8290740.0086230.0000851.2071650.8885000.7113740.0411351.1157940.3136510.9753060.0116720.3203780.0065101.0195420.6978162.4562800.0408900.0092490.6729480.6032380.0174660.0282920.0212610.0283520.0414610.0063700.0041960.0073000.0165370.0000890.0039450.0000840.0097542.4229771.3565853.3583511.8967090.9677860.0290840.0001070.0141770.0132670.0443450.0097880.0032880.0207120.0048871.6752391.5138700.0123400.8231350.0154650.0287670.0109300.2624840.0033970.0129260.0142010.8073530.0213640.0197290.0131280.4849771.4993910.0012060.6646830.0004570.5958180.1786460.1249483.0575730.3712460.0003101.2266940.4899780.9312200.2374650.0002110.2385680.3836140.2374370.0002020.2026780.2196681.4043710.2674390.6013141.3321630.4522990.0003920.9155924.5977120.0503652.2439060.8370470.1541490.7186792.0380070.1226570.2145880.1235720.1218601.9158450.2644790.7728710.5860110.1226090.2614350.0025710.1317050.1408710.2286090.1189140.2439112.3916700.2816520.4690363.8049090.1925750.6883860.0086810.8320340.0007280.0002911.0593590.0084982.0662550.0054530.2398110.5418930.9205431.6358130.2472940.1580761.7870191.0006920.0005951.0630490.0005670.5083380.3849160.1256782.3747320.4381320.4526840.1226550.9036090.4582010.0002940.4506920.3855102.3552500.4662361.5076040.5098590.0960190.0001940.4723660.0003510.1985930.8491560.9692100.9810690.0050700.3938840.0240720.4393260.6115110.3213980.3091970.0011142.6011320.0012660.2467860.3741691.8777640.8199980.0076520.0001331.0661260.0149840.0001020.0000960.0074860.0144641.0161440.8194380.4738850.8604410.1546710.2738490.2338892.2041331.3557460.0166380.9729821.2425230.0093511.7805181.6863270.4317510.2159440.1282340.1798170.2470660.1239000.1186931.1569850.0006590.3858350.0004600.2588822.7398680.0022840.0008310.4473640.9280380.7787100.4366710.0004480.0008750.0002681.7905980.2844400.0003670.0084040.0001041.5019350.0007230.8970540.1363090.5740700.0082970.0002351.4959501.8778750.7625230.0099620.4115911.8810380.0011551.0137300.9097871.8970530.0440290.2275450.0002890.5066022.3956650.0011010.2264150.7601480.7957860.0005500.2366201.6933652.2659200.1210540.2685420.7840280.0004590.3278692.0372740.0011100.0001770.8145730.3833470.3223480.0002600.3793171.3729650.0409500.0130300.0001100.2859560.4936030.0003150.2502380.1189700.0038640.5134511.2406741.1985000.2060780.5785210.3122690.4619380.5225090.0003220.5913801.0975030.4889860.2546880.0002620.1755091.7769120.0088200.0088790.0077581.4214521.0606780.0081190.0000991.4055510.8838781.3196102.6483020.0377012.4241981.8399640.1216130.2004640.1225500.4143560.8635380.6542690.0163151.2785390.6792110.2874470.5186950.2322100.0002590.0007080.2084960.1651392.3286802.2954080.4940171.6498791.6923820.0008810.4701520.3858720.2082410.4512901.4196131.0116411.7154630.2794572.1068000.0011020.1525180.3210710.0002480.1244430.9893950.0005030.7899900.3633690.3865530.6298770.3502821.1764781.6560150.0011370.1239630.0002100.3168720.0002510.4314330.0003520.1438110.5287410.0004191.2770120.1367050.9222171.0811120.4689440.0081980.4811260.0003151.0994952.4064690.9976950.1221690.5209780.0005090.2252870.0002462.5535672.6462210.1889560.3136040.3899270.0002500.0080150.2539940.0002610.0751173.0139552.6305740.2103220.5035091.1320740.0005482.2771020.4726690.0003772.2510780.0009942.0476220.0009050.2187990.1342020.0012601.9989661.1325790.3091840.0005280.8430050.0004210.2024730.4244450.5312040.8663220.1215710.0006410.4727020.7337170.4985200.5365310.0006182.6379100.0015870.1899940.5724370.4070841.6660010.0007720.0001480.2540561.3022180.3202011.9760340.1191380.3644042.3631910.0016010.3768250.8208700.0788162.3798890.0011590.5035690.1194500.1600520.2414910.0007411.0825670.0006771.7435770.3847111.5499040.4694370.1193730.2463100.0002850.9282500.1229921.2238190.1227870.1203260.6430280.4562990.2439490.1266010.0002430.1232731.1195780.2438750.1289320.1254480.2738150.5019070.1214100.0002760.0796880.0088730.0003310.1498050.7953392.6604050.1252530.0002350.1909202.0364300.9805711.0988990.0005633.3042660.3317040.4781050.1905840.1196850.4165271.1257900.0028520.6039510.1223710.1211640.1189670.1204010.0001960.1197380.9310680.0012070.1203440.6880080.2549770.8947450.8466900.2144470.0001960.2862072.5509430.8331020.1892540.3353940.1482982.5182350.3220610.0002490.3961530.0003220.3844481.3696810.3360810.2249420.0741600.4401160.9634611.0058831.0480670.0004880.0000960.9567140.9936130.8532130.4619130.3701920.0003610.0001210.9825862.8918091.8614371.2421422.1683410.0011321.7312821.6193720.4443270.0003470.0002350.4721300.3854990.1196492.3527720.5957051.4156992.3908940.8915460.4589272.7912760.6766200.4578810.2640790.0002300.6097090.3956170.0003890.6456050.2759811.0938280.2518790.2712540.0002340.3812960.2610820.0449260.0001280.0108180.0198592.4214950.1951620.1443260.9096600.4091430.1247190.0004630.4074650.2921800.3715240.2034440.1727280.4268742.2515130.1214132.7040750.2106960.2565030.1566280.2464361.8965340.3888020.3534290.3546830.1243760.1240610.0054570.0124000.4479500.1420580.9424500.6648171.1315470.0240130.0005120.3875480.1893190.2304850.2155140.0604471.8033621.2378231.1504750.6100840.2145611.4496060.0011410.5130950.2163850.0002710.9608900.2645400.1104110.1688330.2169472.5469551.5596432.1182140.0111170.0112490.4768120.1832773.3431530.3747142.6526330.8437770.1881041.8594060.0008020.3870380.8056072.2643100.0009510.0038620.5340110.4120920.1218600.0002350.8420760.4739150.4362880.6454160.0930511.0451000.8776260.1205680.5071200.5061070.3188090.4555090.4625101.6552390.1818790.4634670.5058640.2406320.1209420.9295290.4873700.1452700.2708460.7044460.0003790.2539830.2025780.7197780.1191050.8874820.5054621.5979580.0007340.2022930.3065731.5083180.4234460.1184920.2328530.2304830.7743991.0047242.5716500.1805590.2638270.0002161.0420670.9555580.2164360.5916420.2742410.3686421.7414000.5324980.4734101.3614880.0006610.4633030.3457360.2616410.1206200.6843812.2441760.5042650.1465230.2341391.4402100.5980191.5389632.5393622.5465510.7882442.0437790.0167141.2465212.7271100.0029560.8576810.3823440.9595090.0004440.2126080.0080810.5708470.1254982.7349010.0012350.0001300.2711930.0002200.2322741.0123121.0845470.1191630.0001630.1237202.2463670.6966900.2841230.2608350.1188880.8621370.2367680.0004370.0001160.0912550.2631972.9023260.4739300.823358

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/
With following parameter combination: {'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f19f0c0e_2020-02-20_11-45-257rx2vz4c/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    622    |    158    |    214    |     6     |
-------------------------------------------------------------
| disagree  |    154    |    190    |    93     |    48     |
-------------------------------------------------------------
|  discuss  |    221    |    83     |   1412    |    68     |
-------------------------------------------------------------
| unrelated |    40     |    37     |    60     |   8492    |
-------------------------------------------------------------
Score: 8512.75 out of 9226.0	(92.2691307175374%)
Accuracy: 0.9006555723651034
F1 overall: 0.6967531891948168
F1 per class: [0.6107020127638685, 0.3987408184679958, 0.7925905136121246, 0.984979411935278]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:00<3:01:44,  1.09it/s]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:01:53,  1.56it/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 2001/11898 [00:01<1:14:05,  2.23it/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3001/11898 [00:01<46:38,  3.18it/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3580/11898 [00:01<30:31,  4.54it/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4501/11898 [00:01<19:00,  6.49it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:01<11:30,  9.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 6123/11898 [00:02<07:18, 13.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 4633.91it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 744
  Number of epochs: 3
  Batch size: 16
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=744.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.3680810.3687210.1845390.0622070.4019840.0805290.0139790.0022920.0003800.0001360.0002370.0001140.0001630.0001070.9589010.4999020.0314230.4061060.0226550.7282831.1138090.9771840.5119500.0227460.0013270.0001470.0006180.3916551.5704240.0555450.0019450.0002240.0001000.0000962.0006311.9239790.3369990.0092000.0004550.2146580.0054770.3422180.0082600.1581860.1603180.0036670.0002420.0005570.2723920.0058510.0004120.0003970.0004440.0001150.0012150.8490290.0152580.0006570.0001040.0000960.3838630.0064030.0002060.0001120.0002980.0007290.4780280.0072910.1454230.0022050.3585290.0900640.7222310.0101170.0003550.0008970.0003570.0003540.0002850.0002890.2431640.0040200.1037890.0013430.0001080.0000930.0000930.0000930.0000940.0001600.0000940.0000940.0001550.0001070.0003390.0001140.0001020.0001040.0006060.0006350.0001160.0014050.0406090.0006210.0004820.0002270.0001590.0000930.0002370.0000940.0000930.2208980.0026430.2307660.2003330.4680490.0046000.0690640.0038800.0004240.0002480.0117760.0001950.2990400.0028680.0002640.0001510.0003010.0001040.0004790.0433920.0008530.0001200.0003530.0001600.0002910.0000940.0002300.0002230.2899841.0608250.0077070.6344710.0197790.0013270.0002400.0001610.0002850.4453110.0034290.0003090.0002940.0002840.6995940.0047580.3839380.0026170.0002330.4408870.6743200.4494490.0028860.0001721.3988830.9990691.6054181.3838302.3235960.4735421.3360811.2126810.0285370.0009870.0005060.5598903.2809860.0189290.0003940.0002890.0000980.2143240.0018560.0002380.2516250.0023610.0233370.0030310.0003210.8326500.0072140.1498960.0010860.0007380.0008130.4526170.0026860.0007850.0007000.0004252.4377600.0126780.2910500.0026080.0006940.4211890.1971970.2171150.2434090.4210681.5920010.0078620.0003910.4522710.2333180.0027240.2468790.3120820.0027321.2895460.8273450.0052600.2275930.6555610.6845540.2902150.0024660.4367470.0020161.3133920.8528421.5828132.7539840.6151640.0029320.0005110.0004190.1799600.0658400.0003730.2445850.0098000.0231620.0029840.0006010.0539100.0003210.0003850.0008430.0001060.0001000.0008900.0017180.0028550.0054800.0012340.0008850.0037450.0008040.3889480.0218170.3288610.3777120.0019581.5026640.0150930.3563200.0077860.2547370.2843230.2194010.1724670.6769980.7387690.3312370.0013740.0000980.4657890.0017740.1239020.0012400.3215870.0014310.0003360.0005020.0098160.0012720.2885710.8767100.7023961.5083620.0057650.3552050.0013111.5274600.3806660.0017690.0004210.0004500.0002880.1802200.5786741.8013681.4192600.5062260.0019780.0001180.0001190.0001060.0002990.0003100.0015052.3336001.9339930.0079120.2041410.0011810.0004120.6093390.4910870.0149590.0043130.0005080.0004730.0000940.0000960.3456670.4003890.0014460.0001820.0002560.0003560.0014560.7171360.4881180.0016370.0005500.0004430.0024450.0012290.0011080.0005150.0016783.5505551.8723370.2474580.0101870.0006360.0004210.0525890.0014570.2414760.7775331.3372240.0046570.0006060.0003590.0017600.0006290.0003020.0003120.2718410.7399120.3675620.4198030.6930760.0020040.0023320.1647220.0009660.0009550.0038100.0053750.3575380.0010630.0834920.0003180.0005500.0347540.4140970.0041060.0049170.0027051.3481590.9089960.4607760.9248630.0040170.0001690.0004050.0003480.0004760.0003551.8611911.3956922.8366461.4122920.9392230.3528300.0010420.0001590.3379800.0086100.0002640.3599640.2826720.0007920.3145120.0010060.0012300.0018800.5646602.1723960.2042861.6155630.0054200.0012910.0006070.0003770.0003020.0003820.0832590.4095500.0155410.0001940.0003590.0000930.0001550.0000930.0001533.4304771.8903520.3078390.0012860.3113970.0463230.0012950.8760511.5725982.4196441.0613701.6620061.7095920.0044790.0019630.0011301.7211770.7894420.0020430.0005410.0004670.0007120.0005460.0911470.0014790.0004240.0029010.9031101.2282440.7148550.4297460.0018890.3830960.4723800.1532330.1011700.4311850.0028700.3623921.0980570.0228090.0002090.7724510.3415980.0201010.0003430.0007420.0004130.0000940.0017130.2385190.1905290.0018980.0013390.6517320.0020700.1559750.0008530.0003170.0000930.7831560.0229490.0002030.4453930.8037561.3212500.8430430.0023780.0003660.0004210.0004220.0005780.7691720.3967221.2691310.1331930.3645280.4161660.0011230.0021860.0001150.0014280.0057530.0007410.0006030.0003920.0006770.1598730.3051930.0021930.0005550.0011960.0014380.0010850.0083010.0097500.0061500.0006000.0008710.0009860.5627171.3279950.0027700.5051410.0023821.0448100.1889410.0007050.0011480.0004920.0002920.7269550.0047181.0681890.5489360.7864811.0166510.8533810.6804070.0015260.0187940.0108660.4460600.0033300.0006860.0011120.2916140.2474660.4849200.0013202.8845880.0160410.0009180.0030720.0016040.0015040.0016300.1043240.2578600.6148620.4387150.2650870.3896550.9140460.5283860.6105480.5276650.4550620.0228750.0592490.5137040.6608140.3929260.6952791.1490870.4103180.2687740.7728890.2684390.9671630.4915550.4307190.2935030.2514560.6698421.4605900.1977680.0266480.4019720.6786420.5156970.4217510.1053940.2193300.7821770.9809400.6425440.4573460.1562900.3677320.9022470.4502251.1989610.8366490.5934060.3037870.1116640.4423760.2970560.3982890.1867360.2689650.4804260.1398350.4325050.2900350.2770260.2755781.7964350.4621610.5414310.5491210.5244830.5043150.9319491.0703960.3766520.1264880.1938980.7264420.6784660.2921640.4994570.7082350.2493871.2394200.2269220.9917670.0770740.9470010.4875180.9017290.2495830.5688270.5019320.6560190.1699080.8542030.2907610.1227670.4416580.3423240.5724940.1180800.4080400.8735660.4366740.6456660.2285230.4493570.0420330.0234650.5987070.7765850.9063920.2834950.1786540.0188360.3731790.6933900.3898140.2868820.0645950.9666860.3906230.2884750.8454170.3316491.2966570.9040670.2976710.6972591.2184220.2515160.1249490.1327570.1532260.6778100.1720690.6333370.4158291.1555220.6007130.2396780.2009990.7307800.0096690.4910600.4548510.4710000.4894970.7416520.4170311.2167460.2222820.5772400.7295480.2434050.7143170.4635270.5123270.7180590.3058540.4409060.9524810.5380600.0785550.6744200.2165950.4014450.6919050.4319060.1944080.2857591.4303870.8520730.3917680.0622010.6135790.1884020.2978091.1302780.1814040.0023351.189757

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/
With following parameter combination: {'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f74ea510_2020-02-20_13-50-21rn4ksp8b/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    665    |    54     |    132    |    10     |
-------------------------------------------------------------
| disagree  |    131    |    303    |    82     |    15     |
-------------------------------------------------------------
|  discuss  |    218    |    82     |   1495    |    66     |
-------------------------------------------------------------
| unrelated |    23     |    29     |    70     |   8523    |
-------------------------------------------------------------
Score: 8697.5 out of 9226.0	(94.27162367223065%)
Accuracy: 0.9233484619263742
F1 overall: 0.7791078523696586
F1 per class: [0.7007376185458377, 0.6066066066066066, 0.8214285714285714, 0.9876586128976186]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:01<5:06:47,  1.55s/it] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1501/11898 [00:01<3:07:41,  1.08s/it] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 2001/11898 [00:02<2:05:06,  1.32it/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 4501/11898 [00:02<1:05:27,  1.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 6001/11898 [00:02<36:32,  2.69it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 8501/11898 [00:02<14:44,  3.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 4129.08it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 2975
  Number of epochs: 3
  Batch size: 4
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2975.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.9715251.2401570.8887111.2677621.9913580.6669040.7521670.3760850.3156430.3037040.2990020.6681991.0690850.3508690.2936940.2882120.2866450.2854940.6568780.3032050.2837920.2821460.2814570.2808690.2803350.6522300.2937181.6852960.3288210.2799710.2779641.3947530.3122180.2780930.2768110.2765410.2763140.2761000.2758980.2757060.6479090.2844350.2754040.6474220.2833460.2749290.2746090.2744750.6467350.2818310.2742690.2740100.2739020.2738000.2737030.2736090.9764110.9886550.9885710.2853880.6457730.2792190.2731360.2729680.2728970.2728310.2727660.2727030.2726430.2725830.2725260.2724710.2724160.2723640.2723130.2722630.2722150.2721670.2721210.2720770.6444180.2765880.2720050.6442940.2763020.2718830.2717940.2717560.6441050.2758690.2716970.2716180.2715841.6773380.2864760.2716481.3467390.2825160.2715150.2713750.2713460.2713190.2712920.2712661.0160100.2783080.2712580.6435520.2745910.2711510.2710971.0158440.6500870.6467701.0190750.9803870.6494680.2741830.2709560.2709090.2708900.2708710.2708520.2708340.2708160.2707990.6431660.2736960.2707700.2707310.2707150.2706990.2706830.2706670.2706520.2706370.2706220.2706080.9734860.2756360.2706010.2705510.2705370.2705240.2705110.9733910.2752990.2705050.2704600.2704470.2704350.2704230.2704110.2704000.6427730.6451640.2727682.6656310.2855030.2704280.2703220.2703110.2703010.2702900.2702800.2702700.2702600.2702500.2702410.2702310.2702220.2702120.2702030.2701940.2701850.2701760.2701670.2701590.2701500.2701410.2701330.2701250.2701160.2701080.2701000.2700920.2700840.2700760.2700690.2700610.9729470.2737260.2700580.2700310.2700240.2700170.2700100.2700030.2699961.0147581.0184750.2736990.2699870.6423470.6441660.2717740.9728440.2733320.2699460.9728170.6456490.2716920.2699140.2698990.2698930.2698870.2698820.9727692.3817731.7690460.9795660.9759571.3901820.2748660.2698590.2698320.2698260.2698210.2698160.2698100.9726980.2728430.2698080.2697900.2697850.2697800.2697750.2697700.2697660.2697610.9726490.2726680.2697590.2697420.2697380.2697330.2697290.2697240.2697200.2697150.2697110.2697070.2697020.2696980.2696940.2696900.2696860.2696810.9725700.2723870.2696800.9725581.3476220.2737561.0144380.2724600.9725491.0170440.2724270.6420300.2710100.2696320.2696230.2696200.2696160.9725060.2721560.2696150.2696020.2695980.9724881.6778790.2745820.6419870.6432770.2708890.2695790.2695710.2695680.6419490.6432300.2708430.2695600.6419370.2708160.6419350.6431850.2707980.6419250.2707790.6419190.6431490.2707620.2695260.2695190.2695161.0142821.0167050.6443180.6431020.2707071.0142720.6442680.2706910.2694941.0142570.2718420.2694900.9723730.2716800.2694811.6752570.2738352.6228652.4272831.3932540.2729061.0142360.6441091.3877441.0176070.6440910.2705720.2694450.2694390.2694360.2694340.2694320.2694290.2694270.2694250.2694220.2694200.2694180.2694150.2694130.2694110.2694090.2694060.2694040.2694020.2694000.2693970.2693950.2693930.2693910.6417730.2704300.2693880.2693830.2693800.2693780.2693760.2693740.2693720.2693700.2693680.2693660.2693640.2693620.2693600.6417430.2703570.2693570.2693520.2693500.9722410.2712110.2693500.2693430.2693410.2693390.2693370.2693350.2693340.2693320.2693300.2693280.2693260.2693240.2693230.2693211.0140880.6435971.7598040.6454720.9731550.2710830.9722060.2710690.2693100.2693040.2693020.2693001.3864531.0168250.6435211.0149830.2711201.3445731.3471890.6442950.2701960.2692860.6416670.6425630.6425610.6425580.2701691.0140461.0158160.2710450.2692740.2692690.6416520.2701420.2692660.2692630.2692610.2692600.2692580.2692570.2692550.6416390.6424950.2701090.2692520.2692480.2692470.2692450.2692440.2692430.2692410.2692400.2692391.0140060.9737990.9737040.9736990.9736940.2707960.6416170.2700520.2692280.2692251.6750100.9751981.5907500.2721050.6416100.2700270.6416020.6424060.6424040.2700170.2692132.0473800.2730160.2692160.6415910.6423820.2699960.2692040.2692010.2692001.3863531.0163140.2707630.6415830.2699720.2691950.2691920.6415750.6423451.0147290.6431090.9728480.2706300.2691870.2691830.2691810.2691800.2691790.2691780.9720701.5905800.2718391.0139480.6430531.0146901.0154310.2706590.2691710.2691670.2691660.2691650.2691640.2691630.2691620.2691610.9720530.2705340.2691610.2691570.2691560.2691550.2691540.2691530.9720450.9733980.2705040.2691511.0139172.6658320.2737200.2691541.2167430.2709410.2691450.2691410.2691400.2691390.2691380.9720300.2704520.2691380.6415190.2698270.2691340.2691310.2691310.6415140.6422000.6421990.2698130.2691270.2691250.2691240.2691231.0138920.2704760.2691230.6415040.2697920.2691190.6415020.2697861.3443940.2710410.2691170.9720061.9208570.9749430.2703640.2691110.2691080.6414920.2697630.9720000.9732330.2703400.6414900.6421380.2697531.3443802.7520341.3486880.9738631.6761030.6439120.2697420.2690960.6414790.2697320.2690940.6414770.2697270.2690920.2690901.0138580.6427350.2697200.2690880.9719790.9731611.3455450.6432740.2697101.3443610.2708760.2690840.2690800.9719720.2702440.2690800.9719700.2702360.2690771.0138440.2702970.2690750.6414570.9725730.2702190.2690720.9719630.6425950.6420580.2696710.2690681.3443440.2707970.6414520.2696620.2690640.6414470.2696570.2690620.2690610.6414450.6420350.2696500.2690590.9719500.2701650.2690580.9719480.2701580.9719490.2701530.2690540.6414360.9725240.2701450.2690520.2690490.2690490.2690480.2690470.2690470.2690460.6414301.6754022.3798770.9751640.9730140.9730080.2701130.9719362.4206622.0504701.0165030.6425521.7591401.7608201.0160491.7596961.0160401.7596922.4218173.5732312.7550211.0175010.2701440.9719260.2700721.3443090.9735110.9729610.2700650.9719220.9729520.2700593.0805992.0513070.2716270.2690280.9719171.6758310.6434490.6419490.6419460.6419441.0143280.2700941.0137900.6424731.3867081.7601571.7606891.7606861.7606820.6435250.2695480.2690150.2690140.2690130.9719060.2700050.2690130.9719040.2699990.2690110.2690090.2690090.9719010.2699900.2690090.2690070.2690060.2690060.6413900.6419050.6419050.2695190.2690040.6413871.0142840.6424100.2695130.6413860.2695100.2690001.2165980.9731831.9204480.2712410.2690001.3442740.9733440.2699481.9194880.9741120.6423280.2694961.2165930.6426480.9723850.2699320.2689930.9718840.2699260.2689910.2689890.2689892.7500521.0170390.2699760.9718810.2699130.9718800.2699090.2689860.2689850.2689840.9718770.2699010.2689840.2689820.2689820.2689811.2165800.9731010.2698910.2689810.2689790.2689791.5889623.0822460.6449730.6418440.6418390.6418380.2694520.2689760.6413590.9723410.6422520.2694470.2689741.9194650.2710590.6413590.2694410.6413560.6418242.4199932.7527321.7616201.0156060.6422861.0142030.6422811.0142010.6422781.0141990.6422751.0141971.0146571.7594250.6431891.3865791.3874940.6427231.0141910.6422611.0141881.3451513.0818483.0839672.5399760.2717260.2689630.2689590.2689590.2689580.9718510.2698071.2165570.9729930.2698040.2689570.9718480.9726921.2173970.2700901.3442333.3265162.4231570.2715200.2689550.2689520.6413360.6417780.6417770.2693920.2689511.0137190.6422120.2693890.2689491.5889323.3267751.7620712.0907421.7606161.7602272.0907332.3797481.7609411.3878341.7597822.4212292.6666991.7193811.9211120.2708511.0137141.0145691.0145690.6421831.0141391.7593340.6430320.2693680.2689401.2165381.2176180.2700190.2689390.2689381.6747232.7515951.3470261.0149250.6421640.6417420.6417401.0141241.3450503.0817151.7197530.6429451.3865060.6425680.6417350.6417331.3865010.6425611.3865010.6425571.0141150.2697560.2689310.9718230.2697060.6413140.2693390.2689290.2689280.2689280.2689270.2689270.9718201.6754811.0152330.2697410.6413110.9722240.9725830.2696890.6413100.9722211.6754711.0152150.9726231.6754680.2704400.6413082.3780012.7522531.0163580.6421070.6417050.2693190.6413051.0140870.2697141.3860740.6424931.0140850.6420940.2693141.0136870.9725990.2696611.6747033.1131984.0623131.9234050.9735500.2696560.2689150.2689140.2689140.2689141.0136830.2696910.6412980.9721940.2696450.2689131.6746983.3266493.2425591.7615311.0152250.6420670.6416800.6416791.3864481.3872141.3872130.2700580.2689090.2689080.6412920.2692890.2689080.2689070.2689070.2689060.2689060.2689060.2689050.9717980.2696180.6412900.2692810.6412892.0474510.9735910.2696140.2689040.2689030.2689020.2689020.2689020.2689020.2689010.9717940.9724960.2696031.6746870.2703001.6746870.9731900.2695990.2689000.2688990.9717910.2695931.3441761.7594981.3875210.2699990.6412831.6750490.9731711.6753730.2702751.2164960.9727150.2695832.1640932.5383261.2187052.5374011.3882541.3871351.0147481.3867701.0147450.6419990.6416380.2692520.2688920.2688910.6412762.1644472.0907601.7601791.7598601.7598581.7598560.9732090.9724550.2695610.9717820.9724510.2695580.2688891.6746741.6760081.3455000.6422921.3445182.2927813.0823712.4220982.4214721.7604531.0150590.6419721.0140051.0143541.3867382.4204872.9966683.0830070.9744062.0477120.2705421.3441620.9727770.2695370.9717760.6419190.2692281.6746671.7178450.6426060.6416110.2692250.2688811.2164790.9726450.6419120.2692220.9717720.6419082.2920971.6765170.2701660.2688790.2688780.2688770.6412620.2692160.2688770.2688770.2688760.2688760.2688760.2688760.2688750.2688750.2688750.2688750.2688750.2688740.2688740.2688740.2688740.6412580.6415921.3863610.2698730.6412580.6415900.6415901.0139741.0143050.2695350.6412570.2692021.0136400.2695321.0136400.6419150.2692000.2688701.0136390.6419111.0139670.6419100.6415820.6415810.2691960.9717610.2694850.2688680.9717612.6228672.3356783.4858543.2421502.6667202.1661531.9628860.2703390.6412522.0473592.6237932.7519691.7605532.7512193.3272882.9972752.6246030.2708950.9717580.9723610.2694681.0136331.3866561.3869750.2698210.2688630.2688620.2688620.2688621.0136311.7590351.7596691.7596681.0148981.0142640.2694930.6412450.6415601.0139450.6418751.0139440.2694891.0136290.6418711.0139421.0142551.0142550.6418690.2691711.0136270.6418660.2691700.2688570.9717501.3447220.6421390.6415520.2691670.2688560.9717490.9723330.9723332.2923161.3458101.9621151.3874101.3869341.7174402.6653122.3355840.9734510.2694340.2688540.2688530.2688530.9717460.2694290.2688530.2688520.2688520.2688520.2688520.2688520.2688510.2688510.2688510.2688510.2688510.2688510.2688500.2688500.9717430.2694190.2688500.2688500.2688490.9717420.9723090.2694162.3775282.4211002.7516421.7603810.6424301.0139171.3865981.0145120.2694440.9717400.9723010.9723010.2694081.5888311.7594362.9099991.7604843.2405061.0159731.0142071.0142050.6418200.2691401.0136141.0142020.6418170.6415231.0139070.2694302.3775232.3791800.9733941.3865501.7592581.7595492.3347632.0905062.6655282.7517782.3355332.0905011.7598001.3871570.6420961.0139010.2694200.2688411.0136101.7589560.2699950.2688410.2688400.2688400.2688400.2688400.2688390.2688390.2688390.2688390.2688390.2688391.3441161.3449411.0144320.2694090.2688380.2688380.6412220.6415060.2691210.2688370.2688370.2688370.6412210.6415040.2691190.2688360.2688360.6412200.2691180.9717290.2693670.9717290.2693661.2164340.9724420.2693642.0470051.3873251.0144440.2693941.0136031.7589301.3871040.2696700.6412180.2691110.2688331.3859870.2696660.6412180.2691100.2688321.0136011.0141540.2693850.2688321.0136011.3865371.3868120.6420430.6414911.3862601.7591941.7594681.7594671.7594661.3870810.6420370.2691041.3859841.0144180.6417600.6414871.3862563.0812182.4214391.7599391.7594551.7594541.7594531.7594521.6756991.7175131.3870341.3867930.6420231.3862521.7591750.6422911.0138660.2693651.3859811.3867870.6420170.6414800.2690941.3859800.6420140.6414781.6748791.3451121.7172590.2698631.3859790.6420091.3443691.7172551.5898433.0813393.0824022.4213841.7598951.7594231.0146530.6417381.0138570.6417361.0138571.7588891.7594161.0146471.0141191.0141181.0141181.3865020.6419951.0138541.0141150.6417300.2690830.6412060.2690821.3859750.6419892.6224670.2704690.9717140.2693120.2688200.2688200.2688200.9717130.2693090.2688200.9717120.2693080.2688190.9717120.2693070.9717120.2693061.2164181.2170730.2694740.2688180.2688180.2688180.2688181.2164160.2694701.2164170.2694690.2688170.2688171.2164160.2694660.9717100.2692980.2688160.2688160.2688160.2688161.6746011.6755600.6421591.0138390.9722150.9721861.3864470.6419590.6414531.3862210.6419571.0138361.0140880.2693181.9193062.7509932.7515532.9962583.8163432.8692950.2705641.2164130.2694500.2688131.9193050.2699200.2688130.2688120.2688120.2688120.2688120.9717050.2692810.6411970.2690600.2688110.2688110.2688110.2688110.6411960.2690580.6411950.2690582.1640090.2700660.2688110.2688100.2688100.6411940.9719490.2692740.2688100.2688094.0592054.0617003.2417802.9965333.2410772.0908073.0815762.7517162.0904821.7595401.7174460.9726480.9721610.2692670.9717010.9721590.2692660.2688080.2688070.2688070.9717000.2692640.9717000.2692630.9717000.2692620.6411910.2690471.0135750.6416720.2690470.6411900.2690460.2688061.0135750.6416690.6414300.6414291.0138140.2692830.6411900.6414280.6414281.3861970.2695201.0135740.2692801.0135730.6416640.6414261.2166401.6751930.9725920.6416360.6414251.5890241.2172411.5893891.9201330.9727431.3445261.6752700.2696921.6745890.6420751.2166371.6751860.2696890.2688020.2688020.2688010.6411860.2690350.2688010.2688010.2688010.6411850.2690340.2688010.2688000.6411850.2690330.6411850.6414170.2690330.2688000.2688000.2688000.2688000.6411840.2690310.2687991.0135680.2692610.2687990.6411830.2690290.6411830.6414140.2690290.2687980.2687980.2687980.2687980.2687980.2687980.6411820.2690270.2687980.2687970.2687970.2687970.2687971.0135661.7587921.7592481.7592471.7592471.3449851.3447310.9723462.3779042.0482530.6422651.2166223.5703571.7603422.6649632.6236363.1130222.5381042.2930481.7176841.7592111.7592361.7592353.2401713.3268682.3793200.9729610.9721121.6750040.9725341.6750030.9725331.6750030.2696391.0135630.6416250.2690170.2687930.2687930.2687931.0135620.2692380.2687931.0135620.2692371.0135620.2692370.6411770.2690141.0135621.0140040.2692352.2916691.9204850.9726651.6749952.2925022.8680810.9732230.2692080.9716840.6415910.2690110.2687910.2687910.6411750.6413950.6413950.6413940.2690090.2687900.2687900.2687900.6411750.2690080.2687900.2687900.2687900.2687890.2687890.2687890.2687890.2687890.2687890.6411741.0137751.7587611.7591941.7591942.9096223.2407991.6763000.6419900.9718971.3444730.9723040.6415800.2690031.3440651.0141780.2692180.9716810.6415770.9718950.9720851.0139610.2692160.2687871.3440641.0141741.7587531.7591801.7591801.7591791.7591791.7591782.7507032.0902522.4203822.7510791.7597421.0144062.0473813.3260771.6763132.0477572.0898442.7508852.3349492.0900062.3784980.9728751.6749700.9724751.6749690.9724740.9720750.2691820.2687841.7583231.7591641.7591631.7591631.7591621.7591621.7591611.7591611.7591601.7591602.0896672.0898521.7593432.4201742.4205441.5899721.0142921.3863540.2694070.2687831.3859360.2694061.0135520.6415821.3861440.6417890.2689891.0135510.2691960.9716750.2691720.9716750.2691711.6745670.2695600.9716740.9720630.2691700.2687810.2687812.6221651.3453581.3446520.9722671.7168301.6753640.9724480.2691670.2687800.2687800.9716730.2691660.2687800.2687801.2163790.9721921.2167640.2692980.2687791.2163781.2168961.2168961.2168951.2168950.2692960.2687791.9192710.9725700.2691610.9716710.2691600.2687781.6745641.6753271.6753271.3866942.6646452.0901222.4203182.3346941.7594332.4201382.0481102.0479081.0145061.3444571.0141261.5891621.3447651.3446330.9722481.3444321.7170160.6419380.6413611.6747620.6419141.3442541.3446291.6751373.0811003.0818513.3265561.2180070.2692820.2687760.2687750.2687751.2163740.2692790.2687750.2687750.2687750.6411591.2165711.6750630.2695200.2687750.2687740.2687743.5697592.9962891.9207080.2696470.2687750.9716670.2691450.6411590.2689700.6411580.2689700.6411580.6413540.2689690.2687730.6411580.6413530.2689680.2687730.2687730.2687730.2687730.2687730.2687730.6411571.7585052.3343032.6651112.4205772.2927702.0479970.9725911.6749241.3447813.0809031.9207250.2696290.2687720.2687720.6411560.2689640.6411560.6413490.6413490.2689641.6745570.2694970.9716641.6749190.2694960.2687710.2687710.9716631.9196240.9725130.2691320.2687700.9716630.2691310.9716630.2691300.2687700.2687700.2687700.2687700.2687700.2687700.2687690.9716620.9720210.2691280.2687690.2687690.9716620.6415110.2689590.2687690.2687690.2687690.6411531.3442350.2693140.9716620.2691250.2687680.9716610.2691241.6745540.9723720.6415080.9718491.3862771.7588701.9618870.9725140.2691221.6745530.2694750.9716610.2691210.6411520.2689540.2687671.3859210.6417120.6413381.3861070.2693260.6411520.6413371.0137220.6415231.3861060.6417090.2689521.0135361.0139070.6415220.6413360.2689510.2687660.2687660.2687660.6411501.3861040.2693200.2687660.6411500.2689500.2687650.6411500.6413340.2689490.2687650.2687650.2687650.6411490.6413330.2689480.2687650.2687650.6411490.2689480.2687641.6745500.2694550.2687650.2687640.2687640.2687640.2687640.2687640.9716570.2691080.2687640.9716560.2691070.2687640.2687630.9716560.2691060.2687630.2687630.9716560.2691051.3440401.3445630.2692860.2687630.2687630.6411470.9718360.9719960.2691030.6411470.9718360.2691032.5363453.3261372.9102073.2405143.2406731.0149651.3862761.0140701.7586591.0142481.0138901.3862740.2692991.0135311.3862731.3864522.3779771.2173720.9721091.6748841.6752211.3447120.9721691.3443751.7588141.7590111.3866261.7588331.0142411.0138850.2691160.6411451.3442151.3445500.2692720.9716530.6414790.9718300.2690940.9716530.2690930.2687600.9716530.9719860.2690930.2687601.7582981.7590021.7590021.7590021.7590011.7590011.7590011.7590001.7590001.7590001.7589990.6418450.6413190.2689340.2687590.6411430.2689330.6411431.0137020.2691081.6745443.0809890.2700752.3774371.3868992.3779603.3260223.0817580.2700720.2687580.2687580.2687570.2687571.3440350.2692580.9716500.9719770.2690841.9192491.2171230.2691970.2687570.2687570.9716500.2690830.2687570.2687570.2687570.2687570.6411411.7584672.0476151.6753642.3780841.6755151.3865591.0140410.6414841.0136970.6414830.6413120.6413121.0136961.3862520.6416540.6413110.2689260.6411400.6413110.2689261.3859091.0140360.6414810.2689261.0135241.0138650.2690951.0135241.0138640.6414790.6413090.2689240.6411390.6413092.3776033.0812861.7595721.7589701.7589701.0142001.0138621.0138610.2690921.0135231.0138611.7586301.3865821.7587981.0141971.3862450.6416431.0136911.3862441.0140271.7586281.3865791.7587951.0141941.3862431.7587941.3865773.0808272.3786953.0812732.4205710.6421030.6413051.0136891.3862400.2692530.6411370.2689191.3859061.7587901.7589571.7589571.0141871.9195771.6752750.2693790.9716450.9719580.9719581.3443421.0140001.6748691.3865311.7169113.0809671.7176620.9722870.9719562.3777420.9725792.4196181.0144731.7167432.0894381.3867102.0474162.4200921.3449790.9721190.9719540.2690611.3440280.2692251.7164131.2169871.2167670.6415520.6412990.2689141.3440281.2168211.2167651.2167650.2691660.9716430.6414430.9718060.2690581.2163491.2167630.6415490.9718051.5890400.6417110.9718050.2690561.2163481.5891460.2693250.6411341.3441890.9721101.3443330.6416011.2165101.2167601.2167591.2167591.2167590.6415442.1641081.5895541.2169202.1643570.2695691.2163481.2167571.2167571.2167570.6415421.9194010.9723530.9719441.6748371.2169531.6749420.2693531.9192400.2694580.2687481.2163470.9720481.9195410.2694561.2163470.6415380.6412920.6412920.6412911.2165060.6415370.6412910.2689061.5887310.2693110.9716400.6414310.9717990.6414310.9717980.9719391.2166451.6749360.9722380.2690460.9716390.6414301.9193970.9723400.9719380.9719370.9719370.2690441.3440240.2692010.9716390.6414281.2165031.3444240.2692000.6411311.6746890.9722320.6414271.2165021.2167440.2691451.7164080.2693550.6411300.6412870.9717951.3443180.9720900.2690401.2163441.6749280.2693350.2687450.9716381.6748250.6417181.2165001.9196331.2170341.2167401.9196330.9723271.3443160.6415780.2689000.6411292.1640980.9724270.9719302.1642350.6419171.2164981.2167371.2167371.2167371.2167361.2167360.6415220.2688980.6411280.2688980.2687442.1639420.2695280.2687440.9716360.9719271.2166330.9720280.2690330.2687430.2687430.6411280.6412810.2688960.9716360.2690321.3440200.2691851.6745291.2169190.9720250.9719240.2690310.6411271.3441731.2167820.2691310.6411271.3441720.2691822.0469131.6752551.2169160.2691291.2163410.2691290.9716351.5890120.2692801.2163411.2167272.1643260.2695131.2163410.9720200.9719201.2166262.1643251.3447890.2691780.6411261.3441700.2691770.2687410.6411260.6412770.6412760.2688920.6411261.3441690.9720680.9719180.2690250.6411250.9717840.2690242.1639391.2171030.2691220.9716332.1642211.2171021.2167210.9720140.6414081.6746760.9721980.2690220.9716330.2690221.5887240.6416540.2688891.2163391.6749050.9721950.9719140.9719140.2690211.9192320.9722920.2690200.6411241.2164871.3443951.9196600.2693971.6745250.9721921.2166181.2167151.2167150.9720091.3442961.5891500.2692630.9716321.9195100.9722861.6748030.9721890.2690170.2687391.2163370.9720060.6414011.3441631.6749490.6416781.5888690.2692590.9716310.6414001.2164840.9720041.2166141.3443892.1643591.2170832.1643090.2694831.2163370.6414950.2688841.2163370.9720020.2690130.2687381.9192290.6417691.6746690.6416721.5888671.2168520.2691081.2163360.2691072.1639350.2694770.9716300.2690111.2163361.5890900.2692510.2687371.2163361.6748910.9721770.9719030.9719030.9719030.2690101.9192291.9198691.6751631.2168811.9195960.2693760.9716290.2690081.3440141.7168140.9721890.9719010.6413920.2688800.9716291.2166060.2691020.9716291.6747931.6750630.9721700.9718990.2690060.2687361.3440130.9720420.2690062.1639340.2694631.2163350.2690990.9716280.9718980.2690050.6411200.2688781.6745210.2692731.2163340.2690971.9192271.3446430.9720390.2690031.2163340.2690960.6411191.9193690.9722570.9718960.2690022.1639330.2694561.2163341.9195870.6417461.5888600.2692360.2687341.2163331.2166930.6414781.2164751.2166921.2166920.6414770.2688750.6411190.9717680.9718931.5889830.2692321.5887171.2168311.2166902.1642890.2694481.5887170.2692311.6745190.6416472.1640720.9723391.2165972.1642880.2694451.6745191.2168601.2166881.2166881.2166871.9195800.9722452.1641941.2170420.2690881.2163320.2690872.1639311.2170400.2690871.2163321.2166850.9719790.2689950.9716250.9718870.2689941.2163310.9719780.9718871.2165930.2690851.2163310.2690841.6745180.6416390.2688702.1639300.9723280.9718860.9718851.2165923.1118802.1649830.2694341.9192241.2169411.2166811.2166811.2166810.2690821.2163301.6748670.2692501.2163301.5890640.9721110.9718831.3442680.9720200.2689901.9192231.5893221.2168151.2166780.6414641.2164671.5890621.2168140.2690791.9192230.2693360.6411162.1640650.6418090.9717600.9718810.6413721.2164660.9719701.9194790.2693330.2687300.6411150.6412511.0136350.9718951.9194780.2693310.2687301.2163290.9719680.9718790.6413701.6746510.2692411.2163290.9719672.1641830.2694170.2687300.9716230.6413691.2164630.6414570.6412490.6412490.6412491.3441420.9720111.6747690.2692371.9192210.9722180.2689831.6745150.9721290.2689831.9192210.9722160.2689820.9716220.9718750.2689820.6411140.6412470.9717550.2689810.9716220.9718741.2165801.2166671.2166671.6748540.6416171.2164610.2690680.6411132.1640591.9198981.2169181.2166660.6414510.6412461.2164601.2166651.2166651.2166651.2166651.2166652.1642640.6417870.6412450.6412451.2164591.2166640.2690650.2687280.9716211.2165760.9719571.5889610.6415800.9717530.2689771.2163261.2166621.2166621.2166620.2690630.6411120.9717520.2689761.2163261.2166611.6748480.2692231.5887110.9720860.2689750.6411120.9717510.9718670.9718671.2165731.0138291.9194811.2169060.9719530.9718670.9718661.2165721.2166580.6414441.2164561.3443360.9719960.9718660.2689731.9192182.2921802.1646320.2693890.2687261.5887101.2167861.2166560.2690570.9716190.9718640.9718641.9194630.9721941.5889550.6415701.2164540.2690560.9716190.6413550.9717480.9718631.2165690.9719480.9718631.5889530.6415681.2164540.2690540.9716181.2165680.9719461.0137380.2689831.2163241.2166521.6748391.2168102.1642510.9722731.2165670.2690521.2163240.6414361.2164521.2166501.2166501.2166500.9719440.6413511.2164521.6748360.2692080.9716180.2689661.3440021.6748801.2168061.9195421.2168900.6414340.9717451.5889490.9720691.5889490.2691760.9716172.1641621.2169711.2166471.9195401.2168871.5890311.2167741.2166470.6414321.9193430.6416711.3441280.9719831.0137320.6413620.9717431.9194551.2168840.2690461.6745100.6415861.6746360.2692011.2163230.2690452.1639221.2169651.2166440.2690440.2687230.9716160.2689610.9716160.6413450.6412341.2164480.2690431.2163221.2166421.5890272.1643660.6417471.2164481.2166411.2166410.2690420.2687230.9716160.9718521.6747450.9720881.013728

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/
With following parameter combination: {'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e5facb22_2020-02-20_09-04-41huhpqksz/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
| disagree  |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
|  discuss  |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
| unrelated |   1037    |    468    |   1779    |   8614    |
-------------------------------------------------------------
Score: 6460.5 out of 9226.0	(70.02492954693258%)
Accuracy: 0.7239872247436544
F1 overall: 0.20997464898595944
F1 per class: [0.0, 0.0, 0.0, 0.8398985959438378]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:01<4:30:39,  1.37s/it]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<3:01:33,  1.05it/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 4001/11898 [00:01<1:28:03,  1.49it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 4855/11898 [00:02<54:59,  2.13it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 5630/11898 [00:02<34:15,  3.05it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 6292/11898 [00:03<21:30,  4.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:03<00:00, 3262.71it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 1488
  Number of epochs: 3
  Batch size: 8
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=1488.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0032600.0057050.0198970.0071720.0018530.0014780.0020350.0003500.0001030.1801970.0180800.0017030.0006730.2498430.0179060.0027310.8245980.0485650.5595010.0295330.0020130.0012980.0001190.0000650.0006590.0000860.0000630.0000620.9500641.2827240.0433540.0015060.0001080.0008510.3375700.0097050.0003300.0000700.5764050.0149460.8305820.8497550.8167050.8694850.0203220.8189250.0234820.0005600.0036460.0001340.0000620.0000610.2975460.0087620.0002230.3182540.4135000.5352620.5477310.0093430.0002150.0000630.0000610.0005800.0000690.0000610.0000610.0000612.4048101.6114501.6086560.8269161.4814150.0203520.0003350.0000640.0000620.0010060.5582350.8767000.0110190.0001960.0000620.0018660.0000820.0000610.0003150.0000640.8116280.0091790.0001620.0000620.0000610.0000600.0000600.0024580.0004610.0000660.0000610.0010680.0010670.0005920.0005480.0025210.4615340.0049280.0001060.0000610.0025420.0119831.2201860.3340580.0030420.0000860.0000600.0088970.0001360.0000600.0000600.0000600.7325290.0061140.0001110.0000630.0000620.0000620.0001250.0000610.0000600.0028920.1431070.0040900.0010350.7055340.0058000.0001080.0000620.6512170.0047780.0000940.0073440.4882690.4072980.0029080.5362940.5447280.0042890.0005620.0005440.0005410.0012710.0000680.0000600.0019650.0010210.0010160.0005400.0010840.0000660.0024310.0058820.1079150.0133010.0011140.0019820.0171600.0001630.0000600.0000600.0000600.0000590.0000590.0000590.0000600.0000600.0000600.0000590.0000600.0005370.0000620.0000600.0000590.0000600.0000600.0000600.0005380.0000630.0000600.0025260.0000720.0000600.0001010.0000600.0000600.0000600.0000610.0018550.0028530.0031960.0026380.0034110.0000770.0031800.0015101.2769120.5382270.0026720.0010690.0011580.0010540.0010130.0000650.0005330.0000620.0000590.0000620.0007940.0005390.0000660.0000600.0000590.0000590.0010050.0079150.0119740.0006990.0033110.0121350.4990830.0027110.8358750.0046370.0000800.0056080.0005650.0005360.0000610.0017520.0010980.0005360.0000620.0010250.0015450.0024660.0000700.0000610.0003640.6491960.0036490.0015970.0010710.0000640.0001280.0000600.0000780.0024500.0000690.0000600.0000600.0048280.0000790.6002580.0023510.3055130.0012170.0000650.0027140.0000760.0005340.0000610.0005400.0010430.0000630.0000600.0010090.0000630.0005350.0005350.5993300.0022081.6563470.0085890.0005160.0005471.3792030.0053780.0005532.5867290.0143120.0054630.0000780.0005610.0005360.0000610.0010870.0006090.0420800.4942060.0022110.0029820.0000710.0024370.0024470.0000690.0010050.0005360.7556330.7604930.0034830.0000700.7593710.0029730.0005440.0000620.0005330.0005340.7238380.0023430.7329670.5970280.0028660.7367230.0023470.0000670.0000590.0005322.5618900.0128130.7602181.8263761.8442140.5520770.5504930.6007510.0293160.9393860.0038190.0054670.4840280.3910590.9972681.9573290.7705590.0096230.0068850.0010310.0016340.0010990.0036510.0043342.2621320.0079190.0000830.0662570.0002470.2442290.0007460.0031790.0000690.0000600.0007090.0035960.0000690.0023470.0022440.0005370.8460051.5662960.0043151.3449501.4670880.6918770.0638290.2137470.0006390.0025520.0000711.8063520.0058760.5635960.4026000.0011160.4333590.0011910.0000620.0527630.0026730.0000680.0615370.2501140.0016510.0005560.0010900.0029420.0000680.2008200.0012430.0010243.8380950.0125650.0015940.0015360.0131690.6415230.7209190.0050141.4132330.7296711.6944610.0244331.2495090.0031010.0025900.0032240.0671230.4373770.2226020.0615871.7705611.3428790.0036400.0010620.0005550.0011180.0010450.8865372.1642980.5292990.0093510.0093090.0034081.2172400.8853160.0050620.0632100.0525540.5427040.0013021.3143070.0030540.0051990.5250820.0031340.5108440.0061060.1668840.6896070.0028680.0030120.1703070.1965150.0004950.7360240.0021610.0000650.0000600.7504891.4817000.0038031.4691990.0609920.3307880.0786690.1933180.0081280.0088090.0011790.0005730.0011250.0020120.0014920.0010300.0036330.6944151.6886950.6800950.0014930.0000630.0010320.7411740.0016040.2803700.1211160.9606990.0033120.0033140.0019380.0000630.0006050.0000610.0000600.0000600.5286220.0016960.0011030.4112070.7292040.0015270.0000630.0000600.9088120.0087150.0174070.0025290.0046330.0117490.0058930.0017820.0054510.0076230.0711700.0329350.7222130.2494190.0034970.0026071.1942020.0030480.0000651.0844350.0079520.2587690.7929700.0040960.0715490.0001982.7695270.6392580.7391081.8086750.8552830.0030910.0019770.0124800.8538931.2469190.6307230.6336910.5777561.2675080.6685540.0017670.8866660.6585930.6674861.5151181.3319660.0024950.0005650.0000610.0000600.0000600.8649500.0016240.0000630.0000600.0000600.0012270.0031540.0046160.0052700.0016520.0005380.0010760.0017880.0001110.0015290.0015800.3707840.3499430.0044610.4987920.6528710.7231122.4314630.0055772.7290150.5053380.2373412.2881190.0064200.0025640.9767530.0032000.0000650.0000650.0492910.5412360.0076160.0005450.0015040.0017850.0010830.0015220.0019730.0005800.0014830.0000621.2642590.0035250.0000651.2291742.0540021.2778770.0050551.5557350.4899770.0008670.0000610.0024520.0000640.0000600.0000590.0000600.0000600.0000600.0000600.0024650.0000640.0025890.0026890.4478362.3411651.6303052.6941640.0067710.0048360.8678401.7317942.4642120.0059210.0015390.0010290.0015650.0015200.2285280.1024900.0062110.2384930.3013040.7362330.1849480.0022480.0010140.0010920.0020030.0000630.0000590.0000600.0000600.0000590.6252260.7696470.0012380.0005370.0005340.0000600.0005590.0013090.0000610.0005320.0024810.0025360.0567220.0119580.0134880.9236250.0050150.0006060.0000600.0036740.0008040.0019380.0011740.0013280.0040980.0019470.0043870.0050120.0043100.7132710.0039630.0010421.2294701.6434292.2591821.8462550.0108990.0031640.0021070.0026280.0010210.0029280.0010100.0014910.0010261.8391160.6309360.0036060.5876380.7573440.5461470.5964680.5134550.5164920.0042570.8728880.0033170.0032710.0017800.0019380.0006360.0016710.6579480.0039420.0000660.0000600.0026340.0025940.0000640.0026160.2583550.2853780.0004540.0000600.1781990.2148580.0003550.2020740.2323250.0003780.0000600.0978110.0740210.2191500.0726310.0012330.0026780.0021171.5925574.3164815.6952582.5149930.6270420.3943170.0005890.0000610.0000600.6598970.0015280.0000610.0000590.0005790.0005931.6393960.0026660.0018680.7630230.0010660.4692190.7513350.1299350.0245690.0060241.3306910.5919571.2410940.0016770.0000610.7716060.6375030.6028620.0013430.0015670.0005410.0000610.0016010.0010490.0010070.0010750.0025250.0017030.0010400.0015492.1128990.6371610.8030061.4292072.2522202.0401401.3102100.7219931.3437180.0017540.0006950.0000600.0000590.0005320.0000600.0005330.0011550.0000610.0000600.0007690.0000600.0010060.0006550.0005660.0007620.0000600.0000590.0000590.0007690.0000600.0000590.0032770.0059540.0064840.0064630.0112570.5489590.3838772.0202031.0204940.7510390.1550070.0043872.3430110.0127370.0072550.0072020.0073980.0013010.0006190.0000600.0015250.0000620.0016090.0012150.0006210.0026260.3860410.3237530.7606740.3780800.0068870.0006220.0000600.0020150.0020140.0000610.0000590.0007770.0000600.0000590.0000590.0000590.0005322.0357002.0895682.5943590.4933711.0206370.0043680.0031540.0014230.0031700.0100730.0013440.0032670.0061960.0058820.0122641.8313471.8632051.9133710.1223100.7050341.3398192.7693131.2823030.0086470.0072440.0052000.0000660.0040160.6445240.0046360.0039980.0039691.5140581.5503241.4011350.0040720.0000640.0015490.0014980.0020080.0010090.0098670.0028360.0056180.1579070.0026870.0000631.4941800.0069970.0082890.0037470.0000670.0235300.0002311.6017780.2369550.8540381.6935211.6511590.0018600.8625210.0033810.0000640.0098390.0069341.5980901.8338980.5273520.7103790.0054920.1761960.0898270.3032510.0045211.2541780.4606010.6399860.3381702.0003300.0021940.0000620.8559900.0009700.0005772.3150300.0025140.0000623.2927030.6514200.0032210.5754130.0011440.0005360.0005470.0005360.0005550.0000600.0000630.0000610.1865081.4875841.6505162.0223252.0519390.2197720.0002870.0006400.0005580.0010250.7063620.0036530.0063330.0000670.8769570.0163090.0050400.0154120.0000750.0000590.0000590.0000591.0317270.0011120.7343570.0012890.0000600.0006950.6506430.6109050.6314631.1295640.4262720.6794400.2172110.0031670.0048890.0025340.0005490.0014990.0010080.0015270.0010400.0015280.0021830.0010460.5375260.9302260.0009850.0014431.8848680.0047050.0000640.5237530.0005770.0000600.0097970.0000700.0005780.0000600.0049320.0000640.0000600.0000620.0027030.0025030.0000620.0026130.0000620.0033000.0146270.0049050.0000640.1958520.0053690.3370700.5084540.4548741.7341460.1703310.0031100.0020160.0015170.0025470.0090880.2765120.0101240.3580370.0054870.6673450.4409790.5011020.0199320.3392730.3417650.0028810.0027430.0109790.0033930.0020880.0058550.0149160.1601830.0045210.9563540.0020090.0000620.0005750.8225960.0056240.0097181.0487070.2556750.0452420.0001010.0000590.0030790.2965790.6081000.6056180.0006210.0043860.0000630.0000592.0717160.5562300.0127710.6070400.0023080.5373130.0092050.1770670.0006940.0874770.2403880.5376820.0023230.1438840.0028030.0012760.0005330.0078320.0156500.0043750.7002770.2395821.1380790.0045910.0032370.0021480.0024870.0030220.6691650.0302540.0035890.0167860.6446150.0053450.0240180.0015020.0018932.7416752.0611940.1211280.0029410.2078540.0041160.0063640.0137340.0059040.5604700.0092220.0044210.0054920.0111330.0059620.0299300.5908220.7015911.1732790.9022200.9977560.1148880.2466640.4571530.0628330.2057650.6236400.5328680.4586410.7282840.4988850.4725891.0526920.1134590.1312980.6532360.2939610.2650830.1122060.1101350.5140490.4310781.7377220.2184140.4042160.3586860.8937100.0540850.7086990.8855041.0532460.1771600.2031170.4644861.5027060.4629130.5089770.2445340.4408191.2291110.8916120.0045180.2198820.2818110.8036520.7542640.2738700.2461290.4145421.2987980.3805591.2733850.0257180.0854320.0001400.0159910.4446030.3509580.3489650.8733870.2918321.0598160.1523092.0776220.2066151.2518340.1066070.1839830.2019910.6405780.0039680.4664381.1600570.0015290.1823390.2249810.0179941.0016271.3468831.1355310.6521250.9579970.8996190.7824651.0914970.9200181.2397741.7561570.0538671.4563910.0546880.5122400.1705460.4541000.9293010.0021280.5402210.2037820.8071860.0261460.1931410.2748860.1572890.2575760.2736250.5315940.6566700.0869220.2073240.7404510.2367110.2715860.0076520.4687050.0033660.6614931.5045691.2326940.7299300.1065170.4678410.4537740.5313120.6434970.0498920.3557630.7533330.9089291.5272950.2628010.2533430.8498080.5365820.5820060.4094170.1985820.7295070.0041050.3515980.7505090.5390540.0542900.1056360.2527850.0568860.2535800.5968370.4980160.0834220.0033391.6963260.5085970.2577790.0140641.6265960.4836770.2350960.0603790.2018882.7330161.2487600.4506820.8350611.2384730.0552880.0043560.0027651.7491460.0065490.6505641.1855280.2863100.0538430.5035561.1141920.8449110.0629100.4716580.2262800.0536920.3795370.3306320.6747790.1524650.4979720.2530890.3139520.1613040.5670460.0219150.8271530.3070920.1045970.5353650.6054470.3644550.3593140.0523440.6244980.1566430.1844170.5921210.0269230.0049800.3102750.9039720.9199031.0254780.6044751.9505550.2341660.2115320.1078240.3651890.1062610.0622650.5543850.0546470.4920560.7148100.2607290.5387851.9535920.2646390.6651820.2629080.2125810.8223240.0191380.2678060.8122770.0006560.6654350.7456290.7237990.0526741.3732870.7760600.0974140.3848630.0008410.4717811.1963871.0428461.4950931.2360580.6118530.0032170.0185670.5881300.4482940.3122290.1980630.1955840.0036630.9127190.1205540.6746330.0634440.3212370.3385410.2597150.7752191.4817800.1107180.8638670.4012660.2543700.0542690.2470900.2891671.2743420.0046260.9145070.3354201.1892060.8093170.1056790.9169860.4846080.5378460.4708840.2732511.3449181.2183390.2364652.7364661.5611330.3774980.8482330.5128660.6037190.9028210.1133950.2514650.1748650.4200741.2849570.4597160.9980880.2272461.3574451.3380230.3971120.7411740.0702870.7401810.1141520.5213350.3595370.7983910.2815560.4578870.4214930.4897691.1163330.4950711.0232640.5113700.8584680.9537080.4688230.6912840.3688090.1052420.3681130.2722830.7038731.2445191.5831631.7853730.6649230.0756161.0066980.0155680.4770850.1634540.0634400.2163840.2399210.5504610.0546132.0438490.3512980.2341000.1177730.0005690.2182061.5775590.812494

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/
With following parameter combination: {'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fffdb304_2020-02-20_14-41-14u8hw12yd/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    600    |    89     |    134    |     6     |
-------------------------------------------------------------
| disagree  |    166    |    270    |    123    |    51     |
-------------------------------------------------------------
|  discuss  |    249    |    75     |   1459    |    41     |
-------------------------------------------------------------
| unrelated |    22     |    34     |    63     |   8516    |
-------------------------------------------------------------
Score: 8625.0 out of 9226.0	(93.48580099718188%)
Accuracy: 0.9114977307110439
F1 overall: 0.7353286690172482
F1 per class: [0.6430868167202572, 0.5009276437847866, 0.8098806550097142, 0.9874195605542351]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:00<3:10:43,  1.04it/s]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:07:55,  1.48it/s]  8%|â–ˆâ–ˆâ–ˆ                                 | 1001/11898 [00:01<1:25:37,  2.12it/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 2001/11898 [00:01<54:26,  3.03it/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 2501/11898 [00:01<36:11,  4.33it/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3001/11898 [00:01<24:00,  6.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:01<12:04,  8.82it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 6272/11898 [00:02<07:27, 12.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 4842.41it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 1488
  Number of epochs: 3
  Batch size: 8
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=1488.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.6845601.3691211.6677381.0770640.6552080.6521910.7650680.4952540.4478660.5709260.4430510.4262360.5566831.0259910.4592480.8221930.4373470.4116860.4088310.4074770.5415390.5469530.4108220.4038220.5379920.4074790.4016310.4008350.9974930.7189600.5451290.4035400.3985690.3980350.3976640.3973200.3969900.3966870.3963970.3961220.5310630.5341130.3986700.3952280.5301460.3977340.9918210.4070600.8282560.4028630.3940170.3936850.6639410.5336920.3958420.6635680.6682200.9667030.5378230.3950640.3925330.3923830.3922760.5273810.3941880.3920120.3918870.3917970.3917040.6902370.3958050.3915250.6900020.3954120.3913020.3911770.3911070.6614481.3651190.4032380.3910010.3907880.3907270.3906680.3906120.3905560.3905010.3904490.3903980.3903470.3902980.3902510.3902030.3901570.3901110.6886750.3931350.3900130.3899400.6603090.6629700.5277270.5263360.6896750.6911930.5277410.3909340.3896110.6881721.8289180.9998030.8005780.3931070.3894380.3893740.6879530.3918880.3893070.3892560.3892290.6878100.3916430.3891700.3891240.3890980.3890720.3890470.3890230.3889990.6875840.6898580.8250400.6626200.9599610.5283250.3898690.3888230.6874050.3909390.3887711.2845630.5302760.5249000.3896310.5238710.5247790.5247600.5247340.5247100.5246860.6598680.3903290.3885240.9293120.6624020.6606400.5253970.6597100.3901340.6870220.9874651.3909771.6677950.6666030.9308470.7972120.3907570.3882960.3882650.3882510.3882370.3882230.3882110.3881970.3881840.3881730.3881590.3881470.5233390.3888780.3881150.3880990.3880860.3880750.3880640.5232570.3887680.3880370.6866320.3895920.3880100.3879900.3879800.3879690.3879580.3879490.7935561.0660160.6899510.6880360.3894000.3878971.0639080.7968160.8236760.9589980.3906160.6582580.6595310.6595270.6595100.3890790.5229930.3884080.3877750.3877630.5229600.5235750.3883610.3877330.3877230.3877140.6581170.9861220.9875780.5255540.3882840.9848871.2257520.5265170.5234490.6586360.3887951.1200470.5259510.5234030.3881770.7932130.6597030.5239250.3881420.6579730.7942860.6878240.3887630.3875320.3875201.2229930.6612840.7942170.6595450.3885870.3875020.3874920.3874850.6860890.3886400.3874720.3874620.9846740.3897481.6285420.3921720.7890970.3889450.3874240.6860260.3885290.5226130.3879010.5226000.6582970.3883780.3873810.6577820.3883500.5225700.5230520.8216560.3889031.3862250.6895000.3884040.5225370.9850110.5246040.5229811.8525901.1248161.1222740.3898300.5225050.5229540.3877450.6576890.5233960.6863341.1206910.5249310.8215300.3886980.6858600.6868400.3882270.6576470.5233210.6862770.6868030.6586000.3880910.8210250.5238050.5228440.3876300.5223980.5228230.6862220.3881250.6857890.6867120.5233070.6861980.3880900.3871610.3871550.5223561.8806060.9889300.6875771.5542811.3935021.0661861.2003991.2007941.7980132.8229210.6647720.6865401.1204050.6878730.9851991.5832851.1230130.6878441.1203680.6596140.7934800.6586540.9286731.4702501.4717770.5253600.3874530.6856680.3878970.6856630.3878870.6856550.3878750.3870400.5222400.6578160.3877770.7926440.5233370.5225990.7890471.3869850.3897261.1194381.0892520.8227090.7898290.8218840.3881550.6856010.3877811.4180180.6601250.6863120.6863760.3877630.6855850.3877510.3869710.7886260.6866080.3877300.9238191.7179730.6607720.5228510.5224930.8210990.3880401.0872060.5239050.6576872.2862591.2029190.7945680.7935510.7935430.7935341.1991540.9297290.9290680.7938531.0921242.7775221.3312510.3892000.3869070.6855061.0878860.3885820.9841140.7899802.1191541.2586390.3889420.6572830.5227190.6576060.5227160.9240542.2264361.6362201.6348251.8264101.3366362.2273711.6603970.6602030.9283020.7937081.0638070.3883961.1901670.3886710.9840631.8530370.7957640.6581621.0916632.1518640.9316070.6584550.9282520.9288460.7936410.3877170.6854230.5226770.3871090.3868100.6854151.2550860.5239050.9843170.5233011.2829190.9577550.9852401.4191171.6904550.6599940.5225740.6574830.9281800.7935470.6580500.6859652.1896222.6975260.6902400.3874030.3867640.6571740.8211400.3876632.2814672.1686390.7960650.6580161.1985520.7940430.3875890.5219600.3870280.3867510.3867480.6853450.5225560.5222191.4180400.3888200.3867430.3867360.3867330.9839510.9851360.9851450.6865240.3873170.6853291.3619480.7942561.1199411.2839870.7901400.6861182.1291561.7303021.2005561.0643170.7936280.5226970.3869641.3252291.6335541.4707391.2282070.6869120.9844700.3878331.5811220.9579691.6570562.4511561.7990290.7949600.9282711.6609482.7176051.4220740.8224290.6861010.8210450.9847021.0920070.5231810.7885820.8212230.6860761.3578960.9856530.3877480.5218590.3868980.3866500.3866470.3866450.3866460.3866440.3866420.3866400.6570510.7927500.6577900.7927510.6577850.5223360.6572980.7927410.3873590.7922600.7929690.6577610.6857230.3871611.6841222.5363662.5981961.8658650.5244132.1181641.9614682.7178702.5236030.6889210.6857571.0631630.7933900.3873090.3866091.1978481.4696410.9292730.5227340.7924540.7929160.6577090.7926830.9281110.5227230.7924530.3872881.1190320.6582420.3870590.9838231.5216781.4943621.1996681.9276411.4950260.3884240.3866010.6852050.3870850.3865940.3865920.3865920.3865900.3865900.3865890.6851960.3870700.6851960.6856752.1505201.9612640.7947271.0632620.6862630.9842690.9243882.0019782.1670920.9302130.7930410.6576180.7926060.7928171.2830311.5824101.3348821.7361472.0635802.2274911.6344770.9293260.6578150.6573910.9277990.3873940.3865540.3865490.3865440.3865470.8203601.0914350.3876230.5217520.5219610.3867550.5217530.5219580.3867510.5217530.6853620.6856081.0872671.1200171.0636670.6579641.3333850.5231620.3867420.7921540.5223460.6571490.6573480.6573471.1981680.6581441.3333691.4695681.3345570.5231241.0627500.6579251.9869701.7973191.4702281.4697531.5543351.1994430.9285191.0633340.6579061.1981350.6580980.7925160.6575091.4179320.9570110.9281511.3901182.1192452.4510471.4710910.7936670.7927001.1983111.1988880.9284781.0633000.7930760.7926930.5222790.7923091.5211490.6866960.3869190.3864980.6851040.6855200.3869100.6851030.6855211.1902320.3876080.3864970.7881540.7887070.3870460.7881530.6856520.3869000.3864910.9837001.1197220.9565041.0914850.6578530.9276720.9280371.0874802.4494913.0269331.4919660.7896281.0872900.3874180.3864790.3864770.6850840.5220790.3866540.3864740.5216780.5218501.1899690.3875340.5216740.6852570.3868643.5997602.7619222.3309522.6144721.6344301.2557170.6862080.9840690.3872410.3864630.6850690.6854570.6854540.5220430.7922430.5221790.3866320.7920680.6573860.6572100.6572080.9276210.6575570.6572050.6572101.3856760.8215401.0590771.3258321.3865211.4187500.9849740.9240710.9843440.3872010.5216530.3866160.3864460.5216500.3866140.5216500.6570230.3867800.3864420.5216460.3866080.6568510.5219810.5218120.5218110.3866050.3864370.3864370.5216390.3866020.3864350.6568401.4684071.4693991.2553851.5819281.2555192.1896701.9001232.5921162.1631081.4984241.4694172.9223551.5839241.2836961.2833331.2833270.6579160.5219520.3865890.6568360.3867480.6568310.6571510.5219470.9274061.2225381.3862891.8202961.7886390.6866870.5219760.3865760.6568260.6571440.3867320.3864140.5216180.3865710.3864120.3864110.3864110.5216151.1977981.4689912.6985251.1215131.1196800.8210680.8207210.6573170.8205300.9841120.6575020.8205271.1981341.4689731.4692861.9594931.9600491.9600521.3910312.2822471.8251862.2263131.9885231.2840341.2832380.9846260.3870691.4680281.4692511.4692541.4692551.4692441.7960581.7964231.7924691.0639890.3871490.7920070.7924580.9276600.6574030.6853040.6853340.9839420.6856630.6853310.3867221.8190521.1203751.5534220.6862720.3867130.6849900.3867101.0866340.7888020.7884681.1901391.1905640.3872571.3852510.6860760.3867061.5808081.3905012.0623592.0630941.7966341.8527371.0921751.1944031.2548711.1197291.0913761.1195480.9561741.4180142.7763242.0880440.3881860.3863710.7880270.3867960.5215741.3853760.3874270.3863682.9209721.0893170.6857180.5218820.5217140.5217080.5217110.5217130.5217120.3865060.3863630.3863621.0623892.1650212.0913161.4191671.4184682.2821300.3883280.5215650.5217030.6569090.9838460.6855850.9838850.3869731.3852310.6859930.6852660.6852720.3866600.3863540.3863530.3863530.9835660.3869620.6849560.5218580.3864840.5215560.8203010.6853980.6852610.9838670.8207651.0910041.5999470.6861780.9838750.6855630.5218530.7920930.6571640.7922330.6571600.7922320.9275710.6572950.7922190.5219490.3864790.5215500.7920820.5219500.3864760.6567530.3866080.3863410.6567520.3866030.5215430.3864680.9835550.3869200.3863350.3863340.6849440.6852350.3866250.6849420.3866240.6849450.8204410.8205720.3867580.8201500.6853640.8204381.3252792.7584572.8628831.0647481.1982130.9279320.7924641.0627451.6884231.0878541.5814431.1199031.3616591.3336961.1984670.6575140.9556060.8206660.8205420.6853370.6852050.6852050.6852080.9274261.4684791.4689821.4689661.4689621.4689700.6577400.3865640.5215140.7920500.9839141.5813281.6888722.5808341.5828070.3874380.3863200.8201280.9839401.0871430.7886290.3866920.6849230.3865940.3863171.0623441.7165701.8805741.0637210.7925560.6571000.7921800.9275090.5220160.6568490.7921730.7922970.6570950.6569710.9273790.6572130.5217641.4174642.1225081.4695340.9281180.6572150.9273751.3332361.1983950.9278611.0628241.1981461.3334701.0631831.3333522.4774232.4502490.6585701.0625730.5221160.7920421.4683221.1985041.6845180.6860720.9837901.0910541.3897622.1506381.2554981.5815061.5535991.2267671.4968861.9873311.1201360.6855440.8203801.3573530.9240220.5219741.2218931.1903980.6856110.8203970.7883470.9235121.2222520.8208310.7883330.9235460.9557811.1192070.9237781.1900751.1902991.3255131.3256381.1904271.1902981.1902861.2224500.9842381.3856530.9843641.0870570.7885391.3855040.7887880.6570860.6569450.9233840.5219450.9232770.8205500.8204660.9838731.3856750.6857450.8203571.3855040.9843360.6854010.8203590.8204561.2221200.5221811.2822110.9239020.7883950.9556260.6571601.1189220.6855101.3854170.3871171.2821280.9239321.4886681.4891821.1196250.5220861.3249050.9842681.3253211.1903911.1902831.1902410.5221450.5215981.1896820.3869370.9834871.0870250.3868360.5214730.5215740.6849960.8203311.3854870.9843070.5219641.2218710.5221640.8202001.7163070.7890140.7882481.2220610.7886031.5915560.7889090.9838081.5917690.8210590.9556930.3867280.6566750.5216890.9553950.9839610.5219500.6849781.5915230.6858441.3066291.0872541.1192320.6854440.6851051.0585740.7884711.2824190.9842071.0870180.6854280.9233811.5208080.9843861.0869961.1901381.1193370.9237181.3855391.2828920.6855810.7881570.8203871.4176281.0591450.6853910.9233711.0869391.6239371.5921710.7888500.5217841.0866300.3868011.2217281.1193471.3253270.7886331.1898760.6854730.7881350.9234240.7883201.2823700.9841670.6853411.7870561.3862161.0872710.6854251.3893130.9842550.5219271.0866170.4886341.3839170.9842280.3866941.1187051.1901210.7885410.6851670.6850900.5216760.9835780.7883731.5206500.6857150.7881311.2220240.9840861.1900160.7885371.2220350.9237340.7883320.9234111.1899720.9237280.5218520.9835670.9235481.3252431.5919640.9240280.9838581.3252551.0871981.1900821.3857271.1902761.4887531.4889800.7887210.7881911.5914950.7888081.0868130.6853790.6850791.0867301.0870070.7884140.9837700.5218861.4882580.9842831.9933251.1907521.4887611.1903760.7884871.3854170.7886391.2220311.1383180.6853941.6235821.1904620.9236941.3251631.0871800.5219541.3248700.9841450.9235411.3855150.3869740.6566820.9554381.0869370.7884080.9837261.1190740.7884151.4884570.3870320.8200530.9234130.6570270.9554481.2824631.0871480.6853531.2822631.0871500.6853500.9836700.5218750.8201500.6851521.0867091.1900401.1192320.7884621.3250161.4888240.9238850.9234751.1899191.1901121.5917740.6575200.9233230.7882450.6851171.0866971.0588080.6853081.1897471.1901310.5219960.6849251.1897810.9839951.2221440.5220160.9835241.0869131.3574211.0871710.9839331.1899410.9236421.1190290.6853421.9221811.1906440.9236591.1899210.6853920.9836611.3855211.0590080.7883520.8203030.9837351.0868971.2221790.9236650.6851991.0866990.6571161.1897361.3856381.4888370.7886490.9233741.1899291.1900970.8205961.3854350.6855210.8202381.3854121.4888000.8207771.2220470.9236591.4885111.1903001.4886881.3444091.1232141.2221921.1191930.7923341.3853740.7885351.1189270.9839400.7882951.5915000.7886810.6851020.6850300.6568240.7880701.1897741.7270280.9240461.1899430.3867550.9834361.2824410.927650

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/
With following parameter combination: {'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_e8fcbb14_2020-02-20_09-04-46n0vqk61o/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |     2     |     1     |     2     |     0     |
-------------------------------------------------------------
| disagree  |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
|  discuss  |     0     |     0     |     0     |     1     |
-------------------------------------------------------------
| unrelated |   1035    |    467    |   1777    |   8613    |
-------------------------------------------------------------
Score: 6461.5 out of 9226.0	(70.03576848038153%)
Accuracy: 0.7240712724827703
F1 overall: 0.2109713967898165
F1 per class: [0.003838771593090211, 0.0, 0.0, 0.8400468155661758]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:01<3:22:18,  1.02s/it]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:15:40,  1.40it/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1501/11898 [00:01<1:26:38,  2.00it/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 2501/11898 [00:01<54:50,  2.86it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:01<26:08,  4.08it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 6209/11898 [00:02<16:16,  5.83it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 11501/11898 [00:02<00:47,  8.32it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 5512.61it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 744
  Number of epochs: 3
  Batch size: 16
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=744.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.4211140.4223200.2113610.4047720.4619130.0925100.0164700.0026410.0004200.0001380.0002270.0001140.0001620.0001020.3357690.0240170.0016450.3023620.0168890.0183270.7339950.4468350.4253140.0191200.0014070.0001450.0006330.3635981.4371110.0498250.0017530.0002430.0001000.0000951.4689791.0123690.9686480.0262700.0009050.0136820.0004290.0005900.0001020.0004030.0001300.0000900.0000890.0007060.0029220.0003280.0003040.0008340.0015500.0001280.0535620.8588280.0154240.0018590.0001190.0000890.2292590.0043460.0001560.0000920.0004250.2333760.4735870.0073020.2782210.0041190.0431470.0086510.6540790.0091890.0003400.0002610.0003430.0003400.0002950.0003380.0020510.0016720.0008960.0001010.0000910.0000910.0000910.0000910.0000910.0001540.0000910.0000910.0001530.0000900.0021460.0001100.0000880.0000880.0006120.0019210.0001060.0007970.3397460.0035250.0005130.0002310.0001630.0000900.0002160.0000910.0000900.7003470.0092200.1982240.1661640.4779390.0048080.0003300.0014840.0003230.0002540.0012890.0000970.0114860.0006690.0002200.0001510.0004060.0000900.0005980.0058010.0008040.0000940.0058100.0002010.0002830.0000920.0002220.0002220.3016730.9240820.0067040.5882600.0058560.0015340.0001710.0001520.0002760.3788810.0030480.0003550.0003560.1230741.3655310.0090780.4527390.0030530.0002840.3260820.6546770.4261870.0027360.0001800.0034490.0962130.8525560.0597771.3562020.2610190.4883802.0007180.7607870.0057850.0004740.6075553.6324250.0209790.0004490.0003320.0000890.5976570.0049390.0003090.8280320.4545490.6889600.1829870.0014160.8646560.0137650.0366170.0023020.0309500.0016010.6007210.0033920.4854310.0036900.0003312.3784040.0124510.0006500.0009360.0007140.2515410.0036490.0903780.2875180.4180741.7865700.1116700.0008140.1782910.2077940.0030890.3393190.3452760.0662070.1562170.7326530.0051740.3641740.4242950.6847820.3253000.3584070.2943420.0013841.0756950.7528761.5110342.1223000.6049010.0028960.0004970.0003980.0032850.0511780.0003030.0017380.2080570.4844610.0035540.0004390.0003400.0000890.0013640.0039650.0796200.0004080.0020270.0033090.0039450.0028130.0023840.0024100.2668790.0021530.0010390.0517440.5967501.1167520.7911481.2772610.1719880.0191770.0025980.0441690.3627480.0024900.0050860.8114020.9300420.1214460.0006080.0000950.4500300.0017170.3922880.3139820.7044580.0028270.0003220.0005560.0865330.0024350.0106510.9112400.9512341.8652870.0071340.6485010.0023111.2555590.3365290.0016460.0004390.0004230.0002790.8078250.8401771.2123370.2950390.4413070.0019530.0000930.0000870.0000870.0004230.0005200.0023391.0639300.6906430.1026360.2965260.0014790.0004270.0358520.8293930.0654710.0083890.0005070.0004590.0000920.0000890.2593850.4361740.0015630.0001590.0001790.0004760.0016420.3545860.4188090.0014580.0006900.0007920.0081190.0019800.0011590.0034650.0026742.9261701.7924840.0063560.0009570.0006180.0004171.5786630.5945160.3250560.1877000.1618510.0019540.2953490.0018620.1694920.0010300.0004930.0004090.3315460.6626510.3484390.3662040.6243220.0017980.0127840.0038610.0005380.0014320.0091000.3129750.0492810.0002200.1850440.0005800.0002790.7537480.3964180.0058640.2783050.0041481.3696270.9209010.4832570.8903660.0100600.0001790.0004070.0003740.0004790.0004481.3799470.6660742.2744150.7986490.7553840.0033890.0001580.0001510.0008960.0068610.0003260.0024680.0047730.0001000.0028370.0012210.0018270.0043910.8639762.7419050.3156831.7642760.0430700.0299410.0005040.0003460.0003680.0005370.1694100.2377430.0820310.0003460.0009420.0000890.0001830.0000880.0001493.1017351.7552230.0657680.0023750.0084900.1218180.9043310.8609060.0613520.3294470.5157301.4654621.6178050.0045310.1930520.0018501.9455860.6709030.0018440.0006390.0006330.0009530.0006570.2946570.0025000.0012840.0462131.0612361.5065001.0099800.5080920.0024110.0806730.0981590.0027960.0033240.1395250.0037540.4387150.5096500.0022190.0001610.4438400.2874330.2955930.0008690.0003610.0002740.0000900.1989840.5469590.6739540.0046690.0052280.0043230.0009660.5100530.0018620.0003550.0000900.6751850.1321130.0004840.3514740.8045581.1554300.6775250.0024370.0004630.0004510.0004150.0379320.5907220.3406121.7136950.0529290.3568100.3071970.0007620.0032870.0001170.0035180.0115830.0030990.0010600.0006120.0011560.3048980.3499250.0014870.0005390.0017960.0023390.0016690.6018650.2050210.5096090.0028010.0016640.0010720.0014940.0104650.0004740.4513110.0027270.5314010.0556720.0009520.0192460.0008260.0004860.8260740.0059971.0373970.3399000.6597690.8359830.6488740.7324970.0016050.0024581.3412531.2407100.0033380.0007680.0015530.4531710.2443890.2609250.0009043.0387960.0086200.0018600.0030370.0020170.0017190.0022260.0744070.4001430.9878700.7611200.1797000.3814371.2230470.3419690.9616710.3817640.5366140.0496010.1969411.0473410.6103670.4115740.3787650.9444300.6059670.3177980.6213590.1528690.4302790.1643350.3461420.3593940.2348930.1707400.6812200.1908720.0327420.6584800.5358570.6938050.8564830.0497280.0703760.6557010.0748130.8507420.3011010.3934770.2148331.0208550.4360930.7308441.1857081.0216500.2768480.1670400.1227490.7290730.3058680.1133910.2599590.2048470.4709560.3362600.1472050.2046800.3329230.4787480.3619110.7503890.2131360.2122870.6623780.4804200.7992070.5485010.1216170.5424220.3428880.4037100.1755470.3564470.5546580.1912501.2246580.3468550.8489330.3848881.2844950.7022990.4372070.1666600.5328000.3638000.7327080.5164310.9200630.3855780.3046250.5307190.9364590.3464950.2127040.2906310.8604120.3499010.2009050.2119890.4392540.3176190.1374600.4675160.9520970.9788730.1686430.4168400.2009270.4634280.4367790.5729210.3846640.2413680.6347030.3904460.2158490.7788670.1419710.9639190.2867660.1443891.2763361.4153180.0613810.2137430.0467900.5376480.6957510.3963450.4368470.3813691.0831420.3436220.1829290.1405120.5287260.1454520.4292710.3473550.6297420.4164660.5315230.4487931.9689560.9067660.6136200.5247840.3163580.3833530.3617510.5457040.4114800.4121280.4462770.6208090.2558510.2926900.7613340.3291600.7694420.4450990.4728430.3119350.4109851.2525031.0785750.4151210.0270080.0512660.1850180.6171101.3471710.1843490.1195700.727746

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/
With following parameter combination: {'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_9b7fcfe4_2020-02-20_19-38-54lz8gg1e4/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    732    |    105    |    150    |    11     |
-------------------------------------------------------------
| disagree  |    83     |    267    |    74     |    10     |
-------------------------------------------------------------
|  discuss  |    204    |    69     |   1483    |    65     |
-------------------------------------------------------------
| unrelated |    18     |    27     |    72     |   8528    |
-------------------------------------------------------------
Score: 8683.25 out of 9226.0	(94.11716887058313%)
Accuracy: 0.9253656076651539
F1 overall: 0.7808887418503028
F1 per class: [0.7194103194103194, 0.5920177383592018, 0.8238888888888889, 0.9882380207428009]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:00<3:12:41,  1.03it/s]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:09:15,  1.47it/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1501/11898 [00:01<1:22:33,  2.10it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 5501/11898 [00:02<35:33,  3.00it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 8501/11898 [00:02<13:13,  4.28it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 4665.17it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 2975
  Number of epochs: 3
  Batch size: 4
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2975.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.8540040.8560270.4300400.1824100.1261980.0272710.0188300.0048370.0026650.0023180.0023030.0146880.0284740.0042460.0023460.0021950.0021830.0021720.0214950.0031810.0022090.0021440.0021370.0021460.0021440.0193560.0027871.6216520.0599770.0041130.0021870.0390091.2268820.0392410.0031760.0022520.0020951.2269580.0343160.0029110.0143910.0023750.0020820.0143630.0023600.0020920.0021010.0020980.0143880.0023400.0020990.0020830.0020880.0020770.0020970.0020810.7908410.7496400.7094201.2388640.0350640.0026000.0020732.4474790.0402690.0026490.0024391.1916951.2371760.0199700.0023110.0020560.0020580.0039630.0020800.0032881.2263701.2414370.0179780.0022650.0144561.2238250.0169541.2363051.2413661.2401261.2365920.0162840.0146790.0023641.2268430.0155190.0021950.0733050.0028560.0020760.8047330.0103440.0021490.0020700.0020730.0020630.0020660.0020630.0342660.0023760.0020480.0180790.0022110.0020710.0020670.0463070.0231570.0174280.0355630.0385620.0976010.0028710.0020550.0020560.0020560.0020460.0020540.0020510.0020810.0020470.0143980.0021570.0020520.0020540.0020550.0020970.0020560.0020460.0020450.0021042.4500882.4681472.1033740.0171540.0041762.0679540.0166182.4508161.2436660.8691480.0079830.0020860.0020380.0020990.0020400.0020360.0020380.0020490.0143100.0144170.0022051.4460090.0111950.0021310.0020460.0020430.0020670.0020440.0020470.0020550.0196790.0021630.0021470.0020530.0020440.0020520.0020590.0020650.0020700.0020401.2100510.0088640.0020830.0020450.0020470.0020860.0020360.0020820.0025000.0020500.0020410.0020480.0020380.0021120.0374220.0022640.0020410.0020860.0022370.0020620.0022120.0020780.0041580.0267200.0268550.3022760.0040541.2374980.0204370.0033950.8485770.0061261.2269780.8643360.0184390.0021150.0020420.0020450.0020790.0020560.0020340.8343600.1559300.0518700.7582410.2045811.8524170.0103330.0020770.0020410.0020330.0020380.0049570.0020520.8368490.0056620.0020550.0020470.0020570.0020420.0020470.0020520.0020490.0020490.2673250.0031400.0020480.0020350.0020350.0020500.0020430.0020740.0020470.0020880.0020370.0020340.0020370.2842030.0031420.0020480.0020370.0020411.0456140.0061040.0020500.6845350.3433040.0033300.0266630.0025430.8601970.0300450.0021450.0145880.0021360.0020420.0020600.0020410.0020380.0376580.0021610.0020410.0020380.0020340.0371850.0725620.0022950.0169500.0177370.0021370.0020520.0020520.0020460.0157490.0177670.0021920.0020600.0144160.0020900.0143280.0145910.0020890.0144550.0020820.0144040.0145880.0021150.0020340.0020450.0020430.0267461.2515690.0184410.0143780.0021190.0266170.0144170.0020691.2273700.0307220.0026580.0028530.0372750.0022210.0065450.0724000.0024581.2195620.6613060.0408640.0021680.0266210.0143790.0388970.5982390.5521570.0036920.0020360.0020330.0020370.0020350.0020360.0020400.0020360.0020360.0020480.0020390.0020440.0020400.0020380.0020380.0020330.0020360.0020430.0020310.0020370.0020470.0020490.0020420.0025161.2326680.0054800.0020490.0020560.0020370.0020330.0020330.0020390.0020400.0020350.0020380.0021690.0020340.0020390.0020390.0143050.0020700.0020410.0020370.0020600.0374040.0021400.0020290.0020450.0020580.0020350.0020380.0020430.0020510.0020340.0020310.0020360.0020380.0020450.0020930.0020410.0265750.0143750.0511600.0144190.0372730.0021380.0372040.0021200.0020450.0022260.0020350.0020330.0392670.0270700.7972230.0285001.2260520.8725520.8717510.0164221.2220510.0050540.0143040.0143420.0143370.0143340.0020590.0269300.0312840.0021050.0020490.0020430.0143150.0020920.0020600.0020370.0020470.0020430.0020340.0020540.0020440.0166580.0161730.0020730.0020330.0020380.0020390.0020770.0020470.0021090.0020390.0020490.0020410.0714740.0522960.0373950.0380160.0374100.0021180.6836351.2145500.0048390.0021580.0779120.0379600.7008810.0035560.0143470.0020581.2326730.0170420.0144110.0020720.0020310.0851271.1682940.0045460.0179830.0164000.0020870.0020650.0020470.0020561.1037540.6077330.0032990.0143010.0020560.0020910.0020280.0145320.0145640.0269560.0148740.8508430.0037850.0020670.0020490.0020280.0020430.0020340.0020500.8343941.1645920.0043950.0265870.0143530.0265650.0265990.0024450.0020310.0020290.0020360.3483260.0027180.0045510.0020870.0020330.5754170.0031490.0020590.0020290.0020660.0020310.0020480.0020390.7652420.7152932.0951341.1462760.0359412.0277130.0060090.0021100.5498990.0030920.0020820.0021570.0022590.0020610.0020290.0372380.0057320.0020370.0144830.0020550.0020340.0020360.0020450.0147380.0148670.0145270.0020590.0020440.0020350.0020370.0020380.0273400.0020830.0020320.0144541.2254990.0042340.0143060.0020590.8717870.0037580.0020350.8605362.0082160.8636610.0035660.0020420.0020450.0142920.0020630.8593580.8610350.0035370.0142920.0143200.0020490.8708732.5608770.0699150.0374420.0847670.6881040.0032120.0020340.0142820.0020480.0020330.0142880.0020500.0020350.0020300.0265510.0143290.0020680.0020480.8586010.8582960.8688430.0158180.0020770.0497180.0021340.0020480.0020591.1077210.0038860.0020940.0714050.0021830.0020540.0266550.0020830.0020460.0144490.8576160.0034450.0020510.8576070.0157691.2346000.0040380.0020500.8722420.0034490.0143270.0020710.0020420.0144250.0020700.0020670.0020450.0143450.0143930.0020690.0020430.8349960.0033630.0020600.8274340.0033480.8353720.0033520.0020550.0153570.8260071.2055830.0039200.0020450.0020610.0020530.0020790.0020610.0020470.0152161.4711492.1813900.4729500.4878571.8429390.0048290.8453453.1624291.6121021.2152710.0240940.6621400.4836470.0527430.7306530.0696010.0525070.7715321.4750370.1221590.0268740.0020830.8561790.0032951.8554900.8602910.8424620.0032790.8560360.8572110.0032832.0167820.8426420.0032810.0020510.3722670.2101060.0146920.0143160.0143330.0144120.0266270.0020661.7371230.0167920.0390230.0514750.0514290.0514210.0516230.0144730.0440790.0020910.0020310.0020280.8607940.0032560.0020520.2398460.0023650.0020280.0020300.0020340.4767650.0026930.0020481.2267920.0037410.0020440.0150850.0155620.0147760.0020620.0020360.0149350.0276713.0286320.0061930.0147690.0020830.0020361.1342510.8419841.9733950.0047310.0020500.8424090.8429950.0031711.9623590.8398010.0157880.0020551.1497541.8178720.8548620.0031710.0020420.8532710.0031670.0020410.0020360.0020372.4968230.0409910.0020820.0373780.0020890.0889010.0021510.0020310.0020370.0020260.0399790.0020830.0020420.0020480.0020270.0020431.6231750.0413640.0021300.0020510.0021510.0022421.3659350.7549360.0152530.0143080.0142950.0143020.0020420.0020281.2387140.8614670.0154250.0020420.0020322.0074210.0045610.0142940.0020440.0142880.0143001.7171812.5584060.0573480.0278140.0144240.0266871.2321280.0282430.0145280.9081531.2401850.0283120.0266370.0513650.0146580.2185200.0393432.4634480.0296301.2391702.4764482.4966671.5673861.6277782.1637760.0046690.0020420.0020270.0020380.0021220.0775440.0021210.6793800.0381920.0026880.0020290.0378390.0391250.6901130.0028671.0558852.5201351.7000760.0040960.0021070.0020340.0145200.0146540.0149810.0020490.0020310.0351460.0179310.0020560.0020380.5840053.7172800.0556700.8948700.0522740.0513340.0747890.1091230.0515600.0391620.0517210.8187801.4516530.8793711.3546300.0035930.0268590.0267060.0266660.0143911.6852740.8913300.0476390.0021020.0021390.7025830.7007510.0030460.0020630.0032271.1130341.7494380.5567840.0272040.0143380.0143170.0143370.0266100.2543360.4989310.2111320.0145220.0388180.0143260.0143040.0143010.0388340.0143270.7228010.0150920.0265620.0020610.0020380.8587480.0029730.0143910.0020460.0020260.0020320.0020290.0020290.0020370.8599391.7158760.0285030.0020690.0143210.8600210.8610971.2279350.0161310.8559231.7101650.0288750.8521561.7032761.2278700.0160590.1299510.6167010.0281100.0146790.0147660.0020480.0149570.0275201.2112300.0415890.0147390.0273490.0147590.0020540.0273550.0761740.0021240.9248202.0487912.7958350.8170802.0373200.0041850.0020610.0022640.0020290.0020281.2340130.0033480.0148090.8320920.0028920.0020280.4761703.2151273.5806900.0620801.2504430.0174440.0152840.0165870.0441330.0437300.0399490.0020810.0020710.0020780.0148050.0020501.2270211.2279720.0040471.2261872.4443751.2244240.0044550.8286400.0028691.2298271.2282783.0447541.6809172.0868582.4429730.0045281.2270301.2278621.2287922.4500311.2288471.2037000.8612840.8586672.4524220.0745820.0021150.0810300.0407520.0021070.0021400.0021780.5227540.0026670.0507980.0517270.0392610.0020700.0146200.0772610.0483170.0819500.0021420.6322130.1789750.0022321.3815101.3997300.6591541.3091470.0400840.0388630.0265900.0388573.0567970.0172520.0144930.0020540.0020450.0020520.0147061.5393440.5906140.0516410.0511010.0604010.0614140.0373170.0375130.0020750.0383830.0372330.0020630.0020331.6826681.6764360.8634990.0156590.8459161.1099770.9613231.7285521.7370810.0634660.0266360.0143310.0266090.0266071.6458420.1068321.3393660.1499760.0392141.7285380.0036470.8718920.8596220.0028290.8568860.0152470.0020481.7160302.6806980.0169640.0145460.0020840.0020671.1485280.8506080.0154220.0020610.8491870.0152812.0128251.7003640.0035850.0020290.0020280.0020300.0142920.0020410.0020250.0020280.0020270.0020260.0020291.2257630.0031350.0020260.0020270.0020270.0020500.0020270.0020340.0020270.0020250.0142870.0161510.1428520.0021690.0145320.0147170.0150770.0275680.0272040.0020770.0146340.0020610.0268940.0021660.0271190.0144170.0021141.2270780.0280300.0150790.0268990.0148560.0144391.8172390.0036290.8362750.0027580.0020330.8308412.8185451.1893763.4592233.1708272.1660392.8996541.1779220.0030470.0142981.7276862.8611022.5870240.0538442.1521171.7128082.0295612.7421300.0044140.0382330.0383960.0020680.0321670.0436110.0389290.0020660.0020360.0020330.0020280.0022560.0266230.0511730.0512070.0512280.0266141.2338620.0030800.0143110.0142990.0266260.0158530.0267000.0021620.0267520.0144450.0271090.0268960.0271441.2035490.0030830.0266730.0145180.0020720.0020530.7380610.8297850.0176510.0270640.0020880.0020560.8152210.7757730.7721251.9570860.8749201.5402070.0400680.6459280.0644962.6266071.1887470.0411840.0025230.0020990.0022341.2215530.0383620.0020780.0020690.0020270.0020710.1288870.0021550.0020320.0020260.3608160.0023910.0034030.0026130.0140960.0020570.0020560.0386680.0020700.0075050.3526650.0023460.0378060.2455710.0022432.1061291.6793332.4878800.1590570.0804400.0397910.1863550.1067590.0021230.0371820.0372460.0372390.0020631.1638390.0527172.3246680.0533003.1676970.0293320.0269900.0270700.0146130.0020620.0268480.0270580.0144440.0144710.0271280.0020590.1082910.1100320.0376120.0573000.1505092.6091371.1907380.8982382.0326722.5916821.1876190.8982530.0517610.0388490.0143620.0265540.0020480.0020250.0265410.0510810.0020690.0020260.0020250.0020260.0020440.0020530.0020370.0020380.0020620.0020470.0020730.0020630.8616040.8527980.0284680.0020680.0020470.0020480.0155370.0143240.0020600.0020401.2232250.0029750.0143230.0143110.0020570.0020470.0020360.0143200.0020360.0372430.0020540.0372350.0020660.7025270.0377470.0021191.1017810.0604740.0267880.0020541.2511800.0525950.0426100.0020660.0151860.0020460.0020510.0484260.0020790.0202800.0020610.0020250.0285650.0311030.0020730.0020400.0342320.0413100.0468390.0164110.0168171.6796880.0523020.0511150.0511180.0511251.8476750.0193931.2169050.0476950.0328570.0156050.0214500.0482071.9482431.3148992.5353702.5494752.3192362.4209732.0363930.5383910.0845910.0388560.0388360.0143080.0403860.0546680.0143690.0266430.0020530.0389270.0389590.0143500.0143440.0020390.0389190.0143660.0147411.6869980.8471880.8642080.0026470.0406130.0150940.8376210.8597761.1646072.3202882.5191971.3876282.9078932.0196440.0364610.0261700.0359000.0224790.0365220.0561620.0581310.0294830.0301670.0290300.0300240.0432840.0163040.0300390.0316900.0157350.0020430.0163840.0020510.0486570.0158121.8774410.0037620.8073370.0025930.0026410.0020310.0020390.8389600.0026161.1757280.8291380.0026210.0020350.8232650.0036000.4368290.0023380.7000000.6991160.0025241.1914090.0028670.0020520.0020490.6978910.0025090.6961020.0025530.0020440.0020550.6841010.0025230.0554010.0020660.0020480.0020610.0020490.0020541.6331372.8254780.0188190.0286030.7848020.8262000.0396560.0144190.0144230.0389910.0144330.0269820.0266500.0020531.5838472.5576612.5609482.8652004.3025893.1520050.0041580.9460560.0026700.0020251.6125610.0032211.0644070.0027390.0020270.0020280.0023290.8513910.0026310.0146490.0020520.0020530.0020470.0020550.0020470.0154170.0020500.1795401.2172752.2912780.0035740.0068930.0020580.0020600.0191610.8467080.0026180.0020720.0020492.8035972.7862282.0172301.5245842.0895570.8926633.3695993.5584650.8981511.8555471.8693271.8443580.8087230.0025570.7986290.0622230.0020930.0020420.0020340.0020382.0326060.0033711.9919750.0033411.8424780.0032250.0144360.0020360.0273400.0146530.0020470.0145860.0020360.0020310.0265500.0143040.0142930.0142910.0265540.0020470.0142930.0142920.0142960.0388150.0020540.0265710.0020410.0265420.0177210.0160271.1090021.6438990.8104870.0172690.0178481.1306881.1294281.1439131.9576740.8182190.8082791.6351130.0030641.6076030.0191081.1489021.6261740.0030920.0020440.0020540.0020730.0155180.0021980.0020590.0020590.0020680.0146840.0020750.0020660.0020880.0148450.0020790.0148090.0161140.0020920.0020650.0020730.0020460.0020540.0147300.0020631.2267570.0291270.0020670.0020500.0158170.0020650.0154270.0149750.0020840.0020440.0020730.0020520.0020800.0020710.0020520.0149390.0020660.0020680.0020340.0020490.0020600.0020441.8172740.0523180.0512500.0512821.8581480.0596400.5737670.4941063.7524522.3716371.6328080.7037811.4743640.0622310.7655170.7742082.1044092.0793711.4271410.0645700.0525660.0537320.0525553.1656723.7008320.6732460.7679763.3817880.1612580.1291890.5763300.1929200.2503510.0021800.0266660.0143070.0020360.0020470.0020380.0020340.0266660.0020530.0020410.0265970.0020910.0266340.0020460.0143580.0020371.8225380.0277650.0020461.4254660.7387720.0377400.0724391.4169641.7457200.0382510.0020460.0372290.0143680.0020320.0020260.0020300.0144140.0143900.0143800.0143450.0020350.0020310.0020350.0020320.2528610.0021730.0020300.0020260.0020250.0020240.0020330.0020320.0020260.0020270.0020410.0143740.0266910.0512651.8580280.0523801.3741132.9945492.9311640.0160400.8473550.8709440.8524240.0149000.0020550.8681530.0271930.0023540.8573390.0148661.8409570.8562820.0277441.1884060.0027180.8720330.0270680.0511310.0511390.0511650.0511170.0511400.7005822.4815202.6769531.6791182.4744950.0572670.0283650.7363400.8085590.0732440.9077200.8981172.5889031.1876690.8974130.9334620.0382230.0754420.0656130.0734620.0382380.0377060.0020490.0020410.0510781.8560550.0521371.8549850.0521630.0511400.0511150.0511240.0511390.0510880.8952582.6589120.0529161.7358891.4027210.9221220.0279390.0408370.0020520.0020370.0404740.0020500.0272930.0153970.0413511.8178520.0030430.0278010.0020640.6324210.0023810.5910210.0023591.2267790.0027200.5979330.7038490.0024240.0020350.0020361.4812300.0535321.3298900.0381342.1009140.0754480.0452410.0023780.0128950.0482840.0372300.0020630.0020920.0020310.7642031.7947550.7641920.0024820.0020930.7650040.9982330.8667000.7069800.8307080.0025380.0021120.9422740.0483060.0020500.0373040.0020460.0020321.7737401.8523060.1011310.0388812.0322061.5275751.7434062.4284310.0524311.7424191.7315931.7005330.0280810.8339830.0276301.1635450.8654720.8661220.8546470.8719640.8694230.5180910.0176511.5859640.0242810.8241551.6834610.0733320.1440780.1437960.7668801.1484680.0026480.0020390.0020390.0020421.1454500.0026450.0020350.0020430.0020340.0152431.1480571.6708420.0029350.0020400.0020580.0020483.9313832.0445670.7281780.0024140.0020660.0385120.0020451.8215680.0029970.0163040.0020440.0205390.0173241.1813650.0026650.0160460.0152590.0020460.0020380.0063330.0020440.0020300.0165460.0020511.2205150.0630601.1739853.7734973.2527652.0567660.7232950.6360711.3047180.6821770.1430680.5734260.0023320.0020670.0020370.0143010.0020461.2224110.0149430.0143070.0020332.5920130.0033620.6449261.4994730.0027970.0020350.0020290.7248361.9010790.7803700.0024280.0020330.7442570.0024120.7796050.2853960.0021720.0020650.0020370.0020360.0020370.0020310.0020240.4061040.6953390.0024010.0020470.0020540.0439091.2399511.1023580.8288950.0049140.0120330.0545990.5193780.7006930.6058950.0023400.0020370.4310490.0022410.9358500.8994640.0380790.2571710.0389650.0511310.9793420.3743510.0022181.1444300.0026090.6875170.0023770.0142810.0020420.0020240.0388020.0143030.0142920.0388270.0020480.0143810.0142950.0265510.0145730.0388300.0143080.0020350.0265410.0320120.0180490.0169210.0020500.0020750.0020370.0020571.8214552.0523940.0030570.0020390.0272530.0020730.0020380.0272390.0279090.0020660.0020440.0020500.0020400.0241620.0245470.0020810.0020360.0022310.6928840.0023690.0020290.0723800.0020600.0020370.0020720.0020560.0026160.0020790.0020970.0375590.1513790.0021190.0424000.0020550.0021860.0020510.0372960.0020820.0020290.0020850.8265620.0024680.8525800.8418720.0024650.0020560.0020770.0160490.8275270.8081040.0024360.0146150.8382200.0024652.3103333.7213202.3239143.1685814.5161300.0288840.0450770.0309710.0590120.0295770.0302150.0451890.0020570.0303400.0451320.0463560.1152620.6770420.2989210.1018730.0908520.5622260.0392630.0520850.0512170.0512630.0436130.0568871.7993960.0318710.0020500.0147511.9245170.7476640.0032970.0373330.6532180.0393970.0021990.5241980.1091670.0024521.7629030.5041910.0025300.0021340.0510550.0510810.0510780.0510991.7899130.0519510.0515120.0512170.0511670.0558500.0564270.0143550.0143320.0020440.0020370.0143170.0020360.0143131.2515930.0026111.0068942.0996140.0030152.3765700.0487292.3331013.3634010.8823231.1717060.0025801.0988410.0025411.1602771.0998391.2079040.1417640.0432460.0020551.4025050.6507730.0023580.0020300.0020330.2446750.0022360.0020570.0020310.0020390.0020480.0290180.8080270.8916150.8580210.8871510.9397190.0392730.0265760.0143030.0265701.2387290.0148630.0142940.0265780.0388380.0143180.0142960.0020400.0142920.0143130.0020460.0388430.0265780.0143290.0020430.0265460.0265980.0020400.0265610.0265790.0143100.0143140.0020450.0142930.0143020.1114250.1599400.0662620.0602170.0596190.1594540.1653410.0613790.0020530.2291471.2506010.0516280.0388160.0510680.0266160.0388150.0143030.0265520.0388040.0265630.0510590.0389090.0510650.0266391.2622150.0516220.0388232.0272960.9091000.1514561.8904640.0178100.0153070.0311350.0436600.0020710.0162350.0020310.4111890.6918850.9591312.3222880.3168000.8463390.7382030.0025350.1208530.1757180.5123152.0940510.0275191.7052540.0397840.8831674.3023700.8834451.9802910.8586592.5441250.8562461.7222410.0275920.8823080.8846870.0394481.7259791.7380740.8668360.4910382.0523110.0044680.7700130.3396361.8279911.4856000.8676530.3648471.1313490.0028680.2304010.5449440.5543870.5461480.0027540.4865720.2002940.4882110.0026590.5523810.5428561.7374780.0381181.2863921.6677290.5766290.0040990.6164592.0775090.0061380.4071471.1066950.1036892.0092271.3639330.5509260.5445340.5526890.5458451.5079781.6450241.3736531.9603410.5942971.0879060.0032291.1787930.5882910.5407030.6861741.4612571.5475300.5879350.0692111.8676881.7453940.0836170.0024751.1531930.0033900.0066590.5451460.9619112.2598270.0042090.5405190.7637560.7081650.9548121.3479160.5705741.9692031.7264510.0035240.7643890.0070310.4867560.2906550.1327951.7200771.5516450.1382600.5454951.4582160.1219300.0024880.3980650.5230882.1886621.0001561.6150220.5917800.8147340.0034992.0061040.0036132.2313550.3350790.7026180.7267350.3530371.6236620.0779261.2262120.1923960.6922280.6628880.0067912.3342610.0033230.6801771.0433480.3636820.6607490.5003360.0027371.0436892.2250600.0032290.0022060.4884341.0763960.1861791.7612901.0250860.5496430.5389381.0334110.4901302.2975691.9110910.0242111.4645582.9191900.7851261.5349492.0653010.5375900.5962650.5408730.6412681.6465121.6314841.7263851.7914760.0031771.7788760.0148650.0037793.3815880.0049160.0044881.4082900.4910561.5519740.0400120.0053170.0159420.0028881.7790800.6918660.0050740.4875200.0023852.2244330.0149411.8872770.5447761.0735090.4989810.0029961.2979901.4397590.9070340.0209801.3050882.2641280.0039341.2134150.6340571.0012850.1444381.1370810.0107640.4896622.6301330.0035021.2007461.7736572.1140910.0039360.5421960.0379450.1433510.6147651.3080660.7398890.0029261.2613613.1758430.0036160.0023940.1741620.1759220.6931050.0025501.2630142.3175620.1079710.3584970.0023871.1881380.4884210.0058471.6245120.9993150.0703600.0376821.8736051.4108201.0982280.3710791.0879331.8862171.7593700.0030911.8297730.2568492.3401091.7192500.0029451.7423651.9012190.4895690.4958310.4885490.0037991.1164050.4882760.0032910.5866840.9491202.4754993.3735100.0070083.5732271.8041500.6837350.9752800.5398291.5726752.2986932.3425050.0360340.7139122.2587410.4934090.5367580.0438780.0023160.1076070.6042521.5575031.8081343.6090161.8532050.8338712.8381770.0483260.5286030.0679640.5543040.4892731.7092832.2789941.1013160.7047052.2433010.0063951.6814860.2026870.0024270.5447362.0341230.0030040.0065531.7469030.2711890.6510200.6979972.4401561.6712150.0033620.5985130.0025211.6716880.0028620.4915760.0028741.4492290.7893870.0026530.3672660.5440930.9718790.1143260.5067510.0371580.4877260.0029101.1354371.4371000.6135700.5857042.1056030.0034190.3158650.0086931.8395231.6762521.8135870.5591360.6928440.0024730.0377181.5801760.0047990.3196281.7938482.7352910.8020501.3256251.1690730.0276081.1371680.0802070.0025841.8195790.0046950.5680200.0031180.0377330.0934390.0026071.7287240.3569701.7842520.0033370.6981460.0024890.6483470.2219280.0807290.0113500.7000270.0026721.8025311.9473181.5387000.0385320.0027781.9700830.0061511.1328730.7512600.1954800.9624600.0028820.0024310.5889220.7941921.5970271.2578001.4401101.7323601.7551640.0049351.0131640.5625491.1909113.0276160.0036760.7350030.5436190.5450151.0852300.0026921.1340110.0052701.8174040.1945482.0866390.4908120.5455261.5825820.0114421.6627761.1699491.5722190.5507440.5422281.0306020.5117181.0824260.5442990.0054350.5455670.2150401.0888960.5457820.3496350.6800650.5785531.5514080.0046480.0389190.0420420.0027500.6110000.4209651.5256770.6110020.0026311.0127210.0680900.5839831.2480610.0028922.2708580.5340370.8221250.1234560.6842934.5352281.8895480.0040613.0262001.4435191.3680431.4595621.2792360.0029991.3341821.5688550.1908980.5564990.7220960.0443830.0448860.8339030.5205940.0063600.8140211.1651200.9521511.7363800.0460051.2699942.2687111.2665230.0034721.1219470.0063900.2251541.1340690.1902470.3069740.3837311.3537960.5546020.7022410.5969350.0024020.0022990.7579530.8106091.3660820.8469431.7655330.0046270.0028380.6856151.8086121.4886451.7121992.9050100.9440302.0995381.7215651.1278010.0036170.0030650.0374650.6906430.5438871.7945041.3785731.4307451.8186281.9907780.6959192.2231350.0286341.0290280.4969070.0025292.2498660.5909890.0027691.1538950.4876220.3579910.4875650.4908470.0033010.1745460.3503990.0752790.0022740.8910080.0564081.6151061.3272941.6895161.7052750.5431450.7234560.0095891.6037423.1272081.1142760.5458490.5443170.1735091.7420220.6228762.1264411.0813440.5429970.7381900.5538953.3220920.6182480.5538330.1885280.7045890.7052690.0045140.0124360.6507030.5387620.4911881.4729090.6368060.0377550.0042830.5862600.6352570.6933250.5613470.0186652.7385940.4004100.9163890.7007600.6370610.5244490.4482891.7071010.5583920.0031200.9635830.4878130.1727770.5029330.6934241.8099351.2061971.5777470.2128050.5737551.5685611.1554151.5690240.6745911.6197371.7633960.4923840.5282120.0028881.5520751.4234042.2383470.0036990.1915531.8705140.7026320.7233730.0027701.0235280.4900660.8411191.0602920.5207701.2444320.6146200.5445690.3556270.4922621.5539210.8109380.7990981.4786080.4886930.5512311.7648030.7192240.7028301.0908010.0377460.7026500.4892900.8795560.0025240.6668980.9238550.5501130.6866741.4094930.4880100.5819900.0634051.1842301.4138800.6358560.9537391.6825210.9594490.6908480.1728920.5424641.5366890.0037150.2719140.0023351.1543971.5030580.5545761.9879610.8832651.0338680.7379150.7214790.5295491.6376430.0039570.0418101.3127930.5433520.6165431.1203371.6168190.8237270.6384970.7229450.4599581.0243930.6484762.5246921.8277553.4165101.7924151.8047753.5345811.8007180.0034651.5586010.2011471.1119400.0025380.5626850.1037121.6087310.6735501.7876890.0071600.0023061.6411290.3098301.7232910.9398200.9847570.5709560.0033651.1048271.9563400.9681731.7730650.3419720.5481790.9805980.5784930.0025770.0070510.0745680.0463231.9175280.2747301.551981

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/
With following parameter combination: {'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_fd1a93fa_2020-02-20_14-40-49twnpajcz/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    358    |    124    |    102    |    26     |
-------------------------------------------------------------
| disagree  |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
|  discuss  |    523    |    181    |   1536    |    157    |
-------------------------------------------------------------
| unrelated |    156    |    163    |    141    |   8431    |
-------------------------------------------------------------
Score: 8270.75 out of 9226.0	(89.64610882289183%)
Accuracy: 0.8677929063708186
F1 overall: 0.5334074083762012
F1 per class: [0.4347298117789921, 0.0, 0.735632183908046, 0.9632676378177664]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/11898 [00:00<?, ?it/s]  0%|                                       | 1/11898 [00:01<4:13:01,  1.28s/it]  4%|â–ˆâ–Œ                                   | 501/11898 [00:01<2:49:43,  1.12it/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 4501/11898 [00:01<1:17:06,  1.60it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 5712/11898 [00:02<45:08,  2.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 6843/11898 [00:02<25:49,  3.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11898/11898 [00:02<00:00, 5334.37it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 11898
  Length dataloader evaluation: 1488
  Number of epochs: 3
  Batch size: 8
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=1488.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.8237340.8241280.4129160.1377300.0344680.0069870.8404590.1200970.0150430.6766600.0676980.0061850.0006040.5689930.0406740.0029160.0002180.0000470.0000370.0000430.0000950.0000950.0000380.0000380.0000940.0000390.0000370.0000351.3973691.4976670.0500191.0749090.0336290.0653971.0732870.0307040.0008910.0000631.0862260.0292261.0797461.1022402.1485381.0841880.0247371.0777250.0241880.0005520.0007610.0000510.0000350.0000370.0001500.0000950.0000380.6538541.4449791.4888320.0285720.0005200.0000440.0000360.0000360.0001140.0000370.0000370.0000360.0000362.2792472.1375560.9767850.0248400.4326000.0059600.0001170.0000350.0000360.0001490.0026010.0000900.0000340.0000340.0000340.0000640.0000340.0000340.0000380.0000370.0000560.0000340.0000340.0000330.0000340.0000350.0000340.0005870.0000420.0000390.0000380.0001530.0001530.0001070.0001240.0004630.0005000.0000990.0000330.0000400.0004100.0108791.9882310.0189470.0002020.0000340.0000330.8940010.0077410.0001000.0000350.0000340.8471330.0070370.0000970.0000390.0000380.0000390.0000400.0000400.0000390.0007890.0004970.0005570.0001551.0479910.0079190.0000990.0000381.8779430.0136400.0001310.0011430.0142180.0143620.0001320.5898050.6141780.0042980.0001230.0000930.0000940.0001500.0000360.0000350.0002660.0001510.0001510.0000920.0001480.0000350.0003330.0006360.0018800.0010840.0001540.0002670.1479200.0009250.0000400.0000340.0000340.0000340.0000340.0000340.0000340.0000340.0000340.0000350.0000340.0000910.0000340.0000340.0000340.0000340.0000350.0000340.0000910.0000350.0000350.0013350.0000410.0000350.0000350.0000340.0000340.0000340.0000340.0002080.0003420.0008780.0011210.0000390.0000330.0003250.0002102.0812510.8799290.0043110.0001690.0001840.0001500.0001500.0000350.0000960.0000390.0000340.0000340.0001060.0000940.0000340.0000360.0000340.0000350.0049620.0028080.0095390.0001380.0007840.0010950.0079450.0001271.1067890.0049400.0000580.0010350.0000990.0001010.0000340.0002240.0002090.0000920.0000340.0001640.0002360.0007390.0000460.0000420.0000450.1229960.0006680.0002170.0001540.0000350.0008850.0000440.0000360.0004770.0000370.0000380.0000350.0007680.0000380.0183670.0001210.0009670.0000590.0000520.0004410.0000360.0000920.0000340.0000910.0001490.0000340.0000340.0001660.0000340.0000910.0000920.6549330.0023881.5309620.0128220.0000820.0000931.2990020.0046530.0001090.0039260.0015050.0013920.0000400.0000900.0000940.0000340.0001470.0000900.0008890.0014170.0000950.0005760.0000360.0003590.0003670.0000350.0001500.0000920.9376660.8944350.0030520.0000430.9768200.0032320.0001010.0000330.0001020.0000921.0024430.0031970.9416330.9053040.0029251.0208200.0032040.0000450.0000340.0000910.0050850.0022750.0007380.0115640.0094250.7685461.2704500.6869081.1223803.4577490.0104390.1746020.9375050.2389460.3479463.8678731.7354100.0054820.0008960.0001580.0002190.0001540.0003180.0004960.6458180.0019560.0000390.0003560.0000350.0003540.0000340.0003580.0000340.0000350.0000910.0001620.0000350.0002060.0000930.0000910.6583890.7516300.0020780.0141830.0896290.0013150.0009590.1347980.0003940.0003870.0000350.9254150.0027830.0012430.0032020.0000400.0007560.0000330.0000320.6051080.8665130.0022740.5293801.4631700.0039020.0001040.0001230.0004580.0000370.0018320.0000990.0001495.2499100.0135290.0002460.0002100.0003200.0002460.0003900.0002840.0003830.0002771.4989240.0131340.0026820.0000390.0000370.0141610.1696890.0004430.0553330.0056071.6003601.6341040.0043150.0001970.0000960.0001510.0000930.9466190.0125980.1643130.0013590.0019030.0006610.1762620.5386450.0013930.0002850.0002580.0009600.0000411.5012650.0034580.0007920.0016350.0002120.0045660.0061650.0058170.0006050.0001500.0002640.0012290.0006370.0000360.8941480.0020670.0000390.0000350.8387051.8273070.0040811.9984600.0044370.4921750.0067190.0285530.0012970.0013460.0001520.0000920.0001500.0002660.0002060.0001510.0006360.0042590.0050620.8448140.0018120.0000410.0001520.7894260.0016920.0048500.0031380.0002120.0001490.0003900.0002460.0000340.5744320.0012070.0000350.0000330.0005510.0000920.0000890.0023090.0002670.0000340.0000330.0000330.0014370.0009390.0009340.0020130.0000450.0012590.0022690.0002230.0038150.0073350.0008400.0004600.0031590.0025010.0003870.0003290.0081360.0001090.0000370.0035300.0009060.7257821.2648160.4813700.0483270.0001263.0920820.6907000.6852390.0071740.0017460.0002100.0002640.0024230.0033930.0020030.6919760.0020240.0006640.0014030.7255880.0014310.7677370.7704300.7250250.9918041.5904670.0029430.0000980.0000350.0000350.0000351.0778990.0019840.0000380.0000350.0000350.0058700.0004830.0001750.0010720.0001540.0000930.0001530.0002170.0000370.0002090.0002260.1009520.0005780.0000360.0020960.5429930.0059180.5924230.0011370.6065530.0043080.0044594.8013740.0088110.0006681.3890500.0026190.0000450.0001232.1769060.7271390.2703330.0005510.0002060.0002060.0001480.0002060.0002620.0000920.0002050.0000341.9149650.0033460.0000391.8351012.5242051.8456700.0034373.0069871.0666440.0017900.0000360.0008870.0000340.0000360.0000340.0000330.0000330.0000330.0000330.0008040.0000340.0009400.0010020.0031230.0021590.0017350.0010070.0004510.0007850.0015560.0031230.0038060.0002650.0002030.0001500.0002040.0002030.0065500.0076420.4659230.3613290.0036710.0037320.0009150.0002710.0001500.0001500.0002650.0000350.0000340.0000360.0000350.0000340.9340580.9864560.0015440.0000930.0000910.0000340.0001060.0001370.0000340.0000930.0004070.0004820.0013000.0022780.0009151.0836630.0020640.0000950.0000350.0002060.0000920.0001610.0001490.0001490.0003780.0001560.0004450.0004940.0004340.0000950.0003350.0001510.0022442.8621374.0958675.2596960.0091160.0005080.0045150.0024590.0001520.0003740.0001480.0002050.0001511.0752180.0746750.0003690.4999900.5545260.7283142.6437000.0041370.0003040.0005470.0005330.0002630.0003220.0002050.0002040.0000930.0002060.6459870.0016550.0000360.0000340.0005730.0005910.0000380.0006290.6030931.2571760.0017730.0000370.7474410.7902650.0011270.8391770.7329420.0010400.0000370.0016670.0035310.0014070.0010990.0001510.0002610.0002620.0187660.0027920.0056050.0041950.0020300.5463350.0007700.0000370.0000370.6163010.0009130.0000350.0000330.0001080.0007320.3527800.1114760.0002620.6535550.0008950.0063750.0035660.0025640.0032110.0008852.1200501.0633712.1215220.0028010.0000391.0606981.0596891.0617700.0016170.0016250.0000950.0000370.0002080.0001490.0001510.0001550.0002670.0001490.0008360.0001503.2428561.0636631.1212282.2421933.2415373.1828392.1241831.1060012.1196430.0027050.0002020.0000330.0000320.0000900.0000330.0000900.0002090.0000330.0000330.0001390.0000330.0001590.0001770.0000910.0001320.0000330.0000320.0000330.0001330.0000330.0000330.0001460.0004950.0004990.4824211.7435050.6601410.0066352.2163130.6098130.0530450.6226180.0012573.4505690.0054320.0009660.0009740.0009480.0001510.0002040.0000330.0001620.0000350.0005570.0001640.0000900.0006390.0017950.0020690.0031820.0031780.0006540.0000940.0000370.0001520.0001500.0000360.0000360.0000940.0000360.0000360.0000360.0000360.0000943.2243502.7110644.5075930.0071540.3655330.7476350.0014650.0001520.0009350.5578380.0007920.6950670.0012570.0005610.0005252.5745722.3167840.5482160.0037960.7054651.1198640.9759612.3567982.5809522.6730001.6911250.0019480.0004980.0005120.0004900.0004940.0004962.0375502.0358942.1992780.0027830.0000370.0002070.0002050.0002620.0001480.0004280.0004400.0008460.0004930.0004100.0000330.7024960.0015690.0011260.0003940.0000360.2558820.0003151.4451010.8151261.1174201.8808262.2001450.0024341.1133910.0017370.0000380.0028440.0010870.0069290.9594110.0024910.0020180.0048360.7635470.4040081.1481470.0019080.5174520.0032250.0055634.8854333.4964280.0037680.0000400.0142860.0000530.0000951.2905720.0014060.0000392.0869930.0041200.0004690.0001150.0000960.0000980.0001130.0000920.0000940.0000360.0000370.0000421.0506480.7860212.3299250.6625471.1824060.0048210.0000400.0001160.0001000.0001490.1016280.0005910.0009420.0000350.9180970.0013860.0005290.0004420.0000360.0000350.0000340.0000342.0052300.0020800.9482370.0010570.0000350.0000931.0057581.0182390.8909291.9814221.0173711.0147740.5425590.0009530.0007930.0004120.0000920.0002080.0001490.0002080.0001750.0002150.0002680.0001510.9004470.2636020.0002980.6980962.3137720.6691310.0007031.3733000.0013940.0000381.2218800.0012400.0000930.0000340.0012940.0000360.0000350.0000360.0006140.0011840.0000360.0041820.0000390.0004990.0005390.0005440.0000340.0005330.0005471.0635160.0036680.0058670.7575830.0010870.0004240.0002850.0002190.0003500.0014160.0015630.0014960.0008430.0007420.9830340.0019920.0007710.0008100.4350640.2437960.0016620.0005980.0014150.0006890.0002770.0005050.0005850.0005080.3705550.9024200.0010110.0000360.0000940.7561500.0015360.0015520.5328151.1923950.5702880.0005650.0000340.0005130.6524540.0066340.0056040.0000380.0004290.0000330.0000332.1238070.7286760.0025511.5933840.0016720.6601470.0008211.1401830.0011360.3965681.1885871.1169130.6314990.5684920.6444110.0007360.0000910.0011550.0017750.0006022.5111961.4065082.3936520.0026130.0004640.0002690.0003280.0003870.0004550.0003580.0004450.0021340.6083170.0007030.0079580.0000980.0002110.8591771.3428310.0034150.0004450.0008340.0005780.0009070.7564890.0015720.0014340.0012440.0006150.0007350.0016230.0272160.2872520.1918260.3308981.6711490.5784231.1905780.0276920.3619790.2814480.0112130.7747520.8497000.4915490.6480390.9949730.6540390.6613600.6575600.0318380.3504940.1174480.0800760.4727361.1962740.6465110.8417750.1965842.0085010.0070690.4435910.6525861.1246701.0494410.3715660.3752821.0734530.7312980.0658400.4003981.8236060.2485340.2986220.2029770.4695751.2692150.9831650.3782190.7477760.0657280.9321740.1772480.1526620.2639690.1982870.3070880.0670691.6206400.0390950.2068260.0002340.0112920.6593890.4212330.4078111.5774770.3567950.7657790.3782020.4693990.0177030.0328070.0198160.4353040.4432861.1920120.6762760.1756571.1463560.0240140.0487970.2706890.0024730.6763171.0069580.1061300.6820690.9827790.7506540.6507780.9477130.8867900.4962610.9760600.0325341.1147930.0117210.5139770.0545930.3162250.2615500.0013370.6626900.0342600.7330530.0112840.1181620.1724120.0253780.6694850.0280800.1757230.5912700.0045960.0037631.6175010.5585990.4566290.0027141.2421330.0024390.9488061.6335431.4974231.0991370.5889600.4348740.3168550.3057230.2919810.1162520.7228700.5892861.2895611.3908980.1623710.6857331.6609950.8988910.9823450.3443580.1314380.6860190.0614740.6125980.2574931.6271510.0099960.2334380.4056360.1187480.4828880.5082980.0304060.3299860.0363572.4209200.4011580.7328980.0470811.2513590.2524520.1398550.5258170.3569002.9798711.2793000.4547380.3469940.5079210.3523380.1382360.0037160.4335710.0066420.7964771.0283080.6300990.0122700.3432450.5072940.6147470.0101540.5716560.1674920.0236360.2564870.4082481.1788880.9396010.5341690.1329670.2242830.0392910.6483150.8697621.4366470.4166000.3394140.8227970.8609140.4290100.4189780.0088220.9108410.0274180.4078250.4890950.1003590.0409410.3985061.0698620.3864810.7210800.5986911.4138040.3473070.0472460.8408290.3830680.0247630.0116960.7468460.4261050.7581050.7562480.3000340.7228631.0081240.0649300.4557590.0189401.2943520.9445100.3875930.5650050.7912070.0006510.4882600.4248360.0135570.0089461.0283370.5238470.2946000.2898500.0003750.2407161.2010621.8447961.6538970.3931190.6390190.0017450.0080260.5579340.2578070.6651700.4291140.0259730.0008470.8440410.0190930.9176680.0070390.3711330.1509830.4827171.0767571.4897250.0175650.9986360.1567510.0462160.0237620.3892640.4503921.5557970.0023490.1173210.0183140.8331150.9088530.0488620.8098890.2533770.5363640.0440070.2572101.1030490.9212140.3242861.8930831.5407550.1795870.5634820.8762910.5653020.8802830.0224140.1188330.2890101.2226410.9013470.2297480.5436810.4453250.8435160.2858640.0788930.7596720.1859180.6939870.2645450.5337970.6218070.8206030.2661530.0195440.8721450.6503121.5805030.3675930.0775280.2100540.8400220.5194110.6138801.2452450.3937450.0401390.8182070.5942900.4911231.1631401.1759511.3093870.6635050.2544810.4639570.0122070.0111550.0313050.4868600.3685530.2057240.9121930.0064881.9295600.3125670.4440700.0293880.0006520.0208851.2707911.112519

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/
With following parameter combination: {'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/roberta/grid_search/results/pipeline_f476b17a_2020-02-20_12-00-00491p74b_/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    689    |    59     |    119    |    11     |
-------------------------------------------------------------
| disagree  |    103    |    312    |    73     |    13     |
-------------------------------------------------------------
|  discuss  |    228    |    65     |   1540    |    65     |
-------------------------------------------------------------
| unrelated |    17     |    32     |    47     |   8525    |
-------------------------------------------------------------
Score: 8752.0 out of 9226.0	(94.86234554519835%)
Accuracy: 0.9300722810556395
F1 overall: 0.7976126255219581
F1 per class: [0.7195822454308094, 0.6439628482972136, 0.8376393799292902, 0.9892660284305193]
*******************************************


real	23m12.440s
user	21m14.237s
sys	8m4.193s
ubuntu@run-gpu-mg:~/fnd_implementation$ 
ubuntu@run-gpu-mg:~/fnd_implementation$ exit
exit

Script done on 2020-02-20 21:08:09+0000

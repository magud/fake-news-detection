Script started on 2020-02-05 18:18:35+0000
ubuntu@run-gpu-mg:~/fnd_implementation$ time python3 model_grid_search.py --model=bert --model_type=bert-base-cased[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1@e[1@v[1@a[1@l[1@_[1@s[1@e[1@p[1@a[1@r[1@a[1@t[1@e
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
There are 2 GPU(s) available.
The following GPU is used:  Tesla V100-PCIE-16GB
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/ubuntu/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "multi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/model_pretrained/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/model_pretrained/pytorch_model.bin
INFO:__main__:Loading initialized pretrained model from /home/ubuntu/fnd_implementation/bert/model_pretrained
  ******************************************* 
           Freezing Embedding Layers          
  *******************************************
 
name:  bert.embeddings.word_embeddings.weight
param.requires_grad:  False
=====
name:  bert.embeddings.position_embeddings.weight
param.requires_grad:  False
=====
name:  bert.embeddings.token_type_embeddings.weight
param.requires_grad:  False
=====
name:  bert.embeddings.LayerNorm.weight
param.requires_grad:  False
=====
name:  bert.embeddings.LayerNorm.bias
param.requires_grad:  False
=====
name:  bert.encoder.layer.0.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.0.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.1.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.2.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.3.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.4.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.5.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.6.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.7.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.8.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.9.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.10.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.self.query.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.self.query.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.self.key.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.self.key.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.self.value.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.self.value.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.attention.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.intermediate.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.intermediate.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.output.dense.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.output.dense.bias
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.output.LayerNorm.weight
param.requires_grad:  True
=====
name:  bert.encoder.layer.11.output.LayerNorm.bias
param.requires_grad:  True
=====
name:  bert.pooler.dense.weight
param.requires_grad:  True
=====
name:  bert.pooler.dense.bias
param.requires_grad:  True
=====
name:  classifier.weight
param.requires_grad:  True
=====
name:  classifier.bias
param.requires_grad:  True
=====

*******************************************
Evaluating the following pipelines 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/', '/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/']

*******************************************

*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:00:57,  2.63s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:04<3:53:26,  1.84s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:04<00:00, 2241.85it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 2406
  Number of epochs: 3
  Batch size: 4
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/outputs/checkpoint-3/pytorch_model.bin
eval_separate.py:214: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch in tqdm_notebook(eval_dataloader, desc="Evaluating"):
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2406.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
1.7242522.3478111.4560830.7675390.4740630.3769910.3450100.3314650.3236110.3181350.3139920.3107230.3080720.3058760.3040260.3024460.3010810.2998890.2988390.2979060.6384550.3125810.6377680.3099070.6364720.6490180.3071400.2935541.0136990.6585140.3041290.2919890.2913030.2910050.2907370.2904850.9730100.3084760.2902960.2896221.0104551.0278600.3066510.2893090.6301350.2961811.0096540.3036601.3509220.3097483.1240000.3434330.2887830.2876270.2875040.2874050.2873100.2872190.6285110.2928311.0080950.2987040.2869960.2867340.2866580.2865880.2865200.2864550.2863910.2863290.2862690.2862100.2861530.2860980.2860440.2859920.2859410.2858920.2858430.2857960.2857510.2857060.2856620.2856200.2855780.2855380.2854980.6268410.2893010.2854280.2853490.2853140.2852790.2852451.3476310.2963642.4483761.7112180.2996390.2852050.2850300.2850000.2849720.6263260.2882000.2849230.2848660.2848400.6261970.6293040.2878990.2847720.2847210.2846980.2846750.2846530.2846320.6259920.2874830.2845940.2845500.6259110.2873080.2845140.2844730.6258350.2871450.2844390.2844000.2843830.2843660.2843491.3084762.0754711.0187030.2897240.2843080.2842530.2842380.2842230.2842080.6255750.2865840.6255641.0075590.2891270.2841580.2841110.2840980.2840850.2840720.2840591.0050841.3511650.6323330.2862580.2840130.2839870.2839750.2839640.2839530.6253230.6274190.2860270.2839220.6252800.2859450.2838900.2838680.2838580.6252291.7279081.0132611.0090720.2879771.0048600.6292690.2857331.0048200.2877920.2837770.2837460.2837370.2837290.9664830.2874021.7257971.0124440.2875630.2837002.7881631.0178131.0085162.0606131.6965990.2908790.2836620.2836180.2836100.2836030.2835960.6249700.2852720.2835830.2835680.2835610.2835550.2835480.2835410.2835350.6249100.2851400.2835230.2835090.2835030.2834970.2834910.2834840.2834780.2834720.2834670.2834610.6248360.2849800.9662130.2864720.2834460.6248080.2849180.2834220.6247920.6262640.6262590.2848660.6247770.2848370.2833850.2833740.2833690.6247450.2847810.2833600.2833490.6247250.2847380.2833400.2833300.2833250.2833200.2833160.2833110.2833070.6246840.6260280.2846430.2832940.2832850.2832800.2832760.6246530.6259620.2845760.2832640.2832551.0042881.0070050.6273450.2845280.2832400.2832310.2832270.2832230.2832191.0042520.6272251.6882581.0093320.2858220.2832060.2831930.2831890.2831860.2831820.2831790.2831750.2831720.2831680.2831650.2831611.3455760.2868181.0042000.2856170.2831530.2831410.6245190.6256690.2842850.2831320.2831250.2831220.6245000.6256270.2842430.2831131.0041433.1210870.2923440.2831270.6244760.2841920.2830920.2830850.2830820.2830800.2830770.2830740.2830711.6868680.2874660.2830761.7251330.2875361.0041050.2852770.6244370.2840930.2830470.6244222.4471862.0730490.6298220.2840751.0040680.6265660.9668110.6264370.6254180.2840280.9657790.6264001.0050520.2851170.2830090.2830010.2829980.2829960.2829940.2829910.2829890.2829870.2829840.2829820.2829800.2829770.2829750.2829730.2829710.2829680.2829660.2829640.2829620.2829600.2829580.2829550.2829530.2829510.6243300.2838750.2829470.2829430.2829410.2829390.2829370.2829350.2829330.2829310.2829290.2829270.2829250.2829230.2829210.6243000.2838080.2829170.2829130.2829110.2829090.2829070.2829050.2829041.0039380.2847390.2829030.2828960.2828941.0039290.2847070.2828930.2828870.2828850.2828830.2828820.2828800.2828780.2828770.2828750.2828730.2828710.2828700.2828680.2828660.6242460.2836900.6242450.2836820.2828600.6242380.6250530.2836701.6483791.6516190.6274731.3460801.3477710.2853490.6242290.2836400.2828410.2828370.2828360.2828340.2828330.6242130.2836160.2828300.2828270.2828250.2828240.2828220.2828210.2828190.2828180.2828162.4076512.4124251.6531130.2858760.6241970.2835680.6241900.6249430.6249420.2835580.2828030.6241810.2835470.2827980.2827960.2827940.2827930.2827910.2827900.2827891.0038241.0053730.2843351.7248601.0069000.2843250.2827830.2827780.2827771.0038131.0053320.2842950.6241570.6248680.2834850.2827700.2827670.2827660.6241462.3597871.0080901.0052930.2842470.2827620.2827571.0037931.0052630.2842251.0037920.2842140.2827530.2827490.6241290.2834340.2827470.2827450.2827440.6241240.9661840.2840990.2827420.2827380.2827370.2827360.9654970.2840750.2827350.6241130.2833970.6241120.9661550.2840540.2827290.6241060.6247640.6247630.2833800.6241030.6247550.2833730.2827190.6240980.2833650.6240971.3075040.2846500.9654780.2839960.2827120.6240900.6247281.0043830.2840521.0037441.0050812.0572520.2859880.2827070.2827000.6240800.2833251.0037351.0050530.2840150.2826960.6240740.6246940.2833120.2826910.6240710.2833050.2826890.6240680.6246800.6246790.2832960.2826840.6240630.2832880.6240630.6246660.6246650.6246630.2832800.2826770.2826750.2826740.6240540.6246500.2832680.6240530.2832630.6240510.2832600.6240490.6246370.2832550.6240470.2832500.2826640.6240430.2832453.4979450.6295180.6246300.6246200.6246180.2832350.6240380.2832300.9654170.9665630.2838000.2826530.6240320.6246010.6246000.6245991.6870152.7894672.7912881.7288651.0060681.7259090.6263981.6487320.6262622.4463140.6275570.2832023.4391582.7820160.6280760.6245770.2831890.6240171.0042210.2837950.2826341.0036690.6251680.2831780.6240120.2831731.3450470.2843160.6240110.2831670.2826260.6240060.2831620.2826240.2826220.6240030.6245370.2831551.7246943.1690162.4119512.0311101.6508571.6502631.3471510.2842602.0660710.2853620.9653800.9664240.2836601.3067560.6255570.2831330.6239911.3072721.6496901.6502070.6260600.6245060.6245030.6245010.9658810.2836301.3067480.2841370.6239850.2831110.9653630.6249980.2831080.2825990.2825970.6239781.3072451.6496341.6501360.6259900.2830990.2825940.2825920.2825920.2825910.2825910.2825901.0036260.2836370.2825900.2825880.2825870.2825860.2825860.2825850.2825851.0036210.2836181.0036210.6249950.2830710.6239630.2830670.6239620.6244460.6244450.2830630.6239601.0040960.6249760.6244400.6244380.2830550.6239560.2830520.6239550.2830501.3347450.2840370.2825731.3449891.0050800.2835700.2825700.6239500.2830390.6239490.6244180.9657991.6490281.6499621.6499611.6499570.9671912.3878410.6268080.2830290.6239430.6244041.3454400.6253770.6244031.6868190.9672110.9662411.3356480.2839680.2825580.2825561.7246290.2844780.2825571.6480791.6498922.0193020.2848532.3869010.2853310.2825540.2825503.1079320.2862620.2825540.2825480.2825482.4456580.2853710.2825500.6239270.2829891.6480711.6498411.6498411.6498381.6498351.6498320.6256860.2829830.2825420.2825410.6239220.2829770.2825400.2825390.2825380.9653010.6247870.6243530.9657330.6247830.6243500.6243490.2829660.9652970.9661560.6247750.9657260.6247710.6243422.4077963.1693353.5014182.0214800.6260770.2829571.0035661.0044600.2834230.2825291.0035640.2834170.2825281.0035630.6247940.2829462.0557350.2846971.3347000.6251910.2829410.2825231.3346952.7295292.7899860.2855641.0035610.2833932.4456310.2851323.1666692.4108300.6264600.9656940.9661001.3074801.6492691.3465690.2837871.3449351.0048180.2833741.0035520.2833700.2825140.2825130.2825120.6238930.6242960.2829140.6238930.9656750.2833130.2825110.2825091.3066531.6492320.6254860.2829080.2825080.2825070.2825070.2825060.2825060.2825051.0035420.2833380.2825050.2825040.2825040.2825030.2825030.2825020.2825020.2825020.2825010.2825010.2825010.2825000.2825000.2824990.2824991.0035353.1674631.6512910.6254270.2828852.0276782.4093001.6504171.3081790.2836480.2824960.2824951.3346671.3358440.2836710.2824950.2824931.0035291.0043311.7253671.7261661.3465100.6250510.9656320.6246260.2828681.3066340.6249982.4459762.0683210.2844480.2824900.2824870.6238680.6242410.6242410.6242400.6242390.6242391.3070010.2835970.9652480.9659860.9659860.9659850.9659840.9659830.9659820.9659810.6245980.9656120.2832140.9652441.3073560.6249581.3069900.9663360.6245900.6242250.2828420.2824790.6238590.2828400.2824780.2824770.2824770.6238580.2828360.2824760.2824750.2824750.2824750.2824740.6238550.2828311.0035110.2832270.2824741.3346450.6249500.6242101.3350000.2835640.2824721.0035080.2832171.0035080.2832150.2824700.2824691.0035060.2832100.2824690.6238492.0560272.3988752.0578431.3364552.4466532.3992653.1100062.7898371.7270900.6253150.2828130.2824650.2824640.2824640.2824640.2824630.2824630.2824630.6238440.2828060.2824620.6238430.6241850.2828030.9652240.2831430.2824610.2824600.2824600.2824590.2824590.2824590.2824591.0034951.7252460.2838860.2824591.0034941.7252420.2838790.2824580.2824560.2824560.2824560.2824550.2824550.2824550.6238363.1184162.4100600.9672920.2831210.2824540.2824530.6238341.0038211.0041881.0041881.0041870.2831491.0034881.0041831.3353190.6248461.6483051.6492881.6492880.2837610.6238310.6241571.0038120.2831380.6238300.2827732.3867931.0054880.2831351.3346201.3356180.2834450.2824470.2824462.0556550.2841211.0034831.0041621.3352971.3356080.2834350.2824441.7245170.6251770.2827641.0034790.2831170.2824431.0034780.6244950.2827601.0034780.2831121.3346141.0044540.2831102.3867871.6779422.3880762.3887311.6499093.4989843.1210312.4098880.9671580.2830671.6862371.0047622.7875911.3471491.3075550.6247551.3068931.3457890.2834051.7245101.0047841.3455101.3458180.6247811.0037811.7251610.2837390.2824351.6479591.6491911.6491911.0047010.2830820.2824330.2824320.2824320.2824311.0034681.0041131.7251500.6251010.2827360.6238120.2827340.2824301.0034660.2830700.2824290.2824290.2824281.0034650.2830660.2824281.0034640.2830641.0034641.0040990.2830620.2824270.2824260.2824260.2824260.2824260.2824250.2824250.2824250.2824251.3448421.7254241.0047182.0282330.9667041.7250910.2836761.0034611.7251210.2836721.7244971.0047061.7251190.9664290.6243931.3451340.6247170.9654780.6243890.2827140.2824210.2824200.2824200.2824200.2824193.4389380.2851121.6759750.6249870.6240910.6240900.2827081.3345910.2833101.3345910.2833081.3345910.2833060.6237990.2827040.2824160.2824160.2824160.2824160.2824150.2824151.7244880.2836240.2824160.2824140.2824142.0556230.2838941.0034520.2830140.2824140.2824132.7869042.4093292.4090142.0293570.2838590.2824130.2824120.2824110.6237930.6240740.2826930.2824110.2824100.6237920.6240720.2826901.6479351.6490541.3076730.9660110.6243490.9654510.9657291.0040031.0040330.2829961.3448261.0043080.2829940.2824080.2824070.2824070.2824070.2824070.2824061.7244800.2835700.2824071.6479312.0286862.4469200.6255260.2826800.2824050.2824050.6237860.2826770.2824040.2824040.6237850.2826760.2824030.2824030.2824030.2824030.6237840.2826740.2824020.6237830.2826720.2824020.6237830.2826710.2824010.2824010.2824011.0034370.2829680.2824011.0034370.2829661.0034370.2829650.2824000.9651621.3070771.6487261.6489911.3076090.9659611.3070750.9659591.3070741.3073381.6487191.6489831.6489831.6489821.6489812.7879462.7888261.6498591.6489771.6489761.6489751.6489741.6489731.6489721.6489711.3075890.6245630.9654200.9656810.9656800.9656800.9656790.6242970.6240361.6481802.4082712.0291933.1678691.6883861.3076050.2831710.2823930.2823920.2823920.2823920.2823920.2823910.6237730.6240300.2826491.6479161.6489441.6489442.0285993.1678492.4093923.1681321.6883520.2834432.4455011.0050441.0039660.2829281.0034260.6243080.2826430.6237700.2826420.2823880.6237690.2826410.2823880.6237690.6240210.2826400.2823870.2823870.2823870.2823863.1077692.7889603.7821324.1625172.7309660.2841850.2823870.2823851.0034220.2829130.2823851.0034210.2829120.2823850.2823840.2823840.2823840.2823840.2823840.2823830.2823830.2823830.2823830.2823830.2823830.2823831.0034191.0039400.2829030.2823820.2823821.0034180.2829010.2823820.2823810.2823810.9651440.9656331.7249441.6872141.0044231.7249701.0044481.6866961.6489080.9661180.6242481.3067671.6486340.6247332.4074592.4087253.1680361.6499521.6488741.6488721.0043821.0039261.0039251.7249610.2833961.0034150.2828861.0034140.2828851.3345501.0041530.2828842.0658311.6491530.9660971.3069991.3072370.9658561.3069981.3072351.3072350.9658531.3069961.3072330.6244701.3067570.6244680.2826121.0034111.3350470.2831040.2823740.6237550.6239912.0175450.2835720.2823740.9651361.6483691.6488401.6488391.6488381.3074560.2830770.9651350.9656030.6242220.6239870.6239870.6239870.6239860.9653672.7873291.6496061.6488291.6488271.6488271.6488261.3074440.2830670.6237520.9653640.2828330.6237510.2826010.2823690.2823690.6237500.2825990.2823690.6237500.2825990.9651310.6242090.2825980.6237490.2825970.2823680.2823680.2823671.3065110.9658151.6483501.3074251.3071961.3071950.9658131.3069661.6485741.3074201.3071921.3071921.3071910.6244280.9653550.6242000.6239730.9653540.9655800.6241980.6239720.6239720.9653531.6866152.4081270.9665281.3069591.3071831.6485631.3074061.3071811.3071800.9657981.3069550.9657971.6483360.6246380.6239680.2825860.9651250.9655710.2828080.2823620.2823620.2823620.2823620.2823620.2823620.2823610.6237431.0036190.2828281.3345340.2830421.0033980.2828270.6237420.6239621.6481061.0042771.6866250.6246451.0036172.4459343.1678973.1683592.4473220.2837480.2823601.7244321.7253551.0043190.2828200.2823591.0033950.6242000.9653390.9655560.9655560.9655550.9655550.9655550.9655540.6241720.9653370.9655530.9655531.7248633.1674171.0052182.7770602.0188682.3980441.3461090.6244081.6863710.9660030.6241671.6761250.9659941.3349582.7389880.9666591.3452021.3454390.6244021.3449870.6244002.4074040.9664440.6241631.3067110.2829930.9651171.3069230.6243730.9653290.2827780.9651160.9655401.0038140.2828002.0555631.0044881.0038360.2827990.6237340.6239450.6239450.2825631.3447700.2830071.7244261.0042760.2827961.0033891.3452123.4982873.4510883.4995773.4510873.1199191.3082310.6243590.6239410.2825590.6237320.2825590.9651130.2827660.6237320.2825580.2823500.2823501.3345220.2829880.2823502.0555590.2834240.2823500.2823490.2823490.2823490.6237304.1601103.8311083.4997722.0677430.6248050.2825540.6237290.2825530.2823480.6237290.6239341.3066960.9657240.9655190.9655190.6241371.3066951.3071030.9657210.2827550.2823471.0033831.3349490.2829731.3345190.2829720.2823461.0033822.7770200.2838260.2823460.2823450.2823450.2823450.2823451.0033821.0038080.6241520.9653090.9655100.9655101.3068910.6243290.9653080.9655080.9655080.9655081.3068890.2829451.6478690.6245251.3066881.6764970.6245402.3868880.9663361.3349151.6765110.2831562.7280691.3359402.0178910.9661150.2827400.2823420.2823421.3064860.2829360.9651050.9655000.9655000.9655001.3068811.3070780.9656961.3068800.6243140.9653010.6241160.6239190.2825370.6237220.2825370.2823410.9651030.9654950.6241140.6239180.9652980.6241130.6239170.6239160.2825350.9651020.6241110.9652970.2827290.2823390.9651020.9654910.2827280.9651020.2827270.2823390.2823380.2823380.2823380.2823381.3064820.6243000.2825321.6478630.2831100.6237190.2825300.2823380.6237190.2825300.2823370.9651000.9654840.6241030.6239100.6239100.2825290.2823370.2823360.2823360.2823360.2823360.2823360.2823361.0033730.2827391.7244090.2831410.2823360.2823360.2823351.0033721.0037730.2827371.0033720.2827360.2823350.2823351.0033711.3451531.0039603.1668811.3463500.6243040.9652862.0278921.6870982.0665652.0667732.7878101.3461331.3453381.3453372.4077551.0045391.3451481.3453351.3453341.6867151.3455211.7249892.0665781.6871090.2831011.7244061.7251940.9658830.9654680.6240860.9652811.6482301.6486021.6486011.6486012.4461851.0045451.0037611.7247971.0041511.7247961.0041502.4075581.6490071.6485961.3072140.9656480.6240811.3066591.6484090.9658301.3068431.6484080.9658291.3068421.3070250.9656441.3068410.9656430.6240780.6238940.2825130.2823300.2823290.6237110.6238930.6238930.6238930.2825110.2823290.2823290.2823290.6237100.2825110.2823290.2823280.2823280.2823280.2823280.2823280.2823280.2823280.2823280.6237090.2825090.2823280.2823280.2823270.6237090.6238890.9652700.9654500.6240690.9652700.6240680.9652690.9654490.6240670.6238880.2825060.2823261.0033631.0037421.3451231.7249571.3455002.7873752.0288182.7877311.0046730.6240850.6238860.6238850.6238850.6238850.2825030.6237070.2825030.2823250.2823250.6237061.0035390.2826990.6237060.2825020.6237061.3346741.0039061.0037350.2826981.3447421.3350461.0039051.3348691.3350401.3350401.3350402.3872122.3877541.3355812.3872110.2834062.3969150.9661721.6482000.9657872.0278552.0283991.6487432.4078592.7879032.4084412.4082462.7879012.7880942.0670570.6246150.6238781.7245702.0665120.9659941.6864691.3071802.0662982.0666831.6870282.0664902.0666820.9659891.3450861.3452781.0038961.0037230.2826861.0033580.2826850.2823210.2823210.2823210.2823200.2823200.2823201.0033570.6240650.2824920.2823200.2823200.2823201.0033571.0037190.6240631.0035280.2826810.2823200.6237011.6480151.6485281.6485281.6485271.6485271.6485261.6485261.0040371.0037150.2826781.0033561.0037141.7247512.0562451.0042371.0037140.6240580.6238690.2824880.6236990.2824870.2823180.9650800.9654180.9654180.6240370.9652490.2826550.2823180.2823171.6478431.6485151.6485151.6485151.6485141.6485151.6485141.6485130.6243692.0556941.3353600.2828330.2823171.3344891.3350040.2828312.3866621.3355180.2828310.2823161.3344890.2828291.3447340.2828340.2823160.2823160.2823150.2823150.2823150.2823151.7243890.6243970.2824810.2823151.0033521.0037010.2826641.0033520.2826641.0033520.2826632.3866612.4081671.6488662.4078102.0285201.6486822.4078092.7393102.7879883.1676672.4085381.6488611.3071140.6241871.3448961.3452410.2828232.0657680.6245492.4455872.7878401.6490371.6484921.6484911.6484911.6867652.0664381.0042011.7247301.3454181.6483451.6484881.3351360.2828131.0033490.6240360.2824740.2823121.0033490.2826540.2823120.2823121.0033491.0036902.0558631.3353231.0038460.2826521.0033481.0036882.4457620.2833321.6478370.9657170.6240140.9652340.9653950.6240130.9652340.9653940.6240130.6238521.3066150.2827910.6236921.3346432.0662581.3455633.1184361.3460551.6866071.3071100.6241701.3066130.2827881.3447281.6763591.3071030.9655502.4074641.6488241.6484701.6484701.0039811.7247182.4078152.4464050.6246941.0035041.0036800.2826431.7243822.4460861.3457280.2828000.6236900.2824660.9650710.6240050.9652280.6240040.9652280.6240040.6238470.6238460.9652280.9653850.9653840.2826210.2823081.3064521.3069210.9655400.9653830.9653830.6240010.6238450.9652260.6240010.2824630.2823070.2823070.6236880.2824630.2823071.7243800.2829640.2823070.2823070.2823060.2823060.2823060.2823061.0033430.2826330.2823061.0033430.2826330.2823060.2823061.0033430.2826320.2823061.3064500.9655310.6239950.2824600.2823060.2823050.2823051.0033420.2826301.0033420.2826301.0033420.2826290.2823050.2823050.2823050.2823050.2823050.2823050.2823051.0033410.2826272.0657590.6244840.2824570.2823040.2823040.6236851.7245300.2829480.2823041.3447220.2827770.2823040.6236851.3066001.3069031.3069030.9655220.9653700.9653690.6239881.3065990.9655202.0277872.4079133.1673913.1677272.7778261.0044430.2826222.4454131.3456761.0038091.0036581.6864201.6484471.3070480.2827540.6236840.6238340.6238340.6238340.2824521.0033390.2826190.2823020.9650650.6239832.0659060.2830831.6861011.0039531.0036540.6239980.6238320.6238322.7766971.0044271.7246900.6243121.3448680.9655270.6239800.6238320.6238300.2824490.6236830.9652120.2825970.6236830.6238290.9652120.2825970.6236820.2824480.2823000.6236820.2824480.6236820.2824480.6236810.2824480.2823000.2823000.9650630.9653570.6239760.6238281.6479720.2828880.2823001.3064440.2827400.6236810.6238271.6479710.9656481.3067370.6241200.9652080.6239730.6238270.6238270.6238260.2824450.9650620.2825911.3064430.2827362.0657540.6244411.3448633.1668981.3076722.0661900.2830582.4454091.3073631.3349072.3870921.3353653.1081280.6248792.3867891.3353641.3349171.7248172.0561191.3352221.7248170.2829081.6758521.3350592.4458523.4984952.7881473.1675023.4987983.1678013.1676611.6873120.6242700.9652040.9653470.9653470.2825840.6236780.6238210.6238210.9652030.9653460.6239641.6479650.9656310.9653450.6239641.3065830.9654871.3067260.6241061.0034761.0036340.2825970.2822961.0033331.7246701.0039340.2825961.7243690.2828961.7243691.0039320.282595

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/
With following parameter combination: {'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_576e2a88_2020-02-05_07-02-2235kn2j1s/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
| disagree  |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
|  discuss  |     0     |     0     |     0     |     0     |
-------------------------------------------------------------
| unrelated |    762    |    162    |   1800    |   6898    |
-------------------------------------------------------------
Score: 5173.5 out of 7516.5	(68.8285771303133%)
Accuracy: 0.7168987736437331
F1 overall: 0.2087772397094431
F1 per class: [0.0, 0.0, 0.0, 0.8351089588377724]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<6:45:59,  2.53s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<3:45:07,  1.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<43:51,  1.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2539.24it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 1203
  Number of epochs: 3
  Batch size: 8
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=1203.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0065870.0069310.0038550.0016900.0007810.0005280.0004640.0004830.0004960.0004570.0021950.0023420.5829040.0451390.5988420.0402020.0027840.0004500.0037660.0004510.0047270.0004780.0021900.0026960.0064391.6563830.0639450.0026070.0003390.0020810.0027520.0003660.0002790.0003430.0004270.0002980.0003020.0002980.0002770.0002980.0002850.0004070.0003310.0021020.0003440.0002710.0003020.0209240.1622600.0035780.0003290.0041670.0003470.0002880.5445740.0103720.0004960.0005480.0021280.0004040.0021480.0003430.0021590.0003430.0003030.0003280.0217470.0086910.0005090.0003750.0143460.0023990.0019080.0003870.9527680.2656991.0848080.5682320.0080100.0003830.0024930.0023060.0023960.0003330.0003110.6108670.3467830.1626930.3189350.0061260.0003130.9710410.0147961.6804900.0359932.0510920.5303720.5605240.0059790.0003280.0038570.0003000.0002650.0002730.0002750.0026890.0003030.0002690.0003280.0002710.0002620.0021870.0039620.2901800.0028040.4039640.2769001.1421770.0099800.0024600.0003100.0120740.0003720.0002780.0003370.0003070.3197500.0028140.7350710.2167880.4792820.0039380.3795480.5998420.0047410.0003090.0027541.4885420.0140780.0003720.0002740.0002600.0002610.0002570.2437370.3779340.0028570.5936081.5446260.0106900.5930330.5984010.0066080.4944090.0054100.0003020.0002660.0002840.0002640.0062850.0057580.6154170.0059410.0003070.0086400.0082180.0714780.0060810.0038870.0038120.5130770.0032810.0003070.0002680.0002910.0002920.0002820.0002690.0002660.0003220.0002690.0002780.0002770.0025350.0002880.0002870.0002800.0002610.0002980.0002780.0002740.0020980.0003770.0003170.0003460.2433520.0015430.0003020.0026630.0002730.0002960.0002670.0002590.9326290.0048370.0006380.5119350.1681820.0455800.5316270.5467710.5980630.5611630.0052540.0003080.0002880.0019710.0002910.0002900.0002960.0003070.0003220.0247510.0110120.0042630.0071430.0022220.0039680.0010150.0002630.0002610.0002790.0047290.5043710.0043350.0557340.0024920.0025790.0040070.0003520.0003350.5900090.0074480.9582640.9680500.0064540.0032130.0003060.0035060.0002950.0021620.0040000.0003400.0003020.0090930.0021420.5399270.6003660.6041961.1754430.0069600.5894470.0490170.0026040.9042920.0102310.0022420.6208120.6024871.7931070.0070170.0023310.5786760.5714840.0042730.0026160.0021110.0009981.0185300.0083750.1967260.6521190.0060990.0021170.0003400.0020190.0020650.0052680.5447721.1113960.5418220.0022670.8208300.5960430.0060300.0029361.0493541.0760140.0039210.6631730.3360801.2004190.0185330.0074120.0094821.8890860.0082432.4073640.0116730.0021120.6057750.6107350.0040340.0021090.6085330.0039980.0020640.0003090.0020310.0020530.0188110.0367430.0144460.0043010.0069750.0180270.0067470.6390340.0466920.0146500.0661480.3944790.1874910.3710490.1052530.1052710.0006220.0020981.0375690.0146080.0003300.0002710.0003680.0025910.0002780.0002720.0002660.0002840.0024770.0043760.0022140.0022700.0039890.0194250.1265090.2535000.4716530.1594601.1481030.0034450.4632770.0015350.0022460.8113090.3206990.4507380.9087191.7361020.0066840.5637050.4930021.4355570.4712610.0015790.0398630.0004130.0415890.5490471.0897000.0031371.0790260.0031010.0074120.0003350.0020150.0146040.0146640.0145300.0022950.0002580.1308960.0006380.0054120.6800410.0073280.0244930.4443970.0067790.1389921.0858970.8339172.1487610.0092330.0043250.0239320.0434840.2099600.1899810.0235200.0279030.0433510.7388990.0047400.0070900.0091470.0100340.0074690.0131240.0044510.3488400.0035260.0003280.0003550.0036750.0023250.0045630.0002540.0144300.0028120.0002640.0002640.0002560.9624120.0024820.0002630.0002590.0002630.0002610.0002590.0002620.0028231.1831410.5500440.5284000.0264150.0003540.5795370.5820060.0016900.0058320.0114350.0065570.7578140.0074620.5572220.5928110.0016020.0339430.0056680.0125610.0603770.1711070.1512950.1798710.0102530.0081260.0039610.0104330.0120930.0041930.0003120.0024840.0002710.0020180.0002660.0002670.0002940.0023650.5430510.7108120.0056620.6554130.0041110.0025470.0003160.0025710.0003260.3418880.9634230.5653591.2043721.7407880.1644550.0006330.0002950.0002930.5957680.0014871.1854101.1818790.0026730.0003740.0013090.0003280.0830120.0005140.0077060.0011090.0003680.0003760.0003932.2008311.1530970.0025310.1811450.0051990.0048870.2094820.2763880.0112190.0193020.0025100.0095450.1208470.9913290.0054080.6371260.0015030.3823480.0871470.4955330.5206160.0081760.0041270.0033920.0037270.2125420.0040890.5767271.1580461.6972001.1636173.5064220.0173050.0058170.0091990.0091630.0075130.0045140.0054120.6596440.0209730.0056880.0096670.0163830.0040780.0002910.0002730.0085540.3228650.2031310.0006420.0035190.0002640.0024810.0002600.0025120.0047460.0002700.0002630.9412690.0019060.0002620.6202581.0541940.5427730.0037960.0047941.7076171.1677730.6056090.0080000.0021830.0002860.0002771.9028680.7070990.0061830.0025750.6611780.6471720.6245780.6362480.0013630.0003000.0002960.5346980.0011570.5542930.0036450.0002771.3812751.1990390.0126040.0002730.0020860.0022640.0002640.0038520.2829311.1060140.6874810.0092240.0053940.0048920.0028600.0010340.0002940.0002690.0053420.0090130.9658580.0036580.0004240.0021720.0004400.0021160.0003700.0004210.0021450.0021540.0016540.0021060.0003630.0027980.0002780.0027150.0027471.4939830.0253670.0182932.7422691.7130180.3397080.0242560.0256912.0120510.0176890.0146850.0534740.0146170.0136890.0123630.0182550.0098710.0166920.6927851.2305980.0076920.0002730.0003720.0002870.0037680.8967710.0177310.2129480.0190160.0065530.0100520.0025430.0216680.0031990.0002610.0025240.0026640.0025230.0002570.0002591.1104420.6094340.4444470.0009120.0025540.0025720.0002640.0002600.0002760.0002630.0002590.0002710.0002671.1099550.0018630.0025190.0002620.0002740.5094681.2161750.0108920.0085370.4303400.0084540.0089950.6608730.6680601.1997390.0064900.0075620.0029520.0028670.5518850.0051670.0143720.0093580.0092280.0112160.0094840.0113400.0078040.0033580.4597280.0010080.0055700.5485200.0046610.6763310.0457600.0079470.2116990.0057300.0045000.0101362.8170810.0178540.0140320.0054080.0105280.0031620.0002670.0044680.0002660.0028700.0480400.0024320.0002690.0002620.5826831.1647212.3099050.5786352.3217032.3157370.5934170.5422980.0198820.0114230.5317131.3841890.5858140.0169400.0290450.0384990.0423370.0311250.0075770.0093820.0036880.0003020.0003200.0003110.0023180.2265260.6072560.0038790.0044940.3915030.0097550.2241280.3660230.0094690.0049550.0072900.0002570.0045370.0077360.0078230.0078570.0057920.0081530.0088080.0114741.4163211.1565950.5680350.0065090.7362862.0862700.6143170.5865330.6110143.5336352.3236001.9064800.5899150.0045070.0073580.0024950.5806870.0032250.0038450.0020670.6115871.8334400.6123801.6237212.8491051.9085330.4092760.0026670.0020860.0039460.0020790.0002640.5805190.7933420.0012300.0002650.0164321.6045780.5058690.5970000.0061820.6004571.7670852.2192361.4301713.5752781.1587170.0041000.5861100.5874050.0032310.5895610.0009510.0003930.0025520.1711690.0100070.0107990.0061220.0076390.0094290.0075620.0075110.2549982.4455651.8399491.1547720.9026190.0059960.0002720.0054340.0071660.0072030.0106300.0089440.0054550.0037140.0020550.0003380.0075530.0040030.0057550.0040640.0039960.0058130.0003860.0076800.0039210.0004040.0003610.0003950.0078450.0075300.0021400.0004790.0021250.0040780.0073050.6851970.0010610.0003100.0002920.0002820.0025730.0048240.0002870.0025680.0025340.0025640.0002770.5796210.6523100.5077822.3116541.2383401.5157370.2158460.0105800.0053730.0070480.0086920.3844860.0058110.0068540.0074060.5524160.0166110.0147531.5315131.3436042.0686420.9815222.4316151.6738430.2224200.8398770.0281300.0154600.0106860.0179390.0114120.0002760.0020570.0040230.0020440.0002550.0020620.0002540.0002540.0002560.0002620.0002700.0020320.5840270.0027990.0058090.0065890.0058490.0077990.0045670.0002880.0049760.6014660.6063880.0315620.8309020.0168930.0048240.0059100.0002940.0128330.3066620.3931720.5862030.6016101.0814260.2249380.0959300.9783440.0968220.0994370.3105250.0122740.5921781.5850841.8184450.3222150.8966270.0082470.0054240.0090390.0110990.0105090.0106990.0090090.0072000.0042110.0025640.0002710.0002920.0002820.0025440.5973440.0008950.0026020.5962210.0031070.0021140.0163960.0157650.0157700.0107780.0027370.0052210.5943080.0058100.0040720.0020830.0002740.0073200.0061260.0038410.0002752.2949821.4271501.6170151.2446191.4464900.0803351.0634650.2323060.3573860.0006650.1549660.9096500.0012220.0014130.0003420.0091970.0003590.0097260.0030040.0029581.2795831.2342661.5586481.0978781.5907260.0187660.0131460.6106060.5787511.1434972.2708642.0775750.0185331.3316180.3385440.1717261.1317110.0036660.0031250.0028270.0002820.0029401.0763640.5778580.0034380.0101630.9339300.0193290.0163140.0534440.1259770.1320330.0770521.5759502.2005521.1121130.0094860.0061620.9976970.0105331.2412650.0152670.0072000.5939740.0058890.0025270.7472190.9303790.0031220.0065960.0073480.0351240.0052980.0098950.0103270.0074620.0088580.0070330.0037190.0053600.0002770.0041630.0002700.0050580.0002540.0002640.0002620.0028600.1077570.0003510.0027950.0002610.0233600.0028300.0002730.0025930.0026960.0046520.0002710.0002570.0002660.0002700.0031580.0872690.0003340.0074180.0061530.4609120.0007230.0091720.0131750.0080190.0067540.0120610.4615361.1858130.5951530.0076260.8117740.0090040.3744570.6367370.5648810.0066770.0030260.0042521.0049690.0251630.5891380.0048701.0915290.0082470.5892640.0070910.5494980.5676770.8789260.5700420.5431600.6929270.0026320.6999670.0055880.0003430.0040060.0065940.0101890.0002890.0060170.0039600.0114550.0093380.0075350.0039950.0022010.0176830.0065370.4622970.0184220.6330360.0090661.6847901.5187400.9594721.5455361.7333180.6027090.3515171.2666951.3437980.0218830.3580260.2422370.0106270.0084350.0021400.0043250.9373561.5678000.5004570.0094400.5076950.0058930.0025370.0025070.0069840.0047100.0047510.003239

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/
With following parameter combination: {'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5a318b48_2020-02-05_07-02-27foyclwm3/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    537    |    69     |    169    |    12     |
-------------------------------------------------------------
| disagree  |    28     |    41     |    13     |     0     |
-------------------------------------------------------------
|  discuss  |    164    |    39     |   1559    |    14     |
-------------------------------------------------------------
| unrelated |    33     |    13     |    59     |   6872    |
-------------------------------------------------------------
Score: 7143.0 out of 7516.5	(95.03093194971063%)
Accuracy: 0.9362918312201206
F1 overall: 0.7229746546075881
F1 per class: [0.6933505487411233, 0.3360655737704918, 0.8719239373601789, 0.9905585585585586]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:15:24,  2.72s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<4:01:25,  1.90s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:42:28,  1.33s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<32:55,  1.07it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2515.87it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 2406
  Number of epochs: 3
  Batch size: 4
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2406.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0059140.0071790.0036260.0012450.0003510.0001060.0000540.0000440.0000410.0000400.0000390.0000400.0000400.0000400.0000460.0000390.0000380.0000400.0000380.0000380.0012690.0000970.0012810.0000910.0015950.0052290.0002370.0000451.3724740.0485940.0016580.0000920.0000430.0000410.0000390.0000390.0031330.0001230.0000410.0000470.0047260.1263600.0030440.0001100.0042350.0001320.8539290.0182060.8622170.0176331.2065671.8104290.0348540.0008170.0000600.0000360.0000370.0000840.0013020.0000650.0046720.0001170.0000550.0000450.0000420.0000410.0000500.0000410.0000450.0000400.0000440.0000460.0000420.0000450.0000410.0000410.0000420.0000430.0000400.0000400.0000420.0000510.0000410.0000420.0002160.0000460.0000470.0013440.0000620.0000520.0000470.0000530.0000500.0004980.9798260.0103510.8854830.0176490.0002190.0000430.0001160.0000840.0000390.9059620.0087530.0001220.0000980.0000381.0124170.0105530.0008810.0000440.0000370.0000370.0046420.0077210.0001040.0048680.0000770.0000370.0000370.0046170.0000770.0000380.0000380.0032700.0000630.0000370.0000370.0000380.0000370.0000380.0037280.0091320.1418420.5762470.0042790.0000810.0000900.0000530.7077340.0135760.0001340.0021100.9172560.0063680.0000830.5618940.0040960.0000680.0004570.0000470.8290411.0075550.1006400.0006850.0000400.0000360.0000360.0000370.0000640.0086130.0091220.0000920.0000370.0072920.0000800.0000370.0000380.0000380.0077921.7877680.9253780.9234210.0053450.9086191.0385810.0059050.0027810.0000540.0000370.0003111.6805500.0092210.9880180.0053762.8578491.4443150.0077190.0000813.2776271.7948860.0126241.2337280.6775430.0035100.0000530.0000360.0000360.0000390.0041860.0252140.0001610.0000360.0000350.0000360.0000370.0000350.0000350.0000360.0133750.0000990.0000360.0000390.0000520.0000360.0000360.0000360.0000360.0000350.0000360.0000360.0012660.0000410.0024860.0000470.0000530.0048050.0000590.0000380.8076780.0080690.0061490.0000632.1368640.0091300.0002310.0000370.0001740.0058210.0000620.0000370.0000380.0073780.0000670.0000370.0000370.0000370.0000380.0003660.0000410.0000370.9683450.4952590.0019870.0000470.0005811.3610960.0053120.3532460.3803950.0014940.0000410.0000370.9219491.2068020.0103540.0000780.0000380.0000380.0000390.0000390.0000400.7976520.0041821.3819440.7022170.0025720.0000480.0000380.0000370.0000370.0000390.0000380.0000390.0000380.0000380.0000380.0000381.0227590.0035650.0067470.0000610.0000380.0000381.2746021.1985080.0040780.0000520.0000390.0000381.2517841.2645440.0042110.0000511.2999381.1626690.0038220.0000480.0013380.0000400.0000350.0000350.0000350.0000350.0000350.0000350.0000361.1282410.0035740.0000472.4486810.0076421.4248010.0044350.0052860.0000530.0000370.0030540.0082340.0080970.0012920.0000390.0109910.0012930.0025100.0012820.0012580.0000390.0024980.0012741.4224650.0041980.0000500.0000380.0000380.0000380.0000390.0000390.0000400.0000390.0000380.0000390.0000390.0000380.0000390.0000400.0000410.0000390.0000390.0000390.0000380.0000430.0000560.0000380.0000390.0000560.0015880.0000430.0000380.0000380.0000420.0000390.0000410.0000380.0000410.0000470.0000380.0000380.0000380.0000390.0000380.0015850.0000431.7090910.0068460.0000580.0000420.0009350.0002570.0000420.0036860.0000510.0000580.0000390.0000440.0272490.0001070.0000380.0000410.0000810.0000390.0000410.0000390.0000380.0000380.0000400.0000650.0024590.0000440.0027700.0000450.0041190.0000460.0054260.0000490.0000361.5711190.0082770.0000560.0122040.0089720.0012821.4186281.4257270.0033960.5743130.0013840.0000410.0000390.0000400.0000410.0000420.0015990.0000430.0000410.0000390.0000390.0000390.0000410.0000400.0078460.0000580.0000410.0115431.2059281.1811190.0026810.0013140.0000421.7221840.0051100.0013430.0000410.0000380.0013030.0000410.0000390.0000440.0000400.0000400.0000380.0000400.0000400.0027501.6673420.0036162.1941261.1443670.0025500.0000420.0020020.0000431.1900491.2312220.0026290.0028040.0038540.0000450.0000360.0000360.0000380.0023051.4070430.9988060.0065500.0000610.0001000.0000401.6238370.0077290.0000580.1126010.0006560.0000440.0000380.0013600.0000400.0000370.0000380.0000380.0015350.0028140.0000430.0000430.0000380.0000370.0000390.0025920.0002260.0000380.0016960.0000440.0012910.0026050.0000410.0000370.0012930.0013110.0013120.0000390.0013010.0013280.0043170.0000450.0023540.0000470.0020050.0052740.0000470.0040080.0003300.0000380.0086860.0110172.4923040.0047160.8426910.8972772.8234590.0052660.0000490.0000380.0099100.0000630.8780450.8727460.0048930.0000450.0012750.0013110.0000400.0000350.0013300.1404650.0002991.7044420.0046360.0015270.0000410.0000360.0267400.0000840.0162840.1504130.0078810.0053960.0000460.0000360.0000370.0000360.0113340.0086920.0000520.0034790.0000430.0088300.0000530.0111850.1944270.0003720.0087550.0002400.0000380.0180950.0000680.7253110.0025270.0012950.0013200.0012900.0000380.0013610.0000380.0029740.0026210.0005070.0000360.0012791.8254490.0043030.0013041.4090044.1819981.3320110.0941590.0714690.3529600.3510810.0056250.0013172.3058440.0080110.0001191.8247370.6920970.0023950.0012610.0000370.0012601.4253480.0023340.0000391.4269010.0035510.0000410.0012580.0000371.4293380.0023080.0012640.0000370.0000350.0012570.0000380.0000370.0000350.0012590.0012660.0000370.0107772.0170792.0496910.9294320.0063920.0049470.0042590.0001171.3352330.0020930.0025050.0025130.0000390.0037580.0013720.0000390.0012780.0045540.0070110.0060950.0042900.0044480.0044470.0047120.0097680.0000520.0145780.0000590.0042790.0000430.0062540.0012690.0000370.0000380.0000350.0012710.0056760.0089480.0077040.0020740.0000410.0000370.0000370.0000370.0000380.0000410.0000380.0050400.0000470.0000380.0000420.0000370.0000370.0000380.0000370.0000440.0026490.0000410.0026350.0012880.0000410.0012850.0000400.0013320.0012850.0012710.0000410.0012970.0028280.0020450.0021660.0021340.0000460.0021120.0000430.0021630.0000431.4795960.0020970.0000391.1614471.0963730.0015540.0000390.0032600.0000420.0038920.0027880.0093900.0198780.0136140.0080600.0078941.2196953.6375600.0063110.0000500.0013570.0013301.3891642.4850000.0047365.1083602.6219540.0068191.2460430.0017160.0000400.0001370.0074700.0000520.0000380.0075410.0066470.2801630.0004090.6100520.0008460.0000400.0000380.8369780.0011390.0000400.0000400.0000390.0161590.0000600.0000380.0016010.0000430.0051860.0061330.0062900.0067630.0051540.0058000.0012740.0039960.0000420.0000370.0012730.0000380.0000370.0000380.0000360.0025270.0012840.0054980.0025240.0012700.0034330.0026090.0000450.0050610.0062970.0033270.0047090.0025490.0026330.4442461.7420581.0420090.8526110.0137280.0002180.0289290.0204490.0000630.0063820.0176730.8061150.0010320.0212380.7544010.0009642.1863830.0027181.2179250.0074930.0000470.0000381.1991510.7977041.3795700.0017131.7998850.0022170.0122440.0000530.0111060.0080190.0012800.0024980.0024810.0038380.0049680.8579600.0010700.9226850.9063540.0011160.8336240.0010260.0000380.0007160.0000660.0015982.2548510.0026950.0012731.7143920.0020510.0000380.0000370.0163360.0245780.0066790.0000450.0000370.0000370.0000370.0000370.0000370.0000380.0064630.0000440.0000370.0000370.0000380.0000390.0000370.0000370.0000370.0000380.0000370.0000380.0000370.0000370.0000380.0000390.0000370.0071710.0158242.4658330.6696710.0007941.1539880.0124460.0070770.0049270.0000470.0000400.0000800.4903470.4093700.0004970.0021830.6518190.6667800.3362370.3553530.0680760.0187870.0033960.0063770.0049310.0000450.0092570.0031602.7932682.0547920.0022990.0000450.0000430.0028870.0017480.0015880.0021110.0029110.0020390.0042390.0000580.0025500.0025700.0025540.0025340.0025400.0025560.0024960.0025010.0012790.0024950.0000380.0025010.0037230.0012690.0037370.0025030.0015880.8892440.0009840.0000400.1918950.0002420.0000380.0000370.0000390.0015940.0000400.0000380.0000370.0000380.0000380.0000390.0015960.0000391.4020220.0015020.0000381.7341690.0031130.0012821.7340810.0018370.0000400.0034000.0000410.0034090.0000450.0000380.0000370.0033560.0000660.0000380.0080191.4680962.0148331.3323101.3052281.2635370.8852982.2964450.4379200.0118110.8793470.0009310.0000390.0000380.0000430.0000890.0000410.0000390.0000380.9280610.0009710.0000390.8864250.8873590.0009271.7348070.0017740.0000400.0000380.0000380.5166340.0005530.0000380.0000400.0035320.0080150.0000480.0000380.0036790.0093780.0000620.0040700.0000430.0000380.0000740.0000410.0000390.0000390.0017674.0075902.7245660.0134060.0000480.0000350.0000350.0053820.0032160.0031660.0031740.0033280.0000420.1177940.1106800.3612860.0081580.5868190.0414910.0475970.0000860.0015980.0015980.5239980.0005380.0035220.0000393.3548411.2270030.0012041.5895321.5854500.0015420.0000380.0000362.8282920.0027091.1323391.1107061.6835121.5286690.0014750.0000382.3828110.0048330.0000411.2012610.0011600.0000381.2137620.0030230.0000391.2220640.0012220.5505460.2274990.0002490.9699760.5191541.2641142.4945300.0075743.0639302.5552922.4694000.0058500.0000431.2901012.7498923.8478131.3419570.0058960.0015210.0040851.2111960.0011972.6196541.2169830.0136170.9166040.9141910.0140020.0235440.0008510.0000370.0069320.0069610.0082470.0426340.0004900.0000380.0000380.0000380.0000370.0256770.0348870.0619660.6957830.0006610.7083760.0006680.0000380.0321030.0000670.0000390.0000370.0000370.0046100.0000410.0000380.0046880.0000420.0043690.0043740.0000420.0000370.0000380.0000380.0000370.0000500.0000370.0000370.0000390.0000410.0841630.2119670.9089800.8331170.2969511.8509301.7155640.0090000.8414920.0007692.4023631.2910212.3810130.0056480.0018681.1826100.4562870.0030760.0013320.0000430.0005300.0000390.0000370.0000520.0000383.5328550.0031320.9742890.0093990.0083880.0115130.0001111.0459060.0009801.1114650.0009891.2179900.0010710.0594490.0001010.0000380.0000380.0003390.0000390.0000390.0000440.0056630.0000440.0000390.0000390.0000391.3212970.0011450.0029350.0000410.0000430.0000392.7224792.8192742.7840561.1755640.0011060.0000410.0000370.0000350.0015750.0014520.0000480.0000370.0000390.0013660.0015160.0011102.8433052.0746331.4313040.0049770.0016990.0044691.0664640.0054800.0059260.0000431.0458360.0064430.0000450.5761310.0005070.0000380.0000390.0000390.0000420.0120120.0000480.0000370.0052562.5245682.4096810.0032500.0000380.0000390.0000350.0016370.0000370.0000350.0000360.0015160.0000360.0000360.0000350.0000350.0000350.0015720.0000370.0000350.0015850.0000390.0000350.0015580.0000370.0000350.0000360.0000360.8188240.0006810.0000360.7870000.0006570.7760070.0006450.0098382.7537600.0062120.0057260.0053540.0042371.4445312.2402881.5403182.2315770.7116250.0055500.0049730.0049910.0049940.0050092.9305602.9117220.0071890.0049780.0057430.0049461.0834150.0059360.0094920.0102230.0073020.0021160.0041640.0043350.0044210.0066140.0056250.0023190.0023440.0304981.9217671.2540164.0207270.9496090.0101960.0000440.0000380.0000370.0000370.0032700.0000400.0000380.0015960.0015970.0000400.0118180.0097420.0105620.0120790.9082801.6651892.9328230.6195840.0005022.6396470.8700100.0063200.0000480.0054660.6452810.0005880.6999150.0005621.3008140.4683870.0004970.0000390.6254770.4678670.0004050.0000400.0000400.0000400.0000393.6207572.8079504.0051294.9634432.6967130.0020190.0000410.0000380.0027050.0000420.0000381.6863290.0012730.0000510.0000370.0000370.2016780.0001840.0000370.0000380.0000390.8623890.0006620.0000440.0000370.0000370.0027190.0028790.0000390.0000380.0000400.0027020.0000390.0000380.0000420.0000370.8421280.3441671.0570131.1452640.0180270.0543100.1285480.8851750.0067382.5599840.0031920.0037760.0050120.0012722.2260002.1156364.3007523.0658092.9852773.0455510.0053720.0033280.0033750.0063700.0000420.0034030.0000420.0033690.0000411.2551110.0043730.0000420.0085500.0053060.0026460.0043140.0043210.0030040.3493930.0039770.0038280.0026320.0045380.0038720.0014180.0038510.0122060.0033780.8391961.3421450.2277570.0003540.0121521.2388263.7086900.0063410.0000790.0122260.0057140.6436271.9020702.6031063.1820910.0022251.1446681.0359101.0958240.0259280.9962860.9596951.1173082.3113814.0767580.0094650.0049290.0049370.0049290.0049240.0037190.0000380.0012870.0025150.0000370.0012800.0000360.0000360.0000350.0013300.0000360.0000350.0012730.0000360.0025780.0012720.0000370.0012870.0000360.0000370.0000350.0000370.0089760.0039640.0063320.0122730.0122140.0109040.0068430.9065660.0167860.0075740.0092520.0215010.0044440.0013430.0031320.0017120.0013280.0028380.0031620.0013030.0021620.0015420.0030821.4055512.7474630.0043200.0037310.0037260.0049580.0037280.0037300.0037270.0024960.0037320.0027040.0056210.0013700.0013490.0000380.0032560.0031490.0000390.0000370.0000380.0000370.0000370.0000370.0000370.0000370.0088931.0223730.0007001.4119590.0009500.9583250.0006570.0052450.0015970.0062701.2902351.2786540.0025461.3983413.8800155.5644955.5462404.1507150.0027000.0000520.0067260.0066190.0034100.0000410.0000380.0032870.0012660.0024980.0025150.0024890.0025000.0024930.0024860.0024880.0012600.0025020.0046950.0024900.2585510.3925430.1238680.4413331.1938620.7072230.0553170.0024011.2440130.0037120.0014691.7072160.0047591.7124832.9632850.0049151.1985251.3985850.0024551.3696520.0024794.7448232.2340611.1067433.3314131.7879102.2490221.4894150.0022160.0025690.0000370.0025500.0026260.0035560.0000401.2581180.0041420.0035250.0000430.0012710.0012800.0012710.0000361.4260500.0009142.8437011.4277590.0009151.4240541.4299921.7546552.0143873.4052072.2867742.0224990.0051121.4631670.0023060.0003190.0017750.0000370.0025980.0000380.0014010.0000360.0000370.0000381.2401240.0007910.0003061.2389890.0007900.0000380.0000380.0000390.0000670.6799931.0583900.9478420.3114671.1808910.3645030.0002590.4749220.0003320.0000521.7615750.4346351.1134520.7289220.5956100.9817020.3223171.4765181.5234460.9202160.0005850.0000400.9888411.3474960.0008391.3431530.0008360.0000400.0026591.3401240.0008360.0000410.0000390.0000400.0000850.0000460.0026640.0026870.0247470.0122800.0160920.0067690.0069700.0018690.0082850.0122950.0027960.0025290.0101990.0000420.0091170.0019650.0038391.7069560.2194502.3117800.0051561.6162871.6395160.0010443.2047231.6915731.1585140.0084540.0000430.0000360.0000350.0037140.0000380.0024850.0024810.0024940.0026220.0037090.0036980.0024880.0037000.0012840.0024870.0012610.0012570.0000630.0012950.0000400.0000360.0116520.0134820.0080270.0013100.0113890.0097200.0060170.0072990.0000400.0121650.0101631.1627370.0007010.0009110.0115070.0110730.0000430.0097650.0000430.0000380.0000370.0010330.0000380.0000360.0143710.0085070.0000410.0248250.0000510.0060920.0005880.0000650.0068720.0000420.0000370.0126240.0074630.0070220.0052170.0073540.0000420.0000380.0034290.0000420.0000380.0000390.0000510.0000390.0038000.0000410.0703720.0000770.0000410.0000380.0000390.0032530.0032460.0000410.0032620.0000410.0000420.0000390.0056411.0906770.0059690.0205461.0414871.1175060.0142901.0229180.9830311.9693982.0355912.9335290.0641690.2219020.0408350.2475630.0332720.0338550.0898320.3373960.0922150.8195881.0810291.4592480.2003520.0001740.0774080.1065780.0132870.3135250.0097700.0590580.0137790.0050330.0050190.0050700.0225710.0049600.1825400.0106640.1691930.0122970.0049540.0132790.0101680.0228690.0170420.0085120.0048383.3230893.2923260.0080860.0107430.0109290.0041510.0101640.0100350.0041450.0097370.0049250.0028900.0012760.0000360.0000350.0000350.0012710.0012880.0012750.0012750.0000360.0000350.0000360.0000360.0013090.0000360.0000350.0000370.0000350.0000350.0000350.0000360.0000350.0000350.0000350.0012730.0000363.8817410.0020900.0000360.0012600.0012580.0024800.0024840.0012620.0024860.0012640.0024900.0024860.0012620.0012620.0000360.0000370.0057300.0064111.1487580.0113602.0196912.7338620.9366562.7990740.8900910.0019770.0015020.0015770.0015720.0015610.0000380.0015700.0000380.0000380.0000370.0015710.3276980.0002090.2245850.0001550.2477210.2776580.3278410.3129120.0002000.5499970.2773220.3285130.2715520.2779170.2730910.2672990.5488320.5491280.2803700.5516750.0003230.5965460.0028170.0050040.0046451.4333971.4304110.0056892.8719473.3456671.9549031.9370412.8478383.2390321.9298990.0065340.0082971.9113621.9254820.0109360.9543890.0126921.9512421.9299790.9537591.8973511.8725060.0129190.9889921.0357060.9742600.0063130.0000400.0056470.0000420.0000410.0000380.0000480.0000380.0000380.0000370.0062241.0431590.0005620.0000390.0000380.0000370.0062100.0057561.1423590.0049510.0000400.0000380.0032460.0096260.0103830.0109320.0098830.0096150.0102120.0103850.0058400.0061330.0000420.0060560.0060840.0113711.1662070.0069080.0063591.0568920.0078620.0000430.0133750.0000430.0000370.1578120.0124390.1676520.1762520.0149580.0000440.0000370.0000370.2158090.0092840.0132940.0133671.2379023.1287763.4190401.7750070.8367710.5998260.4879220.0002780.0000470.3932900.4578940.0002670.8683300.4457040.0002580.0002210.3787030.0002230.0182900.0000510.0001140.0001961.7714650.0009000.0001570.0000380.0134681.0715700.0005570.0000420.0082360.0071530.0002800.0065070.0000420.0093260.0000433.4727732.7180160.0068142.7944191.3453500.0060512.7194363.1304541.3515750.0140670.0131100.0054180.0039310.0016191.3413871.3205000.0006682.6767090.0027750.0238741.0626230.0082271.8793010.0404430.0285162.2876581.5587780.7583801.0989740.8270060.0054590.0050591.2163250.0006180.0043091.2068800.0006150.0000500.0038290.0000760.0000670.0000420.0037390.0042201.2493731.2459020.0047060.0000412.0077400.0051010.0117040.0000511.6715820.0266730.0136500.2478970.0218530.0255960.2674760.0357020.0124640.0179220.3450630.0002050.0189671.6890702.6141111.2334834.2791551.2668491.2324790.0056620.0024620.0058750.0000391.2118771.7195070.0064730.0031632.5034010.0067890.0059000.0060720.8933181.4733370.8801850.9065710.0070320.8345560.0039590.0000420.0071690.0119501.2010740.0987610.0020520.0000370.0042400.0019830.0042480.0021920.0034960.0021520.0021010.0018930.0040861.8676640.0051940.0000390.0001760.0055280.0056630.0037550.0028250.0033310.0016760.0018860.0031910.0015930.0000410.0000370.0000410.1405770.0001010.0000370.0206700.0000500.0000560.0000390.0000370.0000390.0000370.0000630.2000540.0001280.0000370.3523840.0001960.0000380.0000380.3426660.0001920.0000380.0037420.0028990.0694800.0000740.0000460.0000460.0000420.0135270.0000612.5653240.0011960.0373820.0000590.0000430.0000410.0000420.0000470.0000440.0000490.0000361.0057470.0004862.0510590.0053170.0000380.0000360.0000360.0049092.1507550.0010170.0000361.0871670.0005200.0023410.0017770.0058590.0045270.0054940.0033440.0030390.0038090.0016440.0045920.0039180.9847042.4852344.9445693.6978581.2679660.0042940.0000400.0102971.1304100.0035960.0033150.0057060.0049930.4984950.0002600.9202000.8458090.9497500.9610030.0097351.0811430.0005150.0001810.0102090.0047081.9330860.0008870.9960381.0170471.3368720.0044740.2812410.0050033.4609320.9757931.9925770.0065010.9633360.0169371.2262901.1985870.0068630.0000430.0080990.0091280.0000440.0022040.0053662.3194660.0010440.0015941.7256170.0007850.0015910.0000390.0078290.0000421.7252950.0007840.0000380.0006000.0025190.0053190.0015770.0013170.0077100.0000410.0000390.0080850.0000410.0012760.0012680.0055860.0042360.0038610.0035580.0043080.0027080.0012840.0015190.0015370.0000370.0071480.0000460.0055950.0000400.8902670.0054762.7099830.8292310.4302472.9531490.0012954.2271430.0055931.5569763.1975301.5690184.0929640.0074773.0357841.5104501.5020441.3363322.3241351.2383631.7926360.0030341.3454471.2525360.7920941.2209550.3314340.0150155.9498455.6322535.5429081.4077150.0020350.0027650.0027130.0027390.0000360.0014980.0022000.0014060.0025650.0025900.0014640.0052150.0025510.0025320.0012810.0037920.0028180.0037810.0013140.0026730.0026540.0000400.0000400.0029570.0053780.0026580.0000400.0053160.0000410.0052920.0026610.000040

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/
With following parameter combination: {'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6c167080_2020-02-05_13-27-303b23grrj/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    377    |    53     |    177    |    18     |
-------------------------------------------------------------
| disagree  |     0     |     2     |     0     |     0     |
-------------------------------------------------------------
|  discuss  |    373    |    102    |   1589    |    24     |
-------------------------------------------------------------
| unrelated |    12     |     5     |    34     |   6856    |
-------------------------------------------------------------
Score: 7097.75 out of 7516.5	(94.4289230359875%)
Accuracy: 0.9170650592392434
F1 overall: 0.5946649269628106
F1 per class: [0.5436193222782985, 0.024390243902439025, 0.8173868312757202, 0.9932633103947845]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:06:46,  2.66s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<3:56:38,  1.86s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:40:26,  1.30s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<32:16,  1.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2658.77it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 602
  Number of epochs: 3
  Batch size: 16
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=602.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0001670.0002120.0001500.0001070.0000750.0000960.3690490.4420450.0552920.0062170.0008600.0002640.2319810.0178890.0014830.0027360.0002090.0000500.0000400.0000360.0000750.0000680.0000500.0001980.0007590.0001780.0000520.3982030.0142570.0005710.0002220.0000980.0000450.1718500.0051100.2331060.0065760.0029170.8109900.0208430.0007920.0001900.5350070.8705450.0202800.4337641.6003400.5689980.0134560.0003120.0157790.0003500.0072070.0001780.0000350.0000570.0001020.0006560.4173370.0071540.0024440.0000700.5544130.1660900.4155410.0068800.4003480.0060060.4576940.0067360.0001360.0000330.0005010.1787410.1353950.6789770.0293060.0008210.0000420.0003490.0053320.0001220.7286320.0094740.0002240.0009700.0000410.0000300.0000300.0000320.0000310.0000540.0000310.0000330.0000310.0000510.0000490.0001150.0000610.0014740.0000490.0000370.0000640.0331390.0024680.0106270.0066140.0001190.0000580.0000660.0001370.0125030.5712920.0053520.0000780.0000330.0004840.0001670.0002970.0001580.0009810.3190060.1309730.0070030.0041210.0006990.0000380.0017240.3913390.5279490.0096280.2950890.6674900.2968500.6781940.0053170.5951760.0072040.0027270.8341530.5144290.0037680.0000770.0001160.4195730.3749540.3409910.0024440.0002310.5497610.0043030.0006411.1320620.3341560.0022611.2728230.0082410.3958430.0025630.1696780.0040930.0003970.0193460.0002670.0004420.0004710.0006540.0004070.0000900.0004020.0000330.0015750.0000430.0000320.0002480.0120290.0019960.0002680.0001510.1014030.0742940.0005270.9263640.4732370.1539690.6976430.1068310.0008331.5534761.0068571.3364480.0072280.0002840.0003840.0000890.0002570.0001600.0259080.0321520.1034650.8706500.0399110.0023840.0008320.0006441.4687700.0077740.0005730.0004490.0016820.0001390.0001270.0008040.0003340.0000960.0000350.1495450.0007190.0000330.0000300.0178080.0180330.0003400.5570320.0029970.0006230.2864612.3196570.0103520.0007860.0090990.6443951.3464920.4974800.3609610.2303570.0010370.0001090.0029090.8610630.5559610.0379580.0010740.2579381.3525041.1157420.0046100.0000840.4479831.0956620.0044140.0001300.0003060.0002610.0000390.7197960.5041400.0025340.2959150.5238490.0169410.0249140.0025940.0024360.0018090.0014450.0003340.0002550.0003940.0032530.0177940.8538140.0688020.0005720.2443650.0051860.3655370.4433510.0016281.6544970.0059620.3887280.0014700.0002280.0000300.0001590.0004170.0011100.0828120.1800020.0057690.0002781.1531800.1280000.2619030.5234690.0026760.0000520.0001710.0009330.0003350.0008420.0001480.0084431.8336442.0905740.0070940.0003920.0000730.0002700.0620040.0002600.0001330.0000580.0000610.0000700.0000690.0001350.0001560.5303862.2102203.0080840.4534190.0095990.0059680.0053550.0017420.0227150.1296260.0037230.0000570.0102860.2123240.0015690.0003520.0001780.0000910.0000850.0000400.0043760.0009910.0001790.0000340.0108060.0000700.0001610.0000910.0000330.0007060.0005640.0004801.4970286.2336000.0179920.0004540.0008800.0006510.0003040.0002750.0001880.3161890.3148380.0023740.0010482.2971271.9378840.1081260.0005350.0003500.2693170.0009320.0465970.0001570.3952760.5455401.1184960.7488200.3812890.6070740.0758860.0006520.4310680.0662590.0002510.0000340.2598810.5079120.0041190.1425450.1198850.0007940.0001460.0002230.0001800.0003070.0020490.3577880.5208331.6152390.8218551.0587010.7417890.0141000.0010390.0001900.2780691.2271823.5448891.8286090.0045620.0001820.0163960.0864090.0002922.4481740.4842271.1589781.1339371.4709870.1680180.0764130.0010940.0000860.0028540.0013580.0006170.0002351.2384911.5588070.8165580.0020310.0002110.0002630.0001390.0000930.0001690.0001550.0011000.0001340.0000770.0001090.0002340.0000690.0001020.4693730.0011100.0000360.0002020.0000850.0001420.0002410.0006820.0512670.0169350.0004430.0004560.0008300.0004010.0004091.1039351.4232280.4659530.0026980.0027450.0003500.0002370.0001070.0004070.0000980.0000310.0001330.0000730.9193150.0027080.0004440.0104880.3681343.2545030.3659640.0013630.0006280.0001750.0009330.0019470.0013950.0024520.0012090.8153381.2998370.1660970.0006930.0005150.0008370.0004600.0001840.0000310.0146720.0003450.0819450.0005410.0008070.0042660.1962970.0050650.0002420.0616700.0154680.2744732.7185020.8388680.7866961.1792880.4019830.0022720.0002300.0003490.0001800.6197390.2580020.3920300.2526890.0245830.9537090.5457530.1748650.0050470.0003070.1161430.2262240.1489120.0014110.1823541.0363010.0930720.6117150.0013590.0606540.3185520.0858630.0042590.2849100.0006770.0002440.0003510.0002000.0010410.0002720.0001240.0655590.0001530.0006250.0198330.0061270.0004760.0001300.0008190.0000550.0000370.0003620.0007670.1878540.0006170.0006260.0005483.7723840.0078630.0005360.0002880.0002910.0008670.0006240.0022020.4443290.0012700.0011240.0008720.5753200.0048260.0001020.0018210.0002290.0002850.0010860.0002900.0003140.1577000.9887580.2615410.0029680.0022840.0213780.3837750.0078721.2278930.2528220.0007530.0003350.0003230.0002040.0002250.0002560.000162

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/
With following parameter combination: {'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_66feb5e4_2020-02-05_12-11-140_tep8u5/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    606    |    31     |    105    |     2     |
-------------------------------------------------------------
| disagree  |    16     |    84     |    56     |     3     |
-------------------------------------------------------------
|  discuss  |    133    |    46     |   1618    |    21     |
-------------------------------------------------------------
| unrelated |     7     |     1     |    21     |   6872    |
-------------------------------------------------------------
Score: 7255.75 out of 7516.5	(96.53096520987161%)
Accuracy: 0.9540636042402827
F1 overall: 0.8046440928169298
F1 per class: [0.8047808764940239, 0.5233644859813084, 0.8944168048645661, 0.9960142039278208]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:05:12,  2.65s/it]  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                       | 501/9622 [00:03<4:42:12,  1.86s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<2:45:04,  1.30s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:10:03,  1.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<22:30,  1.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2427.81it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 2406
  Number of epochs: 3
  Batch size: 4
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2406.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0004320.0006340.0003330.0001260.0000480.0000240.0000180.0000170.0000160.0000150.0000150.0000160.0000150.0000160.0000160.0000140.0000150.0000180.0000150.0000140.0001850.0000230.0001990.0000220.0002371.1937900.0459330.0017181.9641000.0679260.0022780.0000870.0000160.0000140.0000140.0000140.0004230.0000250.0000250.0000390.0016560.0002650.0000230.0000161.9012190.0422650.0046640.0001160.0030530.0000791.4271650.0279980.0005520.0000240.0000140.0000140.0000140.0000140.0003750.0000430.0023880.0002870.0000340.0000250.0000250.0000260.0000300.0000270.0030320.0000690.0000250.0000340.0000270.0000230.0000250.0000300.0000260.0000310.0000240.0000230.0000280.0000340.0000240.0000280.0000310.0000480.0000430.0002200.0000340.0000210.0000190.0000180.0000300.5851041.2094410.0127440.0105710.0032890.0000510.0000210.0000480.0000180.0000181.3917640.0134030.0001520.0000280.0000171.4573330.0163012.0644510.0186130.0001800.0000160.0000360.0002200.0000161.8554640.0157390.0001470.0000150.0002410.0000160.0000140.0000140.0003240.0000160.0000140.0000150.0000140.0000130.0000150.0012620.0005950.0002281.6368820.0120590.0001180.0000490.0000255.8839640.0419070.0003130.0001750.0002210.0000270.0000180.0038180.0000440.0000170.9998220.0069870.0003691.7508651.7875660.0115560.0001010.0000250.0000230.0000310.0000291.5387701.5591000.0095920.0000851.0970850.0066350.0000680.0000190.0000160.0013620.3235770.2527630.1210940.0007210.1625741.2813330.0072630.0002650.0000250.0000210.0000282.2809090.0124820.0068560.0000514.2098142.1269420.0113280.0000904.1969210.3896930.0056661.0362550.0129590.0000820.0000190.0000140.0000170.0000170.0000410.0116600.0000720.0000140.0000150.0000140.0006780.0000220.0000140.0000210.0025990.0000300.0000160.0000150.0000260.0000280.0000200.0000140.0000150.0000140.0000150.0000150.0002240.0000160.0004330.0000160.0001340.0002450.0000260.0000241.3286580.0059460.0003980.0000170.0002310.0000170.0000200.0000140.0000160.0002010.0000150.0000150.0000150.0003570.0000160.0000140.0000140.0000140.0000140.0000160.0000130.0000130.0001880.0001890.0000150.0000140.0002060.0000530.0000151.3296931.4408460.0055360.0000360.0000161.4105791.8763441.3772310.0051740.0000340.0000150.0000160.0000160.0000170.0025220.0002022.0525310.0168600.0000780.0000190.0000250.0000190.0000160.0000170.0000180.0000170.0000170.0000170.0000160.0000161.2856570.0044520.0031270.0000290.0000190.0000221.0293240.0111700.0001860.0000190.0000210.0000171.8358641.6836910.0055740.0000360.1094010.7560850.0024760.0000210.0001960.0000140.0000130.0000130.0000130.0000130.0000130.0000140.0000140.0007410.0000190.0000160.0412960.0001441.9721590.0061000.0002190.0000160.0000140.0002020.0008371.1343850.0036540.0000270.0039820.0001890.0011390.0001890.0001800.0000150.0003570.0001800.8893500.0026150.0000220.0000140.0000140.0000140.0000140.0000140.0000150.0000140.0000140.0000140.0000140.0000140.0000140.0000140.0000140.0000140.0000140.0000140.0000140.0000170.0000140.0000140.0000140.0001180.0002040.0000150.0000140.0000140.0000140.0000150.0000160.0000140.0000140.0000160.0000140.0000140.0000140.0000140.0000140.0002090.0000140.0677930.0006550.0000240.0000190.0001860.0000330.0000190.0003120.0000230.0000210.0000160.0000200.0029550.0000240.0000160.0000150.0000260.0000160.0000190.0000160.0000250.0000150.0000150.2058760.0005220.0000190.0003470.0000181.3299380.0032370.1289470.0003270.0000171.2638161.3767860.0033021.3745680.0043080.0002342.0497071.6634640.0039350.0005110.0000170.0000140.0000140.0000140.0000140.0000140.0002960.0000140.0000140.0000200.0000140.0000140.0000140.0000140.0000200.0000140.0000140.0015190.2481470.0013180.0000160.0001940.0000141.8315280.0042530.0002320.0000140.0000140.0002030.0000340.0000410.0000320.0000350.0000320.0000320.0000310.0000330.0002600.0007150.0000330.0030180.0003610.0000970.0000171.7920370.0038220.0034590.0005540.0000250.0001870.0002000.0000180.0000170.0000160.0000190.0001840.8504560.8053650.0251351.2359710.0043770.5186732.7334950.0721780.2618940.0039290.0016460.0000220.0000180.0002000.0000160.0000160.0000170.0000170.0002800.0004560.0000170.0000270.0000170.0000160.0000220.0009500.0000360.0000160.0001770.0000210.0003070.0003750.0000150.0000150.0004980.0006050.0003210.0000170.0001870.0002260.0000290.0000161.3417000.0025900.0001813.6875150.0069870.0004580.0000240.0000130.0001740.0002011.7939260.0033613.1961431.7086823.4229510.0063520.0000280.0000140.0002220.0000191.8448781.7235412.0184660.0036990.0002550.0346120.0000810.0000150.0061220.0000570.1246432.0165550.3430080.0598820.0001230.0000150.0002580.0000150.0007930.0001720.0002180.0001880.0000150.0000140.0000140.0000140.0002060.0001780.0000150.0001750.0000171.1954520.0020881.3342841.3443650.0023350.9501820.0028420.0000211.1345640.0019571.1207410.0029920.0001850.0001920.0012960.0000160.0006850.0000150.0003560.0004150.0000190.0000160.0002631.7891310.0034340.0010830.5810063.7563050.0072530.0006220.0008370.0024410.0003620.0009740.0004144.6508390.0079860.0000443.1594773.0245420.0051040.0002130.0000280.0001922.0548790.0057260.0000231.9959370.0033940.0000190.0002010.0000142.0313640.0032430.0002010.0000140.0000130.0001930.0000140.0001350.0000140.0002090.0002130.0000140.0024530.0329540.6511550.2924840.0011650.0009320.0011760.0000250.0365170.0000700.0003860.0003860.0000140.0005950.0002430.0000150.0001860.0008183.1164670.0064350.0018950.0002440.0513050.2129410.4583410.0007030.9102200.0013780.2063270.0003220.0291180.0002250.0000140.0000140.0000140.0001830.0016060.0010750.0019030.0008890.0000460.0000170.0000180.0000180.0000190.0002340.0000290.0004840.0000170.0000210.0000170.0000230.0000190.0000160.0000210.0002370.0002190.0000170.0002130.0001990.0000150.0001860.0000150.0001940.0002000.0001990.0000160.0007960.0003060.0004380.4358821.4868350.0021181.5163420.0021410.4358860.0006271.9224850.0026910.0000182.0435771.6795250.0023400.0000170.0002210.0001690.0002040.0002080.0071630.4139630.0191590.3674803.4405322.2553605.7073080.0079950.0000310.0002750.0002530.2709802.7430710.0039364.1967822.5218360.0038061.0122970.0013780.0000190.9943320.0047450.5436470.0007390.0157070.0071351.6284460.0021723.2796370.0043480.0000210.0000152.9933870.0039480.0000200.0000250.0000151.7555480.0023060.0000170.0003050.0000150.0007930.0007450.0007510.0007090.0007550.0007770.0002710.0036840.0000190.0000150.0002170.0000140.0000150.0000140.0000140.0003710.0002090.0003310.0003830.9667910.0014580.7877510.0010181.4661140.0021970.0001950.3424460.0006200.0041511.7745310.0033591.4503061.7511180.0024420.0000180.2989620.1023000.0001431.5555160.2158940.9875130.0012390.0055040.0007080.0000160.4994250.0006270.6140890.0565020.0000830.0000170.4111272.4767961.8160290.0022230.0010270.0000190.0033380.0000230.0009080.0009120.0002010.0003910.0003840.0005720.0042550.0052960.0000220.5184941.9049930.0022830.0786280.0001080.0000140.0000160.0000170.0002810.0003350.0000220.0072441.2671770.0015050.0000170.0000150.0005250.0006750.0001760.0000150.0000150.0000150.0000150.0000150.0000150.0000160.0086020.0000250.0000150.0000150.0000170.0000150.0000150.0000150.0000150.0000160.0000150.0000150.0000150.0000150.0000150.0000170.0000161.5615930.0028711.3632500.9435890.0010800.0020740.0013670.0007750.0005320.0000200.0000200.0000211.3475011.2653340.0014310.0000430.0000220.0002880.0002840.0005110.0006730.0004640.0002530.0004150.0002030.0000140.0006530.0001946.0011264.0239180.0044520.0000260.0000190.0004980.0004810.3866330.0008560.0003780.0003200.0014240.0001540.0008980.0043150.0124570.0005140.0057180.0004810.0008610.0007600.0041570.0003610.0000220.0003580.0010250.0003080.0005550.0012640.0002620.0287670.0000490.0000290.0642090.0000820.0000140.0000140.0000140.0002610.0000160.0000140.0000150.0000150.0000140.0000380.0003170.0000151.9901980.0020930.0000162.2604070.0025780.0002212.2292640.0023320.0000290.0003920.0000260.0003020.0000280.0000180.0000260.0003000.0000930.0000230.0532340.2876890.5159540.2429570.4334350.3419140.4157042.1311310.0065340.0013740.0002060.0000170.0000160.0000150.0000150.0000160.0000170.0000160.0000181.4157490.0014400.0000181.3633951.1638430.0011820.0066070.0000230.0000170.0000170.0000330.0010710.0000170.0000150.0000140.0003030.9500600.0009600.0000160.0555371.4594130.0109020.0000770.0000150.0000170.0000400.0000200.0000170.0000170.0002466.4501554.1578300.0083630.0000230.0000250.0000160.6815920.0010040.0003210.0003430.0002660.0001230.3080210.4090930.5816390.0007980.0009780.0008220.0008270.0000190.0002860.0002560.0014910.0000360.0005130.0000162.2280050.0887810.0001000.5277862.1281370.0020380.0000200.0000160.0532490.0000660.0422920.6752310.1746290.9982410.0009550.0000160.0073830.0007700.0000160.0037630.0000180.0000160.0039230.0063500.0000200.0027800.0000190.0083350.6272790.0006000.0159472.6141290.0171530.0169390.0011614.3981111.5000112.9264780.0030340.0000351.2299750.0694303.3041811.7961340.0021890.0003280.0005081.8511880.0017122.1159980.0023660.0061010.0140200.0008930.0042310.0125670.0022830.0000180.0009450.0016070.0042940.0096550.0002910.0000170.0000180.0000320.0000180.0067820.0056620.0590800.0091040.0000290.0007320.0000160.0000160.0045760.0000240.0000250.0000190.0000180.0003200.0000180.0000190.0004790.0000190.0003260.0003280.0000170.0000190.0000180.0000190.0000210.5848920.0005360.0000200.0000210.0000230.0004960.0008100.0003540.0011960.0003720.0005042.3443500.0024490.0005090.0000381.0399921.9234302.0840380.0021600.0002031.8924581.6659420.0026360.0002161.9820840.0144280.0000270.0000150.0000200.0000191.0411710.0009130.4151110.0013310.0004670.0010460.0000180.4118330.0004680.4098930.0003701.2820590.0011140.0006840.0000190.0000590.0000150.0004650.0000200.0000160.0000200.0023790.0000290.0000260.0000250.0000391.4736540.0012580.0015100.0000270.0000280.0000310.0018670.0021540.0012350.0008420.0000140.0000140.0000150.0000140.0001970.0002020.0000150.0000140.0000160.0002540.0002110.0000140.0010030.0013350.0007950.0004020.0023040.0004130.0010090.0014790.0015350.0000220.0045770.0042770.0000230.0002310.0000190.0000170.0000160.0000910.0000200.0051370.0000210.0000170.0007970.0055300.0061740.0002000.0000140.0000140.0000130.0002630.0000170.0000140.0000140.0001860.0000130.0000140.0000150.0000130.0000140.0001950.0000140.0000130.0002080.0000190.0000140.0002110.0000140.0000130.0000140.0000180.0016620.0000190.0000180.0014390.0000210.0021250.0000190.0000542.5975590.0026730.0007130.0010030.0006611.8436384.7740463.0680312.9950311.4564800.1295750.0008870.0032980.0010710.0514476.2050956.0503870.0053960.0012880.0017130.0018240.0009930.0007590.0007400.0036990.0049300.0002020.0003750.0004090.0003760.0004820.0003750.0001780.0004410.0007630.2800480.8548832.2625710.0069580.0026540.0000160.0000160.0000140.0000161.9987930.0015260.0000150.0002180.0004190.0000141.3593060.0103340.0010740.6260520.7327251.4769940.0042520.0072780.0000240.0201140.0057780.0002300.0000320.0002220.1031560.0000910.4803400.0003720.0000201.0034770.0007720.0000150.0002401.0034770.0007570.0000150.0000160.0000150.0000150.2756641.3426070.1853290.2700890.8691080.0006630.0000280.0000240.0002420.0001170.0000230.0622600.0000740.0000290.0000220.0000150.0000310.0000390.0000180.0000220.0000230.0000420.0000150.0000380.0000250.0000200.0002220.0002220.0000200.0000210.0000350.0002150.0000170.0000240.0000310.0000200.0003380.0003610.0007640.0006540.0002440.0014010.0002531.3747823.0282650.0025510.0002070.5455101.3753050.0050923.6154842.1269215.9425184.3178435.7059594.6753940.0035960.0004350.0003680.0005300.0000180.0059950.0000230.0081470.0000261.0074440.0070880.0000240.0104760.0007160.0003630.0005450.0005100.0003870.0005400.0005960.0005660.0004080.0005390.0007650.0001820.0005380.0008430.0072220.0040751.2017180.3795260.1047590.0030501.6298871.2925290.0112400.0005510.0030510.0106350.0060470.0173242.0098780.0347900.0000453.2293823.6819141.8759641.1349621.8862841.8899881.8656113.2096764.7353030.0046310.0007310.0007650.0009370.0007210.0005290.0000150.0002620.0004780.0000140.0002170.0000150.0000140.0003850.0017430.0000180.0001010.0002410.0000140.0028540.0002670.0000360.0002400.0000140.0000150.0000140.0000211.9699620.2723310.0008783.9574223.7053773.9091082.6311800.2654273.9213463.8058633.7047704.4084032.0437100.0063000.0024220.0010510.0003660.0160430.0037220.0001840.0044790.0015010.0003891.9367383.7732100.0031390.0005570.0005890.0007040.0008120.0006140.0007820.0003890.0025370.0688700.0024510.0011450.0011780.0000140.0011310.0005730.0000140.0000130.0000140.0000140.0000130.0000130.0000140.0000150.0002681.4908170.0009802.2319570.0014581.6489390.0010790.0001940.0010740.0010550.0038950.1483150.0005200.0038962.0303897.5992767.3074895.7466250.0037050.0000340.0004320.0004720.0002290.0000220.0000220.0002240.0002800.0003750.0004960.0003630.0004930.0003870.0004920.0004070.0001880.0003530.0012860.0003730.0005290.0012590.0003311.6872111.2903981.2628180.0012640.0001971.9974470.0016520.0001822.1474270.0017222.1693374.0302630.0029481.9587081.8306770.0013451.9440230.0014065.0925060.2092040.4683050.2193100.0003010.2945450.5793790.0006190.0006810.0000140.0005100.0005140.0010560.0000211.0883640.0016920.0011600.0000200.0001810.0001850.0001850.0000132.0086860.0012522.9504872.0419350.0012702.0071651.7122368.2679656.5221718.3579796.4672266.1908510.0047180.0023790.0002620.0000410.0003130.0000140.0006000.0000140.0002520.0000140.0000150.0000161.1886180.0007390.0000232.8849750.0017660.0000170.0000160.0000170.0000180.0583856.9005122.6137031.7354110.0033081.4177360.0008720.0010720.0000490.0000192.0331950.5459870.9106870.0346090.0027270.0154710.0012930.0121910.1487740.0070660.0000210.0000170.0036011.1146170.0006831.0459310.0006390.0000290.0003151.8759420.0011460.0000310.0000300.0000360.0000350.0000330.0002860.0002890.0011190.0010530.0427610.0003860.0008910.0003360.0013360.0007720.0003380.0003440.0040770.0000170.0010990.0001770.0005111.2532450.0009922.7984950.0028361.8149891.4220390.0008561.9775551.5537811.7424780.0013630.0000170.0000160.0000150.0006140.0000150.0004000.0003650.0004490.0003650.0005210.0005770.0003430.0005940.0002010.0003770.0001810.0002110.0000150.0002690.0000150.0000140.0004420.0004160.0002230.0002320.0004440.0002400.0002270.0002190.0000140.0004470.0002310.0004630.0000150.0000210.0004370.0004710.0000140.0004730.0000150.0000160.0000140.0000290.0000150.0000150.0006750.0002440.0000140.0008520.0000150.0002380.0000170.0000150.0002300.0000150.0000140.0004050.0005080.0002170.0002233.6697830.0020810.0000200.0000260.0000210.0000210.0000220.0000250.0000220.0002240.0000190.0004420.0000190.0000270.0000220.0000210.0002350.0002410.0000290.0002330.0000190.0000220.0000210.0013040.0020820.0007380.0034600.0011680.0006870.0005691.2700462.0017822.6359323.0992934.5176950.0029150.0004230.0004120.0008340.0002670.0004730.0003830.0004350.0005940.0003960.0005490.0006330.0006390.0000620.0005620.0005500.0004280.0003680.0001980.0003590.0011310.0010170.0008490.0009210.0036610.0002510.0003110.0016850.0010500.0017800.0002510.0015460.8116324.2608223.8391752.7310360.3205171.9313391.8078470.0013420.0032900.3343910.0197300.0016701.2047460.0010051.4856110.0015581.4852370.0009890.0000140.0000130.0000130.0001900.0006200.0002840.0001950.0000140.0000140.0000140.0000140.0002290.0000140.0000130.0000140.0000130.0000140.0000130.0000140.0000130.0000140.0000140.0002110.0000141.8485530.0009920.0000140.0049830.0002780.0004040.0006450.0002090.0004150.0002510.0004320.0004220.0002160.0007770.0000140.0000170.0005500.0003761.6252860.0022161.6812620.0377580.2534600.0583640.0662400.0015900.0011650.0001910.0001850.0001810.0000130.0008980.0000140.0000140.0000140.0011480.0117020.0000210.0008310.0000150.0002010.0069141.0538200.8485030.0004530.4388310.0068220.0062380.0067020.0066910.0067900.0063700.0130980.0129600.0067180.0130500.0000210.1105570.0005470.0010260.0009255.0083362.0783163.0551686.0029442.5833750.0114870.0359161.6920222.8315301.5993780.0011250.0002612.2503522.7562480.0019020.4229340.0008952.7878682.2932121.2613952.4532382.1655120.0097001.3290741.2358940.0028770.0003630.0000190.0003680.0000180.0000190.0000160.0000170.0000160.0000160.0000190.0006281.5344000.0007890.0000190.0000250.0000150.0007310.0007261.6564210.0028230.0000190.0000160.0001971.2470483.1786510.0023121.8204911.7289632.9547990.0284790.0011740.0009090.0000200.0008690.0013610.0024401.4497710.0018100.0007850.0003070.0005620.0000150.0010480.0000150.0000140.0016190.0029150.0027590.0031440.0040870.0000160.0000140.0000140.0061051.4510380.3044591.3741651.3394391.3051570.4827672.2084210.3861893.2710660.6051040.0003130.0000201.9481741.7500890.0008723.8120751.8776970.0009360.0000201.9210490.0009540.0031240.0000210.0000160.0000230.0000510.0000170.1823910.0001080.0025540.0070900.0000210.0000210.0022280.0009530.0003680.0011060.0000240.0011820.0000193.3850293.2923920.0023173.4360872.0852920.0017423.1070713.3763821.4158770.0016890.0011650.0023310.0006550.0002021.6773951.9435190.0009463.9516330.0021100.0149040.0171200.0007903.3671670.0023680.0007552.3799080.0059580.3292350.0024690.0040980.0010870.0011001.7568220.0008570.0003562.4044950.0011690.0000240.0004630.0000250.0000250.0000280.2085310.0004510.9164961.5549190.0015010.0000230.0004490.0004980.0013890.0000241.3029060.0009450.0002320.0003380.0003740.0001750.0004460.0003410.0001970.0001980.0005190.0000140.0002401.2477183.0665961.1860385.6052571.5475320.1330750.0006770.0002010.0005610.0000140.6802711.5713330.0012870.0003633.2305390.0022930.0008130.0008080.0012780.0069101.0507990.0024520.0001960.0005000.0002940.0000260.0005800.0008721.6153600.0007610.0001910.0000130.0003410.0001740.0004070.0001720.0003420.0002860.0001750.0001880.0003340.0005220.0004040.0000140.0000140.0005610.0005740.0004070.0003920.0005030.0002400.0002110.0004350.0003100.0000150.0000140.0000180.3086170.0001560.0000150.0042310.0000180.0000180.0000160.0000150.0000160.0000150.0018620.0018130.0000190.0000160.2486220.0001270.0000150.0000150.0039040.0000170.0000150.0007670.0005760.0019230.0000170.0000160.0000180.0000160.0075790.0000200.3475520.0001740.3791850.0001860.0000160.0000160.0000160.0000160.0000160.0000230.0000140.0421080.0000322.1501670.0011540.0000140.0000140.0000130.0008010.8103800.0003770.0000141.5002170.0006820.0007710.0002380.0009090.0007310.0010230.0003680.0005040.0004490.0004460.0007840.0007081.9557273.9710088.0356545.8571611.1696160.0017670.0000160.0044740.0010990.0016720.0014310.0021360.0006850.0005660.0000150.0014540.0006550.0002000.0010530.0000160.0010070.0000160.0000170.0003530.0001901.6106680.0007290.0079070.0028830.0003780.0001870.0001820.0001900.9145040.0552960.3829220.0003501.9106990.0011911.2621031.4134630.1093710.0000770.0005200.0011670.0000230.0002210.0006730.7340300.0003330.0003241.8508730.0008160.0003670.0000141.7145550.0007551.7897030.0007880.0000150.0001450.0003840.0584350.0002620.0002020.2529200.0001250.0000160.3166090.0001510.0003560.0002660.0008970.1412200.0006570.4071380.0006780.2140910.0002780.0002830.0030700.0000160.0143460.0000240.0006730.0000140.0009530.0003001.3549860.0025562.3397310.0245530.0000244.9410240.0030330.0076280.0155060.0077150.9150201.6038570.0164320.0071920.0076731.2372610.9290370.5212530.7796350.0003490.0117030.1358620.7571710.9199400.0075530.0072612.3591390.0111860.0327770.0014090.0002310.0003970.0003950.0003831.9856560.0010240.0002190.0001970.0003930.0010470.0005540.0022680.0007940.0008430.0002170.0014250.0006570.0006440.0003670.0002860.0002750.0000350.0000370.0002390.0004920.0002650.0000350.0005190.0000340.0005090.0002790.000035

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/
With following parameter combination: {'batch_size_seq_length': 1, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_61c7ce4e_2020-02-05_09-07-28do9tnvk7/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    508    |    50     |    145    |     4     |
-------------------------------------------------------------
| disagree  |    20     |    49     |    15     |     1     |
-------------------------------------------------------------
|  discuss  |    224    |    60     |   1600    |    41     |
-------------------------------------------------------------
| unrelated |    10     |     3     |    40     |   6852    |
-------------------------------------------------------------
Score: 7170.5 out of 7516.5	(95.39679372048161%)
Accuracy: 0.9362918312201206
F1 overall: 0.7350690348913407
F1 per class: [0.6916269571136828, 0.3967611336032389, 0.8590604026845637, 0.9928276461638774]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<6:49:35,  2.55s/it]  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                       | 501/9622 [00:02<4:31:49,  1.79s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<2:39:01,  1.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2484.42it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 602
  Number of epochs: 3
  Batch size: 16
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=602.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0008810.0009440.0008250.0003050.0001060.0003310.2977200.2608390.0326270.0039190.0007350.0194170.1187290.0091750.0021420.0007030.2167420.0131020.0007540.0000910.0002540.0002080.0000490.0008440.0029720.0008680.0000540.2704190.0098090.4444550.0153740.0008300.0000530.0029620.0003450.0102880.0012040.2175460.2456130.0063180.2590090.2445940.7757941.1581970.0278230.5273601.2927530.8791850.0222740.0004810.1083990.0021610.1465350.0027890.0000770.0001660.0004910.1130210.2216710.0040930.0024200.0000630.4045030.0068170.0055090.0040210.3519110.0052730.6179090.0101520.0001660.0000230.2433170.2434950.2330330.4468861.4305690.0193580.0002670.0015780.0052610.0002420.2117800.2425300.0032990.1934850.0022720.0000470.0000260.0000290.0000790.0001500.0000260.0621610.0006880.0001400.0000340.0007830.0000290.0007380.0000280.0000220.0000570.0029860.0396890.3035450.3313290.0031210.0003300.0000330.3830750.0240590.4529120.2658930.0023530.0000420.2673770.4360320.0047530.0006630.2067900.8135211.1882190.0113360.0004950.0006520.0000360.0013830.0009890.0009860.0011230.0876570.2167590.4448171.1960020.0092860.8540580.2829380.0047470.4036620.0207100.0005510.0001480.0011240.9606360.0120990.2829930.0024140.0162050.4446220.0088110.0072200.6145521.4402970.0097361.1287460.0074570.4102350.0027130.0021520.6122410.0101030.0029680.0005970.0015100.0076690.0055430.0032160.0003330.0018530.0000400.2863010.0016960.0002720.0008550.0005710.0047350.0014370.0008980.3806940.3132140.0021802.5255810.9321470.2944060.7621200.0045610.0012940.4874970.9107791.2865820.0086270.0009830.0016820.0139210.0006690.0012170.0014680.0019670.0102280.8957440.1219510.0730320.0026510.0019080.7248030.0696910.0043370.0032980.0202540.0040030.3777940.0029620.8344010.2531060.0011980.0055250.0000460.0000210.0000200.0138290.1539080.0019790.5509060.0031200.0021080.0013582.2308500.0103430.0016771.4765061.2936841.0212080.4769360.6992760.3077220.0017020.0000270.0008280.6822610.3831330.6103290.2895660.0356610.0060280.5352710.0023790.0000280.3030700.7400620.0029820.0000390.0034510.0034070.0000420.1357410.3332320.0128460.2932490.3232480.0028470.2879360.0586070.5634460.0110460.0055550.0060790.0019360.0058550.9503861.6467360.7833960.0249630.0017890.3303020.0062510.0597510.3104150.0011361.2853350.0047940.3010960.0019180.0026490.0000340.0000450.1106660.2021700.4645751.4756620.3076070.0013390.7411000.3789210.2369670.2377110.0014020.0000350.0012280.0017250.0018740.0042620.0040090.0019141.5012301.8594120.0077840.0010800.0071170.0010320.2948910.0011360.0002680.0001800.0002910.0002370.0002270.0052890.0156260.7045361.9013802.6341000.4258600.0153510.0032530.0036310.0020800.0109880.4583790.0020120.2698450.0024160.3901310.0105690.0032020.0009470.0003230.0003240.0000430.0063730.0009800.0011700.0000250.0001920.0000250.5429460.0020690.0000260.0342690.0120530.0023580.2969543.2584340.0146510.0508930.0047790.0027330.0014080.0014180.0007070.2961240.4332130.2082540.1426330.0058890.0278810.2554830.0019390.0016650.1724000.0010770.2556800.0007100.9346211.7590752.4337891.2808001.0869570.8085430.5921530.0032230.0636960.0544400.0004230.0000280.4295260.8164130.0047550.0152540.0126100.0034330.0011040.0018280.0010890.0025690.0086270.4674310.4626901.8335380.8811190.8665090.0716170.0038170.4125400.0020120.3513351.4151435.1057362.6301370.0074630.0005510.0045240.5770320.0016163.5770900.3325081.1250530.7273410.6503560.6890270.3649490.0030420.0005030.2868250.0019040.0013770.0010391.1219491.4992470.7524660.0021160.0009510.0013420.0005440.0001300.0006730.0005580.0006290.0006630.0002030.0002900.0008010.0001460.0004170.0005780.0002450.0000220.0027700.0009420.0014810.0011830.0562830.2199830.3887920.0036030.0027670.0036670.0036740.0021560.0033240.1967020.0037320.0418950.0026890.0021150.0030760.0002340.0008330.0005660.0000270.0002070.0002910.7740080.0065030.0012190.0009010.3534602.4505170.3272450.0015450.0003670.0018140.1187330.0207250.2901600.0225830.0069250.7214781.1467320.0106470.0026890.0037410.0039340.0043210.0010830.0000240.0061280.0021130.3276440.0062890.0088460.0829120.6378430.0826800.0004760.0016140.0005041.2711410.7428961.1984121.2960611.4390440.4570790.4557580.3380160.0020670.0011110.2946550.1369040.1607920.2473200.1474810.5092630.5427470.1912210.0061040.0017870.0174450.0250460.0034450.1741580.0326530.5114260.0097800.4779470.0025300.0027560.3165280.2317420.0038080.1765900.0010110.0014550.0018900.0011480.0016520.0008590.0001700.0771630.0001630.3104040.1941490.0038030.5969710.0020410.6803950.0012540.0000320.0029190.0003160.0930080.0011170.0028740.0013032.8073480.0081630.0026050.0047340.0014500.0701120.0326380.1495160.4590790.0024890.4359890.0038390.4385570.0010590.0003290.0060480.0006370.0009960.0013380.0008720.0023420.0027780.0307940.3507260.0017950.0015240.0022550.0043680.5932140.2069940.6116070.4118880.0041620.0021150.0008820.0012440.0011350.000638

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/
With following parameter combination: {'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5cab1326_2020-02-05_08-27-56bp0_l5ze/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    581    |    36     |    114    |     9     |
-------------------------------------------------------------
| disagree  |    25     |    80     |    46     |     0     |
-------------------------------------------------------------
|  discuss  |    152    |    41     |   1626    |    31     |
-------------------------------------------------------------
| unrelated |     4     |     5     |    14     |   6858    |
-------------------------------------------------------------
Score: 7243.5 out of 7516.5	(96.36799042107364%)
Accuracy: 0.9504261068384952
F1 overall: 0.7928009977039902
F1 per class: [0.7736351531291611, 0.5111821086261981, 0.890958904109589, 0.9954278249510125]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<6:58:08,  2.61s/it]  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                       | 501/9622 [00:02<4:37:29,  1.83s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<2:42:18,  1.28s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:08:53,  1.12it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<22:08,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2566.48it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 602
  Number of epochs: 3
  Batch size: 16
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=602.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0002170.0002680.0001930.0001220.0000820.0002070.4809470.5791040.0724320.0087560.0010650.0003110.5090130.0392000.0028590.0003320.0000850.0000520.0000560.0000490.0006670.0000930.0000690.0003010.0005200.0000890.0000430.0627080.0022860.0001350.0000960.0000770.0000780.0134480.2994440.6920080.0257740.8686280.8764920.0225140.0007250.0001290.1988400.3571250.0091610.5638521.7524550.0448630.0030870.0001020.0006770.0000570.0001650.0000440.0000450.0000570.0000960.0252590.0008590.0000760.1044890.0017510.5557480.0089250.5771910.0146260.9789550.0146510.4535980.0067690.0001340.0000390.0172030.3191790.3678920.8517930.2811770.0037040.0000860.0090010.0352140.0005010.0008890.0062030.0001730.3153390.0037030.0000830.0000410.0000390.0000390.0000540.0000890.0000410.0000390.0000510.0000540.0001370.0000400.0002960.0000390.0000500.0000460.0008520.0008220.1944810.3253620.0041740.0001250.0002010.0000640.0128710.5952600.0053520.0000830.0000370.0006580.0002380.0005570.0000660.0008701.5022701.1766850.0098670.0001430.0001040.0000390.0000990.0043970.0204230.0004470.4325710.7414740.4189271.4738610.0114250.8312710.3617730.4546030.3211770.0026590.0000970.0000840.0000691.0591190.4438180.4567530.0031830.0001450.5473570.0081680.0009270.7344610.0078290.0001280.4534630.0029830.0893500.0006320.0054900.1358470.0011680.0162030.0002200.0002170.0001480.0001100.0001040.0000550.0002860.0000610.0809670.0008820.0000830.0002450.0000650.0000910.0002320.0001300.0148420.2588130.0015242.1364411.6663890.3040470.9125070.0103250.0003500.4512400.8552251.2569450.0069500.0002210.0002480.0000590.0002000.0001270.7245780.2298560.1358810.3916830.0025740.0004640.0046310.0018530.3363910.0031510.0005660.0171640.9587890.3711120.4239590.0023990.0001510.0000610.0000410.1556310.0007570.0000440.0000380.0417500.0018370.0001320.8965020.0042510.0005820.0167322.3444180.0103850.0001991.2851861.5286121.3379600.4593560.3539980.1444400.0006870.0000460.0000880.6763460.3280010.0057240.0005430.4623161.3057811.4658690.0060180.0000670.0002270.0005020.0000410.0000390.0004610.0038050.0000550.1934240.1109160.0008200.3754220.6536220.0032730.0023250.0010990.5421000.0029840.0012720.0008890.0046080.0002660.0034780.0015150.5047690.0856510.0007420.0020720.9572020.8889460.4267650.0015701.4423560.0052100.1839490.0007920.0003490.0000360.4280560.0020560.3970070.0017011.5696530.3032440.0010960.9261150.3427200.1719990.2122220.0018160.0000660.0004990.0007080.0004160.0008170.0003930.0051641.6002862.3252200.0079050.0003100.0000430.0006170.2569650.0008790.0001120.0000680.0000800.0000720.0000500.0001220.0002300.8000252.1665372.2997230.4058810.0025850.0115930.0003130.0001940.0002590.2688480.0024920.0029540.8615750.6313030.0026390.0005530.0005180.0003600.0003750.0000410.0035890.0004670.0003650.0000390.0001300.0000430.0003130.0001950.0000380.3698940.0023540.0001953.0929635.6047550.0163650.0003050.0008680.0006540.0001800.0002030.0001180.5404920.5502900.0785100.0332280.0005700.0005610.2503990.0008510.0007760.0001900.0003070.0044370.0000491.1167822.0069683.2555370.5096940.2579500.6440450.5652270.0017040.3253530.0061420.0000930.0000420.3404840.7433500.0033080.4323933.8967430.0127150.0004580.0001900.0001210.0005610.0024090.5238060.5656642.2392841.0842081.1159520.4619940.0023410.0027850.0001800.4920781.9688912.5523951.2194680.0030500.0000790.0930080.3117330.0008071.2988490.5404690.8783910.5474340.1472960.0014490.0004950.0007810.0001210.3462720.7876580.8192950.1775701.5898751.6023220.8009800.0019490.0001460.0001920.0000980.0001270.0001180.0001010.0000990.0000980.0002430.0000450.0001450.0000990.0000810.0001340.0000450.0000420.0002980.0004210.0002360.0003510.0018740.4989910.0032680.0005810.0007780.0007000.0004710.0006110.0018470.3530880.0028900.0074260.0105690.0002640.0002450.0000530.0001390.0000830.0000390.0000380.0000530.7089470.0019900.0001410.3678780.1720632.4983170.3492570.0008200.0000690.0007420.0034490.0016520.0047750.0019790.0012381.0455441.5147730.0053860.0004240.0005640.0006740.0152810.0217920.0000910.2059620.1776680.4093380.0048930.0010300.4997641.8277850.4872640.0014380.0048410.0016891.4002540.5862331.3640181.0633891.6514450.5459990.1953220.0089350.0003520.0002860.4186230.0029050.1235800.3612420.2072020.3673480.6000020.0419360.0056960.0003930.0017050.0021290.0022400.0105200.0004280.0019320.0009100.9064580.0018620.0020520.0021010.0115350.0003160.4299270.0008820.0001310.0032820.0001200.0001810.0001500.0001140.3348220.0007010.2087790.4032040.0009700.0479910.0002760.0002880.0000400.0000390.0005200.0000740.2477970.0005580.0002170.0001670.1124320.0012630.0019130.0003380.0006460.0005470.0013290.0091420.0186980.0418190.4635020.0881990.5917430.0012960.0000710.0201030.0398360.0008551.1156330.0127380.1317260.3137621.0498210.0179540.0020950.0018040.3334620.0142860.4573130.2385410.0423360.0003790.0003380.0002160.0005080.0004410.0004580.000311

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/
With following parameter combination: {'batch_size_seq_length': 3, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_711dee64_2020-02-05_16-31-16su_iwvgw/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    606    |    26     |    97     |     3     |
-------------------------------------------------------------
| disagree  |    15     |    88     |    50     |     4     |
-------------------------------------------------------------
|  discuss  |    135    |    46     |   1630    |    24     |
-------------------------------------------------------------
| unrelated |     6     |     2     |    23     |   6867    |
-------------------------------------------------------------
Score: 7263.5 out of 7516.5	(96.63407170890707%)
Accuracy: 0.955206817709416
F1 overall: 0.813827843805033
F1 per class: [0.8112449799196787, 0.5517241379310345, 0.8968363136176066, 0.9955059437518121]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'constant'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<6:40:22,  2.50s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<3:42:00,  1.75s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:34:14,  1.22s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<30:16,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2667.11it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 301
  Number of epochs: 3
  Batch size: 32
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=301.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0012120.0015400.0011600.2834150.0711470.0163540.0822750.0129550.0017800.0003360.0003430.0008600.0027330.0018350.0004920.0004580.2365790.4574050.1791140.3133800.0159920.8063390.2429240.8109940.0648030.0324640.0014980.0002890.0582180.0025180.0003530.1880710.2661460.5571900.1976320.0057940.2148920.4722710.0188260.0013540.0063410.1229670.0071800.0003060.0001630.0003120.0001600.0002190.0006150.0010710.0001710.0021340.1925530.1771200.0044160.2322970.2581810.0046340.0034850.0019200.4136270.2978510.0050830.0003320.1743010.1084440.5865860.4711640.3550170.0524420.0045140.0004070.0117660.1886970.2109310.0223990.6305980.2672940.1656170.1106650.0220850.0022700.0010740.0012370.1689460.0097590.0003130.0014870.0008340.2328760.1401782.0543390.7694470.0155590.6475430.5530090.0109000.0003920.0005690.0019410.1826490.0088080.0507360.0066740.2513780.3344160.0038070.0001720.0014510.0001190.4086390.2832160.0066500.8956190.0082980.0077070.6253470.1811940.0037120.4515460.3392040.3742982.4959520.0205520.0018740.0001960.0034810.1327850.2161540.2942270.0096650.2165690.0104010.0168150.0784900.6156190.0074930.3030430.3654900.7075990.1433670.0043980.0008610.1680140.8957440.5003190.5205110.2309100.0018990.0037590.0078670.0004221.2966080.0211140.2265760.0084220.0007280.0005570.0007970.7289211.8320870.1104310.0478020.0245390.2170970.0057790.0871840.0042790.0009040.3863750.0816940.0006370.0212740.0007810.0273770.3460790.6243770.0075650.0133140.0009300.3564110.7237350.0075980.1609910.0206110.0313500.0005700.0011420.0008380.4010300.0234460.1637780.1038430.3318081.4519350.0106530.0008230.0077440.4368521.2495130.8320060.0377150.2117462.9671660.9803270.1161880.2542841.2915131.2303550.8237900.1237980.0101350.0008080.4267560.8436770.0043580.0005230.0005340.0006820.0005960.0008090.0012680.0005960.0013660.0014210.5367450.2006770.0045740.0041020.0028400.1225640.0062890.0024890.0003690.0005400.0002470.0271000.0905171.5617200.2880780.0034940.0647650.0018850.4081940.6107220.0089720.0050870.0007330.1063770.1749440.0118480.2120930.1429820.1357760.7831141.2492990.5207280.0063870.3442920.3042230.4484050.5606270.2561360.0871760.6234260.0074650.0043130.5196700.0122400.0045890.2010600.0013160.0005860.0008220.0012690.0028480.0116650.0022100.0001450.0019190.0377210.0009562.2030070.0427360.2315310.1910980.4378810.0020790.0473550.0010820.0005450.0008150.0030850.4934580.0256010.1118830.4148270.0788140.0043250.0034130.003587

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/
With following parameter combination: {'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'constant'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_5f4a402a_2020-02-05_08-42-29ic3wddzo/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    564    |    34     |    92     |     3     |
-------------------------------------------------------------
| disagree  |     5     |    68     |    15     |     3     |
-------------------------------------------------------------
|  discuss  |    189    |    55     |   1656    |    18     |
-------------------------------------------------------------
| unrelated |     4     |     5     |    37     |   6874    |
-------------------------------------------------------------
Score: 7259.0 out of 7516.5	(96.57420341914455%)
Accuracy: 0.952192891290792
F1 overall: 0.7996356972824558
F1 per class: [0.7752577319587629, 0.5375494071146245, 0.8908015061861215, 0.9949341438703141]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:03:34,  2.64s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<3:54:52,  1.85s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:39:41,  1.29s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<32:01,  1.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2498.17it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 301
  Number of epochs: 3
  Batch size: 32
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=301.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0004670.0005910.0004620.3365180.0843400.0204860.0167750.0030710.0006930.0001970.0003910.0004500.0100270.2032110.0146830.0011680.1815880.0110800.0009020.4157220.0212370.1354560.2302440.6661640.0312730.0015090.0003400.0001650.0009140.1045060.0802890.1998780.1492020.3682800.2436030.0070880.1602740.4361490.0488900.0016530.0025370.0556400.0101060.0003600.0001310.0001900.0001300.0001650.0010450.0003450.0001130.0009820.0312500.0806510.1037840.1233020.2433000.0043600.3151610.2081050.4998110.4009970.0079300.0008280.1268600.0282890.3407000.2712900.5819510.6868660.0110530.0004210.3494090.0232330.2739720.0066380.1166480.3111280.1825120.0025150.0358850.0025630.0014370.0006740.0009170.0003640.0001310.0007070.0008000.1029850.0025242.8243570.5634420.0075120.5291530.5975520.0071120.0026480.0008340.0024580.2304380.0166270.7498890.0108900.0030120.1149090.0022980.0001840.0571930.0006160.0769390.3215330.0043771.0140600.0100620.5011420.8070590.2266750.0021080.3373090.2477700.3548602.2710200.0186160.3485090.0029650.0012290.2887120.2525620.4670700.0144300.1001310.0087090.0014340.2676920.6461040.0060650.2430300.3184490.3878050.0114080.0020220.0588490.0051890.3870470.1973550.4717250.3476460.0026920.0015990.0073050.0008701.9285100.0137450.0006080.1505060.0012340.0004300.0003920.5080102.0534480.0575820.0023450.1924450.1777490.2289770.2437950.0210480.0004030.0288750.0026960.0001180.0007590.0003440.0411820.8194341.1402490.0139440.0059710.0274170.4963540.4388490.0691640.2002640.0023900.0020880.4051711.8338710.1598420.4832280.0153820.0009550.1577070.3462631.3483350.0086100.0018370.0043270.1959770.4400610.3843830.0041620.2110882.7794901.2657420.0818040.4515521.0696120.8695090.7732670.0052010.0053110.0375310.5075860.9911140.0051000.0006360.0004830.0007000.0004760.0010810.0004110.0016180.0006120.0005740.0151470.3876400.0087130.0017250.0069040.3351000.0048900.0031780.0005060.0002420.0004470.1519420.0015961.3753830.0970360.0018070.0088650.0454280.4192370.5851290.0040540.0102290.0003510.1223460.3061540.1071600.6172720.0031980.1829030.7045780.9459950.3086200.0178170.0046920.0939920.2568670.9866350.0163070.0016500.0069950.0180420.0202170.3392390.0240900.0097500.1304590.0011930.0011250.0003710.0026970.1169360.0101150.0731600.0004090.0007340.1246930.0022470.6317910.0035790.0068250.0127300.1334080.0118610.1730170.0010880.0004390.0088410.0066100.5527560.0334360.0804270.0693240.0313030.0011790.0012210.001341

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/
With following parameter combination: {'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_7394721c_2020-02-05_17-10-51iyg6hm30/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    600    |    36     |    88     |     4     |
-------------------------------------------------------------
| disagree  |     7     |    79     |    44     |     1     |
-------------------------------------------------------------
|  discuss  |    154    |    46     |   1643    |    28     |
-------------------------------------------------------------
| unrelated |     1     |     1     |    25     |   6865    |
-------------------------------------------------------------
Score: 7264.5 out of 7516.5	(96.64737577329875%)
Accuracy: 0.9547911037206402
F1 overall: 0.8088478099333751
F1 per class: [0.8053691275167785, 0.5392491467576792, 0.8951239444293109, 0.9956490210297317]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'cosine'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:03<8:50:55,  3.31s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<4:54:23,  2.32s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<2:04:57,  1.62s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 6501/9622 [00:05<59:05,  1.14s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 7190/9622 [00:05<32:14,  1.26it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 7719/9622 [00:06<17:40,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:06<00:00, 1473.82it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 1203
  Number of epochs: 3
  Batch size: 8
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=1203.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0003630.0005970.0008460.0003270.0001100.0000450.0007380.0001470.0000400.0000480.0000730.0001470.5562230.0428020.8928100.0595370.0037380.0002360.0001130.0000210.0005560.0000430.0003630.0003480.0004251.8559610.0714000.0026620.0004010.0001880.0020460.0000820.0000200.0000200.0000250.0000160.0000160.0000380.0000170.0000160.0000170.0000170.0000190.0000560.0000290.0000640.6800290.0150000.0042210.0001890.0000300.0006140.0000640.0000430.7775670.0141660.0002690.0024290.0002560.0000210.0000990.0000170.0000620.0000160.0000170.0000170.0005230.0005920.0000250.0000340.0002770.0000670.7943190.0108980.0001650.0000211.6383520.8318720.0106820.0001520.0001160.0000850.0000960.0000200.0000160.6475930.8561770.3929610.8515580.0098940.0001250.9780930.5673123.0858360.0328631.1175480.0651820.0010640.0000270.0000160.0002970.0000190.0000170.0000170.0000160.0000880.0000170.0000160.0000160.0000160.0000160.0000600.0001190.0000830.0000170.0003960.0001290.0000650.0000180.0000660.0000160.0001170.0000160.0000160.0004040.0000290.0001720.0000210.8821650.0068950.0046200.0000510.4719050.0083660.0000780.0000160.0013780.0591060.0016720.0000280.0000160.0000160.0000160.0000160.2435580.3654360.0025190.5672440.4889680.0032990.6573860.7542240.0625050.2258040.0015240.0000250.0000150.0000150.0000150.0026230.0016481.1510640.0071640.0000600.0006871.0350900.0076370.0002160.0000990.0001230.0005620.0000190.0000160.0000160.0000160.0000160.0000160.0000160.0000150.0000160.0000160.0000170.0000160.0001660.0000170.0000160.0000160.0000160.0000160.0000160.0000160.0000710.0047150.0000430.0017290.0011940.0000240.0000160.0008070.0000190.6127110.0059880.0000450.0000170.0000160.0018290.5284890.0613390.0025770.3496200.3516470.8163540.7101440.0034680.0000310.0000160.0002620.0000160.0000160.0000150.0000300.0000150.0026390.0006440.0005981.0156160.0045910.0007150.0000190.0000160.0000160.0000160.0002830.6457570.0035670.0004700.6001640.0038360.0001250.0000280.0000160.3129590.1511750.0006680.0011010.0005580.0103510.0000590.0009980.0000200.0003760.0002190.0000170.0000160.0011680.0000900.0023300.0351740.0078190.0225470.0001810.0071220.6652050.0025851.3158430.0052500.0001720.7950570.8133492.0348390.0075570.0000920.7380291.6591590.0061200.0003170.0001700.0000360.1687810.0006920.0003160.0008260.0001360.0000600.0000160.0000600.0000600.0000620.0050920.0487180.7967420.0037390.5705980.0095830.0001320.0000920.0001050.0002830.0000220.9598900.7347643.3433690.0147070.0016850.0004572.3732380.0078621.9351400.0063820.0000820.7100210.7476470.0024560.0000680.7078560.0023050.0000630.0006630.0000590.0000652.4940960.4224900.0017070.0006690.0013570.0002910.0001350.0001450.0002281.1493190.0039450.0009460.0006760.0008460.0005290.0006750.0000180.0000600.0248670.0008640.0000190.0000150.0000160.0008120.0000240.0000160.0000150.0000180.0004310.0005770.0002220.0001180.0002190.0010330.0004140.0002640.0001230.0000620.3354540.0009511.4387880.0040060.0000720.7406350.0639520.5990042.6011151.8991280.0052570.5930000.0020142.5751600.0103400.4015340.8640910.0454310.0024520.9790441.7204710.0045592.4917110.0065600.2695450.0007210.0000980.0003610.0003560.0003760.0012470.0000190.0001030.0000160.0001110.0007810.0001420.0039850.1093230.0004170.0014881.2960330.8042950.4217030.0031760.6286681.2022050.0044770.0181260.0320270.0049120.0000970.0040104.0861450.0102320.0047280.0005460.0004100.0002070.0004110.0004900.0014980.0007100.0000170.0000510.0003800.0002120.0005180.0000170.0004810.0001450.0000160.0000150.0000150.0007530.0000190.0000160.0000160.0000160.0000150.0000160.0000160.0005002.2436310.6186280.0071770.0003520.0000160.6091120.4959690.0016280.0018180.0021410.0005480.0014500.0061230.0014730.0010990.0000190.0001630.0004610.0005950.7302242.2810571.6833591.5281021.6588250.8863090.0020221.5248960.7418080.2224670.0004890.0004060.0000160.0000700.0000160.0000150.0000160.0000980.9983251.0705500.0023481.1381990.0035410.0002420.0000160.0002790.0000161.3847792.7556462.8900463.9721220.0117130.0000820.0000170.0000170.0000190.6673460.0013591.1182920.6654090.0013480.0000210.1762170.0003670.0007510.0000190.0008681.0008150.0019870.0000260.0000211.0780410.3713740.0007390.0399580.0004600.0003330.6386341.0637220.0023230.0003820.0003290.0007290.0003760.7697400.0021670.2739600.0005360.0036610.0017440.0053830.0044800.0017370.0000690.0015810.0015680.0002920.0007920.3243180.0051000.0129010.0047071.3733420.2677080.9436910.0093220.0005920.0001960.0019110.0004311.0247250.4383991.3992300.0027860.0004770.6588470.0012010.000019
1.0893101.1281430.0020810.0000210.5719480.0010320.0002320.0000160.0002720.0004360.0000160.0000150.8824480.0015610.0000190.9336060.0585100.0444811.0320240.0024551.4425390.1131890.0760580.0377530.0001280.0001200.0000171.5524650.5881540.0011890.0004930.5627920.4332700.5199770.0016150.0000220.0000200.0000160.0018140.0000180.4136300.0011330.0000170.0224940.1031970.0006680.0000170.0002790.0003010.0000220.0002892.6538053.7923010.0205040.0004720.0005540.0005630.0004570.0000190.0000160.0000180.0007900.0004660.6799840.0011550.0000180.0001250.0000170.0001950.0000180.0000160.0001360.0011590.0000180.0000700.0000160.0004850.0000160.0002930.0004630.9061560.0048560.1858253.9561942.9876022.3579770.1172260.7157231.2489290.0024390.0003960.0013530.9038670.2960730.0021390.0020160.0008450.8020680.7686431.2522540.0022610.0000200.0023800.0000190.0003631.4595800.8863070.3458140.0021570.0005530.1431800.0005960.0012520.0000670.0000210.0001230.0000590.0000840.0000160.0000160.0052830.4094490.5127100.0007690.0006170.0006960.0000180.0000150.0000540.0000160.0000160.0000160.0000150.0059680.0000240.0011980.0000170.0000200.0001950.0050590.0033450.0033050.4263460.0008540.0028651.3107064.2094195.3762260.0080080.0007030.0008210.0104800.0040680.0013730.0075340.0003470.0003320.0057070.0003750.0009850.0003260.0002890.7865350.1015180.0782730.9555870.0016772.1960632.0158052.1933283.1337511.5264021.5479842.4414660.5750720.0011660.0004130.0001920.6517250.2648660.0003740.6679560.0009180.5804401.2696570.0024860.0000190.0000170.9296081.6474943.6234521.2013923.4608913.5222480.8403320.1905360.0027910.0027080.6297160.6218500.9912720.0016930.0004700.0004670.0004871.2338380.0018050.2427970.0004480.0000170.0000160.0000160.0000600.0110441.0668400.0028560.0031990.0522760.4870532.5532516.2711482.5670340.0040750.0013110.0000200.0004950.0004100.0002370.0002380.0001530.0010570.0010000.0015830.4286720.3190161.0919130.0015451.1000263.3192401.0946181.1029361.0968983.5457021.3293760.7066990.0459570.0001950.0064270.0004660.3466340.0008460.0001080.0000660.8092392.9069160.8463920.9252680.7754900.1953370.0005250.0010180.0000890.0001020.0000550.0000170.5161470.5709170.0007080.0000170.0000577.2833951.2197610.6182360.0008160.0028561.3582621.0073841.2037602.1952990.6123460.0877490.5576970.3505340.0019080.3972930.0004870.0000160.0006170.6504760.6235280.0012140.0003360.1504620.0005320.0003240.0001950.5340200.8957290.8185301.0768241.7649510.0073920.0000250.0001560.0001810.0002140.0003160.0002580.0001390.0001000.0000700.0000180.0002370.0001130.0001580.0001090.0001080.4190460.0005220.0002020.0001210.0000180.0000200.0000170.0002530.0002340.0000610.0000260.0001930.0001250.0001671.0539740.0012000.0007770.0000170.0000170.0002190.0004470.0000170.0002090.0002090.0001700.0000160.0022520.8241810.9522742.3251421.0057991.6289150.0023480.0004930.0002770.0003200.0005930.0041590.0002320.0004110.0002360.0002020.0010430.0007200.6388940.6681730.0399620.0369590.1521770.0374080.0003840.0004190.0003320.0002310.0002770.0002530.0001170.0000160.0000600.0022200.0000790.0000170.0001050.0000160.0000170.0000260.0000160.0000160.0000621.9344340.0021360.0002090.0003050.0001650.0002920.0003570.0000730.0022700.7104742.1939832.4280870.6565590.0009150.0001190.0002720.0000160.0009600.0030270.0012010.0007970.0156440.0133730.0043730.0007020.0011410.0015560.0011460.0124720.0005971.9333101.0889213.5242700.5010480.1388730.0005450.0002620.0003310.0003540.0005400.0004470.0003590.0003180.0013770.0009780.0000160.0000170.0000160.0013940.0009720.0000160.0023290.6905610.6723320.0007390.0019320.0007110.0010130.8015100.6587191.6502613.4957911.6473040.0017350.0000640.0000170.0001910.0002110.0001970.0000180.0238220.0008431.5013713.7346172.0217490.6841590.6757930.8603572.2466480.0022150.7002350.1430870.0001630.0001400.0000280.0084250.0000240.4768620.0038360.0033820.1522210.1934540.0049020.0251570.0032430.0010680.0007350.5053430.6768661.0067840.4612921.9733340.0088251.4618850.0312671.2888600.0156490.0003500.0001960.0003240.0000180.0006701.2590820.0225520.0004700.4502800.4485440.0017060.0004830.0010620.0004710.0004530.0002530.0024340.8386380.0036460.4799790.0007610.1099140.0003840.0754300.0020720.0007730.6222640.0009470.0001320.0007990.8602070.0008510.0001550.0001520.0003550.0001300.0004070.0001330.0001660.0011060.0002890.0004230.0001590.0000160.0001330.0000150.0030470.0000180.0000160.0000160.3507760.0033490.0000180.0006250.0000160.0005280.0001460.0000160.0272700.0055520.0046010.0000200.0000360.0000160.0000160.0009420.0018390.0000170.0001250.0026790.3140070.0039970.0028030.0007620.0004760.0003960.0007072.1369405.5136110.0073930.0012860.0008550.0012080.0008150.2791160.0004110.4105230.0047940.0001230.5250650.1087530.0055950.0002490.0121040.3759180.2831220.0004811.1280520.0010520.0001260.0002970.7555991.0566090.0009920.0005720.0936110.0000970.0001490.0036120.0010910.0000190.0004010.0001040.0008440.0820990.0005630.0001100.0001930.0002330.2193720.0007500.5348860.2776521.8436340.0029740.0027820.2977130.0033410.4274330.5143540.7069270.6987600.0048160.0029400.6770280.0048960.0020800.0024700.0004040.0001060.0002310.0003310.0002590.0003150.0003260.0002050.0001530.0001760.0003290.0002350.0002330.000169

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/
With following parameter combination: {'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'cosine'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6ea7aeae_2020-02-05_15-05-585u1j_4ev/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    546    |    35     |    146    |     8     |
-------------------------------------------------------------
| disagree  |    25     |    81     |    47     |     5     |
-------------------------------------------------------------
|  discuss  |    186    |    43     |   1575    |    17     |
-------------------------------------------------------------
| unrelated |     5     |     3     |    32     |   6868    |
-------------------------------------------------------------
Score: 7200.5 out of 7516.5	(95.79591565223176%)
Accuracy: 0.9426314695489503
F1 overall: 0.7751410233727103
F1 per class: [0.7294589178356713, 0.50625, 0.8699254349627175, 0.9949297406924525]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:40:52,  2.87s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<4:15:33,  2.01s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 5001/9622 [00:03<1:48:28,  1.41s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<34:51,  1.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2539.87it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 1203
  Number of epochs: 3
  Batch size: 8
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=1203.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0003250.0003460.0001890.0000760.0000330.0000190.0000160.0000270.0000180.0000160.0000700.0001140.7394540.0568951.1440980.0762850.0047810.0002930.0001740.0000210.0001700.0000210.0005290.0002420.0002030.4826340.0185780.0007070.0012260.0000980.0001590.0000300.0000200.0000150.0000570.0000170.0000140.0000120.0000120.0000130.0000280.0000200.0000350.0000410.0006300.0000330.0000200.0002380.0009230.0000360.0000560.0000640.0000160.0000210.0073580.0001530.0000170.0013340.0034410.0000720.0000620.0000140.0000870.0000140.0000130.0000130.0002951.0493760.0154910.0004872.0968580.0295880.0006300.0000340.0019591.7209250.8629030.9451710.0121300.0001770.0002370.0000770.0000950.0000150.0000121.3782911.2078030.1696960.6596950.0075120.0000951.2513310.0136833.6311830.0387510.7269370.3033730.0033770.0000480.0000140.0001440.0000170.0000130.0000150.0000130.0000420.0000150.0000140.0000140.0000120.0000130.0000910.0002640.0002760.0000140.0121400.8781060.0075730.0000770.0000490.0000130.8262630.0067840.0000660.0000310.0000120.0001260.0000170.7631840.0059580.0004390.0000171.8817540.7469550.0055860.0000530.5132630.8939450.0073690.0000650.0000120.0000120.0000120.0000120.0077400.3523590.0024430.1704250.4455170.0030020.8886050.8654400.0073510.3636750.0024130.0000270.0000120.0000120.0000120.0028860.3050461.1245910.0070040.0000560.0001850.0315900.0116880.0002180.0001180.0001140.5275620.0030970.0000300.0000120.0000120.0000120.0000120.0000200.0000130.0000120.0000120.0000150.0000120.0000590.0000800.0000130.0000140.0000130.0000140.0000120.0000680.0000550.0065200.0000450.0000380.0006310.0000150.0000130.5557140.0028040.0000300.0000120.0000110.0000120.0000120.0021160.0002040.0004320.0005260.0001700.0006740.6332470.8718800.0041290.0000310.0000130.0002090.0000130.4820440.0022140.5222170.0023760.0011400.0003080.0001690.9951460.0045260.0001980.0000120.0000110.0000110.0000120.0001580.0003100.0002220.0002170.0005600.0001930.0001310.0000610.0000180.0037440.0012301.0361540.7647160.5181480.0027130.0000280.0001300.0000120.0001120.0000610.0000140.0000120.0001180.0000350.0003700.0028740.0408120.0057690.0000850.0040610.7618990.0030211.2183140.0048890.0000570.6742361.1556641.9157260.0071160.0002930.6788760.6498020.0024270.6231370.8482070.0033021.1766570.6446820.0027150.0000870.0000850.0000440.0000130.0001060.0000480.0000560.5928811.4325550.7780280.0026940.7100900.0213530.0002180.0000890.0000800.0003200.0000300.7592750.0034320.3385540.0020870.0057950.0003811.1563030.0038800.6374330.0021510.0000481.4572830.7686850.0025010.0000480.6038440.0019530.0000420.0000510.7029580.0023140.9935461.7078390.0056270.0003690.0009010.0002220.0001870.0001170.0002590.0022360.0001170.0001630.0001220.0001710.0000850.0002290.0000180.0000590.0005970.0004030.0000130.0000120.0000150.0001750.0000120.0000150.0000120.0000160.0001810.0003300.0000530.0000570.0001140.0000580.0001410.0001320.0014470.0000620.0032180.0000220.0005640.0000140.0000390.0000670.6440730.2968340.0014370.1530950.0004580.0390750.0002110.1551030.0363770.0001120.0010840.0000253.1638941.5224281.7225490.0045572.5287850.0067100.2997210.0007960.0251760.0004990.0003710.0004420.0001300.0000120.0001170.0000130.0001200.0001080.0002010.0002540.0003600.0001370.0002490.0019400.6331700.4041390.4753650.3036321.0437320.0053420.6309760.0036910.0022100.0000870.0015825.7131020.0140600.0441030.0009000.0005600.0002350.0005150.0004440.0006320.0002300.0000120.0004720.0026250.0005760.0020750.0000160.0005570.0001380.0000120.0000110.0000120.0012600.0000150.0000110.0000110.0000110.0000120.0000110.0000110.0008391.2618740.0631550.0035090.0003520.0000130.5469410.6874080.0015580.0002020.0004770.0002170.0001140.0002063.4485692.3051230.0050720.0001760.0001680.0002190.7057602.1021991.6716921.2550251.6657160.8952630.0019780.9085170.7386660.0065080.0000250.3890730.0008360.0000830.0000130.0000120.0007100.0001340.9066630.4896890.0010981.1240570.5957480.3174820.0006670.4034660.0008620.3614651.7203451.5803072.0739160.0049300.0000610.0000110.0000130.0000140.8214710.0016661.2345130.1237350.0002640.0000180.0001300.0000120.0003060.0000160.0004200.8795630.0018340.0000650.0000221.5756801.3683460.0026790.0013080.0006120.0005950.0877990.8924880.0019300.0002360.0018260.0049050.0000730.0089170.0002620.9491700.0018230.0011910.0002640.3592810.0017530.0005440.0000800.0001830.0003230.0004250.0001880.0013820.0285440.0033140.1738701.0918490.8778750.8185820.2081550.0183390.0002520.0049240.0269961.9764451.0104011.8582070.0037110.0005240.8962690.0016240.0000171.2184351.5049680.0027250.0000170.6001910.0010770.0001690.0000110.0003060.0003580.0000120.0000110.0000290.0000110.0000160.6282940.7262470.0026960.0004110.0002942.4964821.8468000.9432100.8482430.0014970.0000190.0000282.3094160.8881760.0015840.0000950.7914390.7868670.8737430.0015580.0000630.0000190.0000170.0006440.0000130.0011850.0002710.0000120.0005950.0010670.0003120.0000800.0011390.1181900.0002190.0001302.8338403.8652260.0094950.0003880.0001740.0001550.0001520.0003440.0000140.0000300.0002190.0290630.0177680.0000920.0000140.0000840.0000150.0000890.0000180.0000130.0000550.0000650.0000200.0000770.0000140.0011510.0000140.0189670.0008191.8889860.0039070.0010583.8231472.6741901.8521750.0052930.3661250.0122710.0004060.0003750.0016980.0006040.0002650.0003040.0001890.0003340.0234470.0330070.3921100.0008550.0000130.6472680.0009910.0003140.0002260.0003270.3893930.0022520.1044380.5587060.0010620.0002670.0000700.0000200.0001240.0000570.0000760.0000130.0000130.4962370.3230300.0023290.0000160.0004270.0014730.0000140.0000111.0046690.0014740.0000170.0000140.0000130.0003390.0000120.0001310.0000120.0000130.0001950.0017830.0020240.0042240.0005130.0002560.0004831.5215643.4064665.8568090.0086890.0005570.0004840.0004680.0006840.0005920.0012570.0002380.0002530.0003830.0004210.0002960.0002700.0048111.2157210.9013410.0042081.1569210.0030600.9817860.0894661.9857692.9538471.6241041.3933672.7652721.7924540.0028950.0004310.0001920.0015330.0004970.0000120.2437640.0003410.0006010.4579460.0009320.0000130.0000120.0011420.0162250.1160690.0049490.7246580.0212900.1458570.0006700.0002590.0002730.0005720.7457761.9361730.0027590.0002920.0004640.0001830.0025030.0001580.0010520.0003000.0000160.0000120.0000120.0000841.2173651.2104731.1492310.0019420.0017200.0019601.7634024.0161162.0854710.0029480.0006630.0000300.0002320.0001860.0002760.0006520.0001490.0002950.0022100.0058590.0053930.0023270.9415430.0012881.0681853.3138030.7084850.8045071.0922201.8802200.0040570.7811950.0023270.0002160.0002890.0003200.0014530.0003520.0000870.0000571.0207193.3185241.0197615.1167177.5576216.5548530.7322870.0167490.0001090.0001700.0000710.0000180.0017840.0083290.0000390.0000190.0000478.7319771.1824240.9528120.0024350.7894731.3512350.0022660.0008620.2802890.0004930.3258840.6817670.8239550.0019080.0013570.0000140.0000120.0004140.4481180.5190800.0010220.0318820.0014760.0002490.0003200.0001220.8479662.0152171.7030851.5950931.7440310.0022300.0000280.0001970.0001580.0002000.0003590.0003380.0001290.0000950.0000870.0000170.0002220.0001180.0001710.0001230.0001450.0002250.0001160.0006500.0001730.0001390.0000940.0000140.0002510.0002770.0000470.0000200.0000800.0001070.0002230.9930920.0011280.0000650.0000140.0000130.0001640.0004260.0000130.0001270.0001370.0001410.0000120.0002370.0003870.0001890.3012410.0499180.0032930.0005360.0007250.0003850.0005220.0005240.0006560.0003870.0006590.0003350.0002360.0006530.0007280.0002910.0002371.2323420.0016110.0029200.0003700.0001730.0003710.0004700.0002470.0002500.0003330.0001530.0000140.0000540.0012400.0000720.0000140.0002720.0000140.0000150.0000130.0000200.0000180.0000430.2944060.0004000.0001930.0004520.0001990.0049090.0002140.0001190.0003800.7783033.1563273.2536970.8171390.0010490.0001340.0001220.0000130.0003000.0002970.0000540.3687580.0008660.0003950.0009630.0014010.0278120.5381970.0026580.0011500.0006080.9855721.0059383.1879830.0118490.0510100.0003860.0002820.0004080.0003000.0006120.0004550.0003830.0003480.0003560.0002940.0000120.0000110.0000110.0002760.0000300.0000120.0002150.1949840.0593790.0001200.0016180.0005260.6684770.0023990.0012300.0023120.6622040.0032070.0001130.0000920.0000120.0002260.0002000.0002710.0000120.6852680.0627710.6376120.0014043.0003640.0602131.1271801.1035663.4013330.0033501.1643200.0013780.0000130.4870410.0005090.0004140.0000120.0014890.0002000.0001761.5920011.7358201.9756671.2292622.4335950.0032550.0005210.8616761.2441291.6032311.3697121.1363770.0016060.3534670.4819341.1162970.0884200.0002670.0000660.0001280.0000120.0001130.0013650.0012750.0001890.0005930.7192810.0009790.0007650.0001240.0005740.0047350.0007800.0028120.7176740.7448880.0008750.0000940.2275660.0003980.0046860.0031740.0007100.6456850.0010590.0000910.0005580.6867320.0007260.0002420.0001970.0002310.0001710.3204290.0004220.0002300.0003370.0003200.0001700.0003210.0000130.0011410.0000130.0009170.0000120.0000150.0000490.3395700.0441440.0000530.0008180.0000130.0005990.0001290.0000120.0004230.0002100.0001820.0000110.0000110.0000110.0000160.0001980.0006930.0000130.0000940.0003700.5040050.0008490.0002650.0004500.0002060.0001560.0003643.0408387.1131020.0076740.0004810.0003300.0004170.0004080.0004830.0001170.0011580.0004840.0000940.6427250.2248870.0026080.0001222.2033400.0915180.0248830.0005220.0003620.0000840.0002780.0001970.0045180.8429840.0008480.0002040.0001570.0000120.3139960.0219800.0009670.0000130.0017100.0001160.0002450.6753120.0023070.0000730.0007880.0004500.4471810.0008921.8554811.9266790.0251900.0011480.0024020.0019900.0025080.0012630.0025730.5571840.0433380.0020050.0010821.2354620.8011490.6385470.0011560.0000640.0000600.0003410.0004200.0002800.0003910.0006730.0001340.0006400.0000810.0002280.0001500.0001560.000119

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/
With following parameter combination: {'batch_size_seq_length': 2, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_6477fe2a_2020-02-05_10-46-049g5jn5y4/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    560    |    33     |    79     |     8     |
-------------------------------------------------------------
| disagree  |    12     |    76     |    44     |     5     |
-------------------------------------------------------------
|  discuss  |    188    |    53     |   1663    |    20     |
-------------------------------------------------------------
| unrelated |     2     |     0     |    14     |   6865    |
-------------------------------------------------------------
Score: 7270.0 out of 7516.5	(96.72054812745294%)
Accuracy: 0.9524007482851798
F1 overall: 0.7936574420778848
F1 per class: [0.7766990291262136, 0.5083612040133779, 0.89312567132116, 0.9964438638507874]
*******************************************


*******************************************
EVALUATING PIPELINE 
{'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'linear'}
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                                                                                                          | 0/9622 [00:00<?, ?it/s]  0%|                                                                                                                                | 1/9622 [00:02<7:04:24,  2.65s/it]  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                       | 501/9622 [00:03<4:41:41,  1.85s/it] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                   | 2001/9622 [00:03<2:44:45,  1.30s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 7501/9622 [00:03<32:05,  1.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9622/9622 [00:03<00:00, 2590.63it/s]
  ******************************************* 
              Running Evaluation                
  *******************************************
 
  Length dataset evaluation: 9622
  Length dataloader evaluation: 301
  Number of epochs: 3
  Batch size: 32
  ******************************************* 
 
['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/outputs/checkpoint-3']
INFO:__main__:Evaluate the following checkpoints: ['/home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/outputs/checkpoint-3']
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/outputs/checkpoint-3/pytorch_model.bin
HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=301.0, style=ProgressStyle(description_width='initial')), HTML(value='')))
0.0005220.0005870.0004090.3639820.0911220.0187250.1140870.0165710.0022100.0003390.0001830.0003580.0010870.1464690.0105950.0008520.0033750.0004050.0002810.3421760.0173370.6265470.1574510.8065840.0391560.0384690.0032750.0002040.0103710.0010400.0002370.1651920.1741560.3235370.1216690.0035400.2180410.6560680.0237980.0008470.0005280.0769430.0068430.0002140.0000610.0000880.0000590.0000840.0001430.0004670.0000710.0015080.1624740.1736270.0033330.0305880.2553800.0045410.0007580.0006021.1216520.7758770.0134790.0008150.0013840.0016810.1987410.4087670.3483650.2640610.0108430.0003060.5910690.2026510.2466190.0056620.3298990.3548190.1980250.0026500.1620210.0592410.0186440.0150270.0009740.0021430.0000840.0002920.0007530.0307510.0052501.3201880.0546850.0230520.6868190.5774440.0067220.0026840.1473670.0060650.1117650.0235760.2176400.0031360.0017100.0004410.0010010.0001520.0571720.0005800.0117120.3860850.0044180.9451650.0264260.5194530.7673600.1365580.0012520.1258290.1646440.4968892.5017090.0204370.3733100.0030720.0184990.1474610.1767330.3432350.0052010.2549280.0615160.0016270.0179510.4167420.1372460.0955170.2440090.6368360.1158390.0011120.2299280.0025480.1905390.2187870.5153770.4064280.0032410.0016330.0019750.0036581.5671180.0112100.0008620.0583670.0006700.0003850.0001440.0996521.5899100.0180650.0015400.0010980.2757330.1576260.0103980.0008330.0001950.0055850.0026860.0001040.0067620.0002070.0042080.0933012.1374560.0158780.0069990.0008770.4038220.2617000.4269470.0819350.0017310.0016370.6341912.3353590.5510670.3338900.1855210.0013140.1717600.3437231.3595200.0098710.0007490.0029270.4139870.8214820.3572480.0029880.2090963.2858421.5618130.0524650.0967410.4361650.1675850.1799540.0703720.0006950.7402530.3926140.5772410.0030730.0005270.0003080.0003950.0002390.0003600.0003690.0006080.0001910.0002180.0093540.1817240.0017170.1376540.2184170.0267760.0086440.0056080.0005600.0001900.0082660.2191650.0778801.7395700.1627950.0042550.1048650.0869670.3807970.6361270.0036650.0011340.0002370.1675850.5219700.5486740.1101780.0047910.8479700.9588121.0567940.3801760.0090760.2119570.0921570.1569840.5876230.1546820.0008520.0073700.0766460.1700800.5948170.4208760.0027220.2075750.0014450.0006900.0003130.0004860.0147830.0023640.0004650.0000640.0006400.0641160.0014441.3504640.0107750.0049500.0273210.2069260.0019100.4075420.0561860.0497310.1549990.2419780.8340270.0081380.1269540.0829540.0016560.0025810.0049690.000502

*******************************************
EVALUATING PIPELINE 
Pipeline directory: /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/
With following parameter combination: {'batch_size_seq_length': 4, 'lr': 4e-05, 'lr_type': 'linear'}
EVALUATION OF CHECKPOINT /home/ubuntu/fnd_implementation/bert/grid_search/results/pipeline_697ed470_2020-02-05_12-50-53nhmjgnvj/outputs/checkpoint-3
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    597    |    29     |    97     |     5     |
-------------------------------------------------------------
| disagree  |    17     |    79     |    47     |     0     |
-------------------------------------------------------------
|  discuss  |    146    |    52     |   1637    |    24     |
-------------------------------------------------------------
| unrelated |     2     |     2     |    19     |   6869    |
-------------------------------------------------------------
Score: 7263.25 out of 7516.5	(96.63074569280916%)
Accuracy: 0.9542714612346705
F1 overall: 0.8025960537143917
F1 per class: [0.8013422818791947, 0.5180327868852459, 0.8947799945340257, 0.9962291515591007]
*******************************************


real	19m20.349s
user	23m17.263s
sys	7m28.146s
ubuntu@run-gpu-mg:~/fnd_implementation$ 
ubuntu@run-gpu-mg:~/fnd_implementation$ [Kubuntu@run-gpu-mg:~/fnd_implementation$ exit
exit

Script done on 2020-02-05 19:06:43+0000

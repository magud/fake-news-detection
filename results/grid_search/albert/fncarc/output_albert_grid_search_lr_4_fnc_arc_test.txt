Script started on 2020-02-20 08:55:00+0000
ubuntu@run-gpu-mg:~/fnd_implementation$ exittime python3 eval_separate.py --model=albbert --model_type=albert-base-v1 --dataset_name=fnc_arcM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=albe[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=alber[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=albert[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C_separate.py --model=albert [1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct_separate.py --model=albert[1@ M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ce_separate.py --model=alber[1@tM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cs_separate.py --model=albe[1@rM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ct_separate.py --model=alb[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
There are 2 GPU(s) available.
The following GPU is used:  Tesla V100-PCIE-16GB
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-spiece.model from cache at /home/ubuntu/.cache/torch/transformers/e941f532bbbf6d6b7c96efbde9c15d38fc236e7fb120158bfc766814e6170529.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/albert/model_pretrained/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "finetuning_task": null,
  "gap_size": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_memory_blocks": 0,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30000
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/albert/model_pretrained/pytorch_model.bin
INFO:__main__:Loading initialized pretrained model from /home/ubuntu/fnd_implementation/albert/model_pretrained
  ******************************************* 
           Freezing Embedding Layers          
  *******************************************
 
name:  albert.embeddings.word_embeddings.weight
param.requires_grad:  False
=====
name:  albert.embeddings.position_embeddings.weight
param.requires_grad:  False
=====
name:  albert.embeddings.token_type_embeddings.weight
param.requires_grad:  False
=====
name:  albert.embeddings.LayerNorm.weight
param.requires_grad:  False
=====
name:  albert.embeddings.LayerNorm.bias
param.requires_grad:  False
=====
name:  albert.encoder.embedding_hidden_mapping_in.weight
param.requires_grad:  True
=====
name:  albert.encoder.embedding_hidden_mapping_in.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight
param.requires_grad:  True
=====
name:  albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias
param.requires_grad:  True
=====
name:  albert.pooler.weight
param.requires_grad:  True
=====
name:  albert.pooler.bias
param.requires_grad:  True
=====
name:  classifier.weight
param.requires_grad:  True
=====
name:  classifier.bias
param.requires_grad:  True
=====
/home/ubuntu/fnd_implementation/albert/grid_search/results/pipeline_42fc55b2_2020-02-19_16-37-21kqp3l0ll/params.json
INFO:transformers.configuration_utils:loading configuration file /home/ubuntu/fnd_implementation/albert/grid_search/results/pipeline_42fc55b2_2020-02-19_16-37-21kqp3l0ll/outputs/checkpoint-3/config.json
INFO:transformers.configuration_utils:Model config {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "finetuning_task": null,
  "gap_size": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_memory_blocks": 0,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30000
}

INFO:transformers.modeling_utils:loading weights file /home/ubuntu/fnd_implementation/albert/grid_search/results/pipeline_42fc55b2_2020-02-19_16-37-21kqp3l0ll/outputs/checkpoint-3/pytorch_model.bin
INFO:__main__:Creating features from dataset file at /home/ubuntu/fnd_implementation/data/processed
  0%|                                                 | 0/28972 [00:00<?, ?it/s]  0%|                                       | 1/28972 [00:00<6:33:32,  1.23it/s]  2%|▋                                    | 501/28972 [00:01<4:30:57,  1.75it/s]  3%|█▏                                  | 1001/28972 [00:01<3:06:26,  2.50it/s] 14%|████▉                               | 4001/28972 [00:03<1:56:33,  3.57it/s] 57%|█████████████████████                | 16501/28972 [00:03<40:44,  5.10it/s] 69%|█████████████████████████▌           | 20001/28972 [00:03<20:31,  7.28it/s] 78%|████████████████████████████▊        | 22562/28972 [00:03<10:16, 10.40it/s]100%|███████████████████████████████████| 28972/28972 [00:03<00:00, 7500.17it/s]
INFO:__main__:  ******************************************* 
INFO:__main__:              Running Final Testing           
INFO:__main__:  ******************************************* 
 
INFO:__main__:  ******************************************* 
INFO:__main__:  Model: albert
INFO:__main__:  Length dataset testing: 28972
INFO:__main__:  Length dataloader testing: 906
INFO:__main__:  Number of epochs: 3
INFO:__main__:  Maximal sequence length: 256
INFO:__main__:  Batch size: 32
INFO:__main__:  Learning rate: 4e-05
INFO:__main__:  Learning rate type: linear
INFO:__main__:  ******************************************* 
 
test_separate.py:231: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch in tqdm_notebook(test_dataloader, desc="Testing"):
HBox(children=(FloatProgress(value=0.0, description='Testing', max=906.0, style=ProgressStyle(description_width='initial')), HTML(value='')))

-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |   1257    |    381    |    722    |    88     |
-------------------------------------------------------------
| disagree  |    110    |    250    |    66     |    32     |
-------------------------------------------------------------
|  discuss  |    546    |    245    |   3344    |    242    |
-------------------------------------------------------------
| unrelated |    324    |    193    |    511    |   20661   |
-------------------------------------------------------------
Score: 20235.75 out of 22597.75	(89.54763195450874%)
Accuracy: 0.8805743476460031
F1 overall: 0.6432413702069855
F1 per class: [0.5366061899679829, 0.3274394237066143, 0.7414634146341463, 0.9674564525191983]

real	2m51.406s
user	3m3.935s
sys	1m13.435s
ubuntu@run-gpu-mg:~/fnd_implementation$ exit
exit

Script done on 2020-02-20 09:00:34+0000
